{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpO2hkRxNtxmXQ7X5DE6Zt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkhok22/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_ARIMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import libraries and Initialize wandb"
      ],
      "metadata": {
        "id": "XmGPuEOFuBm3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "vWcCwrG1gWq_",
        "outputId": "418fd71e-f0d4-4d50-a7fb-abd8c166bde4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlkhok22\u001b[0m (\u001b[33mlkhok22-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250727_070620-edq4mj50</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/edq4mj50' target=\"_blank\">arima-model</a></strong> to <a href='https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/edq4mj50' target=\"_blank\">https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/edq4mj50</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import glob\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initialize wandb\n",
        "import wandb\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"arima-model\")\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unzip and load data"
      ],
      "metadata": {
        "id": "yzGqhQOutzN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip dataset\n",
        "zip_path = \"/content/drive/MyDrive/ML-FinalProject/data.zip\"\n",
        "extract_path = \"/content/data\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "for zip_file in glob.glob(f\"{extract_path}/*.csv.zip\"):\n",
        "    with zipfile.ZipFile(zip_file, 'r') as z:\n",
        "        z.extractall(extract_path)\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(f\"{extract_path}/train.csv\")\n",
        "test = pd.read_csv(f\"{extract_path}/test.csv\")\n",
        "stores = pd.read_csv(f\"{extract_path}/stores.csv\")\n",
        "features = pd.read_csv(f\"{extract_path}/features.csv\")\n",
        "\n",
        "# Log data loading completion to wandb\n",
        "wandb.log({\"step\": \"data_loading\", \"status\": \"completed\"})"
      ],
      "metadata": {
        "id": "wJtBIknOttDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Process data"
      ],
      "metadata": {
        "id": "FjnfzfOCuOju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Date column to datetime\n",
        "train['Date'] = pd.to_datetime(train['Date'])\n",
        "test['Date'] = pd.to_datetime(test['Date'])\n",
        "features['Date'] = pd.to_datetime(features['Date'])\n",
        "\n",
        "# Merge train with features and stores\n",
        "train_merged = train.merge(features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "train_merged = train_merged.merge(stores, on='Store', how='left')\n",
        "\n",
        "# Merge test with features and stores\n",
        "test_merged = test.merge(features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "test_merged = test_merged.merge(stores, on='Store', how='left')\n",
        "\n",
        "# Sort by Store, Dept, and Date\n",
        "train_merged = train_merged.sort_values(['Store', 'Dept', 'Date'])\n",
        "test_merged = test_merged.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "# Log preprocessing step to wandb\n",
        "wandb.log({\"step\": \"data_preprocessing\", \"status\": \"merged_and_sorted\"})\n",
        "\n",
        "# Display basic info to verify\n",
        "print(\"Train merged shape:\", train_merged.shape)\n",
        "print(\"Test merged shape:\", test_merged.shape)\n",
        "print(train_merged.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXpss60RhNBg",
        "outputId": "d1f18732-17cf-4e28-9268-8acf66c65d76"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train merged shape: (421570, 16)\n",
            "Test merged shape: (115064, 15)\n",
            "   Store  Dept       Date  Weekly_Sales  IsHoliday  Temperature  Fuel_Price  \\\n",
            "0      1     1 2010-02-05      24924.50      False        42.31       2.572   \n",
            "1      1     1 2010-02-12      46039.49       True        38.51       2.548   \n",
            "2      1     1 2010-02-19      41595.55      False        39.93       2.514   \n",
            "3      1     1 2010-02-26      19403.54      False        46.63       2.561   \n",
            "4      1     1 2010-03-05      21827.90      False        46.50       2.625   \n",
            "\n",
            "   MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  \\\n",
            "0        NaN        NaN        NaN        NaN        NaN  211.096358   \n",
            "1        NaN        NaN        NaN        NaN        NaN  211.242170   \n",
            "2        NaN        NaN        NaN        NaN        NaN  211.289143   \n",
            "3        NaN        NaN        NaN        NaN        NaN  211.319643   \n",
            "4        NaN        NaN        NaN        NaN        NaN  211.350143   \n",
            "\n",
            "   Unemployment Type    Size  \n",
            "0         8.106    A  151315  \n",
            "1         8.106    A  151315  \n",
            "2         8.106    A  151315  \n",
            "3         8.106    A  151315  \n",
            "4         8.106    A  151315  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group train data by Store and Dept to check number of observations\n",
        "train_grouped = train_merged.groupby(['Store', 'Dept']).size().reset_index(name='count')\n",
        "\n",
        "# Filter Store-Dept pairs with at least 10 observations\n",
        "valid_pairs = train_grouped[train_grouped['count'] >= 10][['Store', 'Dept']]\n",
        "print(f\"Number of valid Store-Dept pairs: {len(valid_pairs)}\")\n",
        "\n",
        "# Filter train_merged to include only valid Store-Dept pairs\n",
        "train_filtered = train_merged.merge(valid_pairs, on=['Store', 'Dept'], how='inner')\n",
        "\n",
        "# Handle missing Weekly_Sales in train_filtered (if any)\n",
        "# Use last-value strategy: forward fill within each Store-Dept group\n",
        "train_filtered['Weekly_Sales'] = train_filtered.groupby(['Store', 'Dept'])['Weekly_Sales'].ffill()\n",
        "\n",
        "# Check for any remaining missing Weekly_Sales\n",
        "missing_sales = train_filtered['Weekly_Sales'].isna().sum()\n",
        "print(f\"Missing Weekly_Sales after ffill: {missing_sales}\")\n",
        "\n",
        "# If there are still missing Weekly_Sales (e.g., at the start of a series), fill with group mean\n",
        "if missing_sales > 0:\n",
        "    mean_sales = train_filtered.groupby(['Store', 'Dept'])['Weekly_Sales'].transform('mean')\n",
        "    train_filtered['Weekly_Sales'] = train_filtered['Weekly_Sales'].fillna(mean_sales)\n",
        "\n",
        "# Log missing data handling to wandb\n",
        "wandb.log({\"step\": \"missing_data_handling\", \"missing_sales_after_ffill\": missing_sales})\n",
        "\n",
        "# Verify the filtered dataset\n",
        "print(\"Train filtered shape:\", train_filtered.shape)\n",
        "print(train_filtered.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTS2niIhjrGG",
        "outputId": "f468c2cc-4094-4a1f-cb1b-cc3f3f639099"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of valid Store-Dept pairs: 3167\n",
            "Missing Weekly_Sales after ffill: 0\n",
            "Train filtered shape: (420927, 16)\n",
            "   Store  Dept       Date  Weekly_Sales  IsHoliday  Temperature  Fuel_Price  \\\n",
            "0      1     1 2010-02-05      24924.50      False        42.31       2.572   \n",
            "1      1     1 2010-02-12      46039.49       True        38.51       2.548   \n",
            "2      1     1 2010-02-19      41595.55      False        39.93       2.514   \n",
            "3      1     1 2010-02-26      19403.54      False        46.63       2.561   \n",
            "4      1     1 2010-03-05      21827.90      False        46.50       2.625   \n",
            "\n",
            "   MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  \\\n",
            "0        NaN        NaN        NaN        NaN        NaN  211.096358   \n",
            "1        NaN        NaN        NaN        NaN        NaN  211.242170   \n",
            "2        NaN        NaN        NaN        NaN        NaN  211.289143   \n",
            "3        NaN        NaN        NaN        NaN        NaN  211.319643   \n",
            "4        NaN        NaN        NaN        NaN        NaN  211.350143   \n",
            "\n",
            "   Unemployment Type    Size  \n",
            "0         8.106    A  151315  \n",
            "1         8.106    A  151315  \n",
            "2         8.106    A  151315  \n",
            "3         8.106    A  151315  \n",
            "4         8.106    A  151315  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2caBZS-XucG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries for stationarity test and ARIMA\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Function to check stationarity using ADF test\n",
        "def check_stationarity(series, store, dept):\n",
        "    result = adfuller(series.dropna(), autolag='AIC')\n",
        "    p_value = result[1]\n",
        "    is_stationary = p_value < 0.05  # 5% significance level\n",
        "    wandb.log({\n",
        "        f\"stationarity_store_{store}_dept_{dept}\": {\n",
        "            \"p_value\": p_value,\n",
        "            \"is_stationary\": is_stationary\n",
        "        }\n",
        "    })\n",
        "    return is_stationary\n",
        "\n",
        "# Create dictionary to store time series for each Store-Dept pair\n",
        "time_series_dict = {}\n",
        "\n",
        "# Iterate over valid Store-Dept pairs\n",
        "for _, row in valid_pairs.iterrows():\n",
        "    store, dept = row['Store'], row['Dept']\n",
        "    # Extract time series for this Store-Dept pair\n",
        "    ts = train_filtered[(train_filtered['Store'] == store) &\n",
        "                       (train_filtered['Dept'] == dept)][['Date', 'Weekly_Sales']]\n",
        "    # Set Date as index\n",
        "    ts = ts.set_index('Date')['Weekly_Sales']\n",
        "    # Check for sufficient data\n",
        "    if len(ts) >= 10:\n",
        "        # Check stationarity\n",
        "        is_stationary = check_stationarity(ts, store, dept)\n",
        "        time_series_dict[(store, dept)] = {\n",
        "            'series': ts,\n",
        "            'is_stationary': is_stationary\n",
        "        }\n",
        "\n",
        "# Count stationary and non-stationary series\n",
        "stationary_count = sum(1 for v in time_series_dict.values() if v['is_stationary'])\n",
        "print(f\"Number of stationary series: {stationary_count}\")\n",
        "print(f\"Number of non-stationary series: {len(time_series_dict) - stationary_count}\")\n",
        "\n",
        "# Log stationarity check to wandb\n",
        "wandb.log({\n",
        "    \"step\": \"stationarity_check\",\n",
        "    \"stationary_series_count\": stationary_count,\n",
        "    \"total_series_count\": len(time_series_dict)\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyRySAG_jyMN",
        "outputId": "fc123369-3492-4fd6-f393-d3dc8438c876"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of stationary series: 2275\n",
            "Number of non-stationary series: 892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ARIMA and evaluation metrics\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Suppress statsmodels warnings\n",
        "\n",
        "# Function to calculate WMAE\n",
        "def calculate_wmae(y_true, y_pred, is_holiday):\n",
        "    weights = np.where(is_holiday, 5, 1)\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "# Define train-validation split and train ARIMA on a subset\n",
        "val_predictions = []\n",
        "arima_models = {}\n",
        "arima_order = (1, 0, 2)\n",
        "\n",
        "# Limit to a subset of Store-Dept pairs for faster testing (e.g., first 10 pairs)\n",
        "subset_pairs = list(time_series_dict.keys())[:10]\n",
        "print(f\"Training ARIMA on {len(subset_pairs)} Store-Dept pairs\")\n",
        "\n",
        "# Iterate over subset of Store-Dept pairs\n",
        "for store, dept in subset_pairs:\n",
        "    ts = time_series_dict[(store, dept)]['series']\n",
        "    # Set frequency to W-FRI to avoid warnings\n",
        "    ts.index = ts.index.to_period('W-FRI').to_timestamp('W-FRI')\n",
        "\n",
        "    # Sort dates and split into train (first 80%) and validation (last 20%)\n",
        "    dates = ts.index.sort_values()\n",
        "    train_size = int(0.8 * len(dates))\n",
        "    train_dates = dates[:train_size]\n",
        "    val_dates = dates[train_size:]\n",
        "\n",
        "    train_ts = ts[train_dates]\n",
        "    val_ts = ts[val_dates]\n",
        "\n",
        "    print(f\"Store {store}, Dept {dept}: Train size = {len(train_ts)}, Val size = {len(val_ts)}\")\n",
        "\n",
        "    if len(train_ts) >= 10 and len(val_ts) > 0:\n",
        "        try:\n",
        "            # Train ARIMA model\n",
        "            model = ARIMA(train_ts, order=arima_order).fit()\n",
        "            arima_models[(store, dept)] = model\n",
        "\n",
        "            # Predict on validation set\n",
        "            val_pred = model.forecast(steps=len(val_ts))\n",
        "\n",
        "            # Get actual values and holiday flags for validation\n",
        "            val_actual = val_ts.values\n",
        "            val_holidays = train_filtered[(train_filtered['Store'] == store) &\n",
        "                                        (train_filtered['Dept'] == dept) &\n",
        "                                        (train_filtered['Date'].isin(val_dates))]['IsHoliday'].values\n",
        "\n",
        "            # Ensure holiday flags match prediction length\n",
        "            if len(val_holidays) != len(val_pred):\n",
        "                print(f\"Warning: Holiday flags length mismatch for Store {store}, Dept {dept}\")\n",
        "                continue\n",
        "\n",
        "            # Calculate WMAE for this Store-Dept pair\n",
        "            wmae = calculate_wmae(val_actual, val_pred, val_holidays)\n",
        "\n",
        "            # Store predictions for overall evaluation\n",
        "            for date, pred, actual, holiday in zip(val_ts.index, val_pred, val_actual, val_holidays):\n",
        "                val_predictions.append({\n",
        "                    'Store': store,\n",
        "                    'Dept': dept,\n",
        "                    'Date': date,\n",
        "                    'Weekly_Sales_Pred': pred,\n",
        "                    'Weekly_Sales_Actual': actual,\n",
        "                    'IsHoliday': holiday\n",
        "                })\n",
        "\n",
        "            # Log WMAE for this pair\n",
        "            wandb.log({f'wmae_store_{store}_dept_{dept}': wmae})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ARIMA failed for Store {store}, Dept {dept}: {e}\")\n",
        "            # Use last-value strategy for failed models\n",
        "            last_value = train_ts[-1] if len(train_ts) > 0 else train_filtered[train_filtered['Dept'] == dept]['Weekly_Sales'].mean()\n",
        "            val_pred = np.full(len(val_ts), last_value)\n",
        "            val_holidays = train_filtered[(train_filtered['Store'] == store) &\n",
        "                                        (train_filtered['Dept'] == dept) &\n",
        "                                        (train_filtered['Date'].isin(val_dates))]['IsHoliday'].values\n",
        "            if len(val_holidays) != len(val_pred):\n",
        "                print(f\"Warning: Holiday flags length mismatch in fallback for Store {store}, Dept {dept}\")\n",
        "                continue\n",
        "            for date, pred, actual, holiday in zip(val_ts.index, val_pred, val_actual, val_holidays):\n",
        "                val_predictions.append({\n",
        "                    'Store': store,\n",
        "                    'Dept': dept,\n",
        "                    'Date': date,\n",
        "                    'Weekly_Sales_Pred': pred,\n",
        "                    'Weekly_Sales_Actual': actual,\n",
        "                    'IsHoliday': holiday\n",
        "                })\n",
        "\n",
        "# Convert predictions to DataFrame\n",
        "val_predictions_df = pd.DataFrame(val_predictions)\n",
        "\n",
        "# Check if predictions DataFrame is empty\n",
        "if val_predictions_df.empty:\n",
        "    print(\"Error: val_predictions_df is empty. Check if any Store-Dept pairs had valid validation data.\")\n",
        "else:\n",
        "    print(\"val_predictions_df columns:\", val_predictions_df.columns.tolist())\n",
        "    print(val_predictions_df.head())\n",
        "\n",
        "# Calculate overall WMAE if DataFrame is not empty\n",
        "if not val_predictions_df.empty:\n",
        "    overall_wmae = calculate_wmae(val_predictions_df['Weekly_Sales_Actual'],\n",
        "                                 val_predictions_df['Weekly_Sales_Pred'],\n",
        "                                 val_predictions_df['IsHoliday'])\n",
        "    print(f\"Overall WMAE for ARIMA(1,0,2) on subset: {overall_wmae}\")\n",
        "\n",
        "    # Log overall WMAE to wandb\n",
        "    wandb.log({\"step\": \"arima_training_subset\", \"arima_order\": \"1,0,2\", \"overall_wmae\": overall_wmae})\n",
        "else:\n",
        "    print(\"Skipping WMAE calculation due to empty predictions DataFrame.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeFHGRk7kEw5",
        "outputId": "b25689e8-f851-4b01-ecc7-c2408f87056c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ARIMA on 10 Store-Dept pairs\n",
            "Store 1, Dept 1: Train size = 114, Val size = 29\n",
            "Store 1, Dept 2: Train size = 114, Val size = 29\n",
            "Store 1, Dept 3: Train size = 114, Val size = 29\n",
            "Store 1, Dept 4: Train size = 114, Val size = 29\n",
            "Store 1, Dept 5: Train size = 114, Val size = 29\n",
            "Store 1, Dept 6: Train size = 114, Val size = 29\n",
            "Store 1, Dept 7: Train size = 114, Val size = 29\n",
            "Store 1, Dept 8: Train size = 114, Val size = 29\n",
            "Store 1, Dept 9: Train size = 114, Val size = 29\n",
            "Store 1, Dept 10: Train size = 114, Val size = 29\n",
            "val_predictions_df columns: ['Store', 'Dept', 'Date', 'Weekly_Sales_Pred', 'Weekly_Sales_Actual', 'IsHoliday']\n",
            "   Store  Dept       Date  Weekly_Sales_Pred  Weekly_Sales_Actual  IsHoliday\n",
            "0      1     1 2012-04-13       47378.974270             34684.21      False\n",
            "1      1     1 2012-04-20       29380.548784             16976.19      False\n",
            "2      1     1 2012-04-27       24169.315220             16347.60      False\n",
            "3      1     1 2012-05-04       23549.203121             17147.44      False\n",
            "4      1     1 2012-05-11       23475.412720             18164.20      False\n",
            "Overall WMAE for ARIMA(1,0,2) on subset: 4254.326906265126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare test set predictions\n",
        "test_predictions = []\n",
        "\n",
        "# Iterate over test set Store-Dept-Date triplets\n",
        "test_grouped = test_merged.groupby(['Store', 'Dept'])\n",
        "for (store, dept), group in test_grouped:\n",
        "    # Get test dates for this Store-Dept pair\n",
        "    test_dates = group['Date'].sort_values()\n",
        "    test_holidays = group['IsHoliday'].values\n",
        "\n",
        "    # Check if we have a trained model for this Store-Dept pair\n",
        "    if (store, dept) in arima_models:\n",
        "        try:\n",
        "            model = arima_models[(store, dept)]\n",
        "            # Forecast for the number of test dates\n",
        "            pred = model.forecast(steps=len(test_dates))\n",
        "        except Exception as e:\n",
        "            print(f\"Forecast failed for Store {store}, Dept {dept}: {e}\")\n",
        "            # Fallback to last-value or mean\n",
        "            train_ts = time_series_dict.get((store, dept), {}).get('series', None)\n",
        "            pred = np.full(len(test_dates), train_ts[-1] if train_ts is not None and len(train_ts) > 0 else\n",
        "                          train_filtered[train_filtered['Dept'] == dept]['Weekly_Sales'].mean())\n",
        "    else:\n",
        "        # For new Store-Dept pairs in test set, use department mean\n",
        "        train_ts = time_series_dict.get((store, dept), {}).get('series', None)\n",
        "        pred = np.full(len(test_dates), train_ts[-1] if train_ts is not None and len(train_ts) > 0 else\n",
        "                      train_filtered[train_filtered['Dept'] == dept]['Weekly_Sales'].mean())\n",
        "\n",
        "    # Store predictions\n",
        "    for date, pred_sales in zip(test_dates, pred):\n",
        "        test_predictions.append({\n",
        "            'Store': store,\n",
        "            'Dept': dept,\n",
        "            'Date': date,\n",
        "            'Weekly_Sales': pred_sales\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame\n",
        "test_predictions_df = pd.DataFrame(test_predictions)\n",
        "\n",
        "# Create Id column in the format Store_Dept_Date\n",
        "test_predictions_df['Id'] = test_predictions_df.apply(\n",
        "    lambda x: f\"{int(x['Store'])}_{int(x['Dept'])}_{x['Date'].strftime('%Y-%m-%d')}\", axis=1)\n",
        "\n",
        "# Prepare submission file\n",
        "submission = test_predictions_df[['Id', 'Weekly_Sales']]\n",
        "submission['Weekly_Sales'] = submission['Weekly_Sales'].round(2)\n",
        "submission.to_csv('/content/submission.csv', index=False)\n",
        "\n",
        "# Log submission file creation to wandb\n",
        "wandb.log({\"step\": \"test_predictions\", \"submission_file\": \"created\"})\n",
        "wandb.save('/content/submission.csv')\n",
        "\n",
        "# Verify submission\n",
        "print(\"Submission file shape:\", submission.shape)\n",
        "print(submission.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABiYObsCkd9B",
        "outputId": "eca564d5-b459-44f0-e8ed-5dab2d0f767e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file shape: (115064, 2)\n",
            "               Id  Weekly_Sales\n",
            "0  1_1_2012-11-02      47378.97\n",
            "1  1_1_2012-11-09      29380.55\n",
            "2  1_1_2012-11-16      24169.32\n",
            "3  1_1_2012-11-23      23549.20\n",
            "4  1_1_2012-11-30      23475.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wandb\n",
        "\n",
        "# Generate test set predictions with robust fallback\n",
        "test_predictions = []\n",
        "\n",
        "# Global mean as ultimate fallback if department mean is NaN\n",
        "global_mean_sales = train_filtered['Weekly_Sales'].mean() if not train_filtered['Weekly_Sales'].isna().all() else 0.0\n",
        "print(f\"Global mean sales (fallback): {global_mean_sales}\")\n",
        "\n",
        "# Check departments in test set not in train set\n",
        "train_depts = set(train_filtered['Dept'].unique())\n",
        "test_depts = set(test_merged['Dept'].unique())\n",
        "missing_depts = test_depts - train_depts\n",
        "print(f\"Departments in test set but not in train set: {missing_depts}\")\n",
        "\n",
        "# Iterate over test set Store-Dept-Date triplets\n",
        "test_grouped = test_merged.groupby(['Store', 'Dept'])\n",
        "for (store, dept), group in test_grouped:\n",
        "    test_dates = group['Date'].sort_values()\n",
        "    test_holidays = group['IsHoliday'].values\n",
        "\n",
        "    # Initialize predictions\n",
        "    pred = None\n",
        "\n",
        "    # Check if we have a trained model for this Store-Dept pair\n",
        "    if (store, dept) in arima_models:\n",
        "        try:\n",
        "            model = arima_models[(store, dept)]\n",
        "            pred = model.forecast(steps=len(test_dates))\n",
        "        except Exception as e:\n",
        "            print(f\"Forecast failed for Store {store}, Dept {dept}: {e}\")\n",
        "\n",
        "    # If no model or forecast failed, use fallback\n",
        "    if pred is None or np.any(np.isnan(pred)) or np.any(np.isinf(pred)):\n",
        "        train_ts = time_series_dict.get((store, dept), {}).get('series', None)\n",
        "        if train_ts is not None and len(train_ts) > 0:\n",
        "            pred = np.full(len(test_dates), train_ts[-1])\n",
        "        else:\n",
        "            # Use department mean if available, otherwise global mean\n",
        "            dept_mean = train_filtered[train_filtered['Dept'] == dept]['Weekly_Sales'].mean()\n",
        "            pred = np.full(len(test_dates), dept_mean if not np.isnan(dept_mean) else global_mean_sales)\n",
        "\n",
        "    # Ensure predictions are valid floats and round to two decimal places\n",
        "    pred = np.where(np.isnan(pred) | np.isinf(pred), global_mean_sales, pred)\n",
        "    pred = np.round(pred, 2).astype(float)  # Round to nearest hundredth and ensure float\n",
        "\n",
        "    # Store predictions\n",
        "    for date, pred_sales in zip(test_dates, pred):\n",
        "        test_predictions.append({\n",
        "            'Store': store,\n",
        "            'Dept': dept,\n",
        "            'Date': date,\n",
        "            'Weekly_Sales': pred_sales\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame\n",
        "test_predictions_df = pd.DataFrame(test_predictions)\n",
        "\n",
        "# Create Id column in the format Store_Dept_Date\n",
        "test_predictions_df['Id'] = test_predictions_df.apply(\n",
        "    lambda x: f\"{int(x['Store'])}_{int(x['Dept'])}_{x['Date'].strftime('%Y-%m-%d')}\", axis=1)\n",
        "\n",
        "# Prepare submission file with rounded Weekly_Sales\n",
        "submission = test_predictions_df[['Id', 'Weekly_Sales']]\n",
        "submission['Weekly_Sales'] = submission['Weekly_Sales'].round(2)  # Ensure rounding in submission\n",
        "submission['Weekly_Sales'] = submission['Weekly_Sales'].fillna(global_mean_sales)  # Final NaN check\n",
        "submission['Weekly_Sales'] = submission['Weekly_Sales'].apply(lambda x: global_mean_sales if pd.isna(x) or np.isinf(x) else x)\n",
        "\n",
        "# Verify no empty strings or invalid values\n",
        "submission['Weekly_Sales'] = submission['Weekly_Sales'].apply(lambda x: global_mean_sales if isinstance(x, str) and x.strip() == '' else x)\n",
        "\n",
        "# Save submission file\n",
        "submission.to_csv('/content/submission.csv', index=False)\n",
        "\n",
        "# Log submission file creation to wandb\n",
        "wandb.log({\"step\": \"test_predictions_fixed_rounded\", \"submission_file\": \"created\"})\n",
        "wandb.save('/content/submission.csv')\n",
        "\n",
        "# Verify submission\n",
        "print(\"Submission file shape:\", submission.shape)\n",
        "print(submission.head())\n",
        "print(\"\\nNaN values in Weekly_Sales:\", submission['Weekly_Sales'].isna().sum())\n",
        "print(\"Infinite values in Weekly_Sales:\", np.isinf(submission['Weekly_Sales']).sum())\n",
        "print(\"Empty or non-numeric values in Weekly_Sales:\",\n",
        "      submission['Weekly_Sales'].apply(lambda x: isinstance(x, str) and x.strip() == '').sum())\n",
        "\n",
        "# Check problematic rows in new submission\n",
        "problem_lines = [4925, 9497, 36405, 47110, 49709, 49710, 49711, 49712, 49713, 49714]\n",
        "problem_rows = [line - 2 for line in problem_lines]  # Adjust for header and 0-based indexing\n",
        "print(\"\\nChecking problematic rows in new submission:\")\n",
        "print(submission.iloc[problem_rows])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkTvAZRIozpM",
        "outputId": "897d8874-f9da-4ba4-f356-38f63c406e8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global mean sales (fallback): 16005.536820398784\n",
            "Departments in test set but not in train set: {np.int64(43), np.int64(77), np.int64(39)}\n",
            "Submission file shape: (115064, 2)\n",
            "               Id  Weekly_Sales\n",
            "0  1_1_2012-11-02      47378.97\n",
            "1  1_1_2012-11-09      29380.55\n",
            "2  1_1_2012-11-16      24169.32\n",
            "3  1_1_2012-11-23      23549.20\n",
            "4  1_1_2012-11-30      23475.41\n",
            "\n",
            "NaN values in Weekly_Sales: 0\n",
            "Infinite values in Weekly_Sales: 0\n",
            "Empty or non-numeric values in Weekly_Sales: 0\n",
            "\n",
            "Checking problematic rows in new submission:\n",
            "                     Id  Weekly_Sales\n",
            "4923    2_77_2013-01-11      16005.54\n",
            "9495    4_39_2013-07-12      16005.54\n",
            "36403  14_43_2012-12-07      16005.54\n",
            "47108  18_43_2013-04-26      16005.54\n",
            "49707  19_39_2012-11-02      16005.54\n",
            "49708  19_39_2012-11-09      16005.54\n",
            "49709  19_39_2012-11-16      16005.54\n",
            "49710  19_39_2012-11-23      16005.54\n",
            "49711  19_39_2012-11-30      16005.54\n",
            "49712  19_39_2012-12-07      16005.54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Score: 4901.43392"
      ],
      "metadata": {
        "id": "SxF51D7K3Ym1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "43mqnwxWseAx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}