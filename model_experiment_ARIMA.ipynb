{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJjZ+lkzl0a2MjqxtloheB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkhok22/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_ARIMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PDYnlCfGwHKE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "yaM9pdXpwFlO",
        "outputId": "b9c37919-96c9-4839-ad13-7a102084886c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlkhok22\u001b[0m (\u001b[33mlkhok22-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250727_073137-go2lmaed</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/go2lmaed' target=\"_blank\">arima-enhanced</a></strong> to <a href='https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/go2lmaed' target=\"_blank\">https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/go2lmaed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Data loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries and Initialize wandb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import glob\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initialize wandb\n",
        "import wandb\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"arima-enhanced\")\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Unzip and Load Data\n",
        "zip_path = \"/content/drive/MyDrive/ML-FinalProject/data.zip\"\n",
        "extract_path = \"/content/data\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "for zip_file in glob.glob(f\"{extract_path}/*.csv.zip\"):\n",
        "    with zipfile.ZipFile(zip_file, 'r') as z:\n",
        "        z.extractall(extract_path)\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(f\"{extract_path}/train.csv\")\n",
        "test = pd.read_csv(f\"{extract_path}/test.csv\")\n",
        "stores = pd.read_csv(f\"{extract_path}/stores.csv\")\n",
        "features = pd.read_csv(f\"{extract_path}/features.csv\")\n",
        "\n",
        "# Log data loading completion\n",
        "wandb.log({\"step\": \"data_loading\", \"status\": \"completed\"})\n",
        "print(\"Data loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Preprocessing\n",
        "# Convert Date column to datetime\n",
        "train['Date'] = pd.to_datetime(train['Date'])\n",
        "test['Date'] = pd.to_datetime(test['Date'])\n",
        "features['Date'] = pd.to_datetime(features['Date'])\n",
        "\n",
        "# Merge datasets\n",
        "train_merged = train.merge(features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "train_merged = train_merged.merge(stores, on='Store', how='left')\n",
        "test_merged = test.merge(features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "test_merged = test_merged.merge(stores, on='Store', how='left')\n",
        "\n",
        "# Handle missing values in features\n",
        "for col in ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']:\n",
        "    train_merged[col] = train_merged[col].fillna(train_merged[col].median())\n",
        "    test_merged[col] = test_merged[col].fillna(test_merged[col].median())\n",
        "\n",
        "# Add specific holiday flags\n",
        "holidays = {\n",
        "    'Super Bowl': ['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08'],\n",
        "    'Labor Day': ['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06'],\n",
        "    'Thanksgiving': ['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29'],\n",
        "    'Christmas': ['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27']\n",
        "}\n",
        "for holiday, dates in holidays.items():\n",
        "    dates = pd.to_datetime(dates)\n",
        "    train_merged[holiday] = train_merged['Date'].isin(dates).astype(int)\n",
        "    test_merged[holiday] = test_merged['Date'].isin(dates).astype(int)\n",
        "\n",
        "# Detect and cap outliers in Weekly_Sales\n",
        "q1 = train_merged['Weekly_Sales'].quantile(0.25)\n",
        "q3 = train_merged['Weekly_Sales'].quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "lower_bound = q1 - 1.5 * iqr\n",
        "upper_bound = q3 + 1.5 * iqr\n",
        "train_merged['Weekly_Sales'] = train_merged['Weekly_Sales'].clip(lower_bound, upper_bound)\n",
        "\n",
        "# Sort by Store, Dept, and Date\n",
        "train_merged = train_merged.sort_values(['Store', 'Dept', 'Date'])\n",
        "test_merged = test_merged.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "# Filter Store-Dept pairs with at least 10 observations\n",
        "train_grouped = train_merged.groupby(['Store', 'Dept']).size().reset_index(name='count')\n",
        "valid_pairs = train_grouped[train_grouped['count'] >= 10][['Store', 'Dept']]\n",
        "train_filtered = train_merged.merge(valid_pairs, on=['Store', 'Dept'], how='inner')\n",
        "\n",
        "# Handle missing Weekly_Sales\n",
        "train_filtered['Weekly_Sales'] = train_filtered.groupby(['Store', 'Dept'])['Weekly_Sales'].ffill()\n",
        "missing_sales = train_filtered['Weekly_Sales'].isna().sum()\n",
        "if missing_sales > 0:\n",
        "    mean_sales = train_filtered.groupby(['Store', 'Dept'])['Weekly_Sales'].transform('mean')\n",
        "    train_filtered['Weekly_Sales'] = train_filtered['Weekly_Sales'].fillna(mean_sales)\n",
        "\n",
        "# Log preprocessing\n",
        "wandb.log({\"step\": \"preprocessing\", \"missing_sales\": missing_sales, \"valid_pairs\": len(valid_pairs)})\n",
        "print(f\"Train filtered shape: {train_filtered.shape}, Valid pairs: {len(valid_pairs)}\")\n",
        "print(f\"Missing Weekly_Sales: {missing_sales}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBE37Vj5wIsh",
        "outputId": "a0ea423a-fab5-4987-95c0-71d932348b27"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train filtered shape: (420927, 20), Valid pairs: 3167\n",
            "Missing Weekly_Sales: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stationarity Check and Time Series Preparation\n",
        "# Function to check stationarity using ADF test\n",
        "def check_stationarity(series, store, dept):\n",
        "    try:\n",
        "        # Check if series is constant\n",
        "        if series.max() == series.min():\n",
        "            wandb.log({\n",
        "                f\"stationarity_store_{store}_dept_{dept}\": {\n",
        "                    \"p_value\": 1.0,  # High p-value for constant series\n",
        "                    \"is_stationary\": False,\n",
        "                    \"adf_statistic\": 0.0\n",
        "                }\n",
        "            })\n",
        "            return False\n",
        "        # Run ADF test\n",
        "        result = adfuller(series.dropna(), autolag='AIC')\n",
        "        p_value = result[1]\n",
        "        is_stationary = p_value < 0.05  # 5% significance level\n",
        "        wandb.log({\n",
        "            f\"stationarity_store_{store}_dept_{dept}\": {\n",
        "                \"p_value\": p_value,\n",
        "                \"is_stationary\": is_stationary,\n",
        "                \"adf_statistic\": result[0]\n",
        "            }\n",
        "        })\n",
        "        return is_stationary\n",
        "    except Exception as e:\n",
        "        print(f\"Stationarity check failed for Store {store}, Dept {dept}: {e}\")\n",
        "        wandb.log({\n",
        "            f\"stationarity_store_{store}_dept_{dept}\": {\n",
        "                \"p_value\": 1.0,\n",
        "                \"is_stationary\": False,\n",
        "                \"adf_statistic\": 0.0\n",
        "            }\n",
        "        })\n",
        "        return False\n",
        "\n",
        "# Create dictionary to store time series for each Store-Dept pair\n",
        "time_series_dict = {}\n",
        "\n",
        "# Iterate over valid Store-Dept pairs\n",
        "for _, row in valid_pairs.iterrows():\n",
        "    store, dept = row['Store'], row['Dept']\n",
        "    # Extract time series\n",
        "    ts = train_filtered[(train_filtered['Store'] == store) &\n",
        "                       (train_filtered['Dept'] == dept)][['Date', 'Weekly_Sales']]\n",
        "    # Set Date as index and ensure weekly frequency\n",
        "    ts = ts.set_index('Date')['Weekly_Sales']\n",
        "    ts.index = ts.index.to_period('W-FRI').to_timestamp('W-FRI')\n",
        "\n",
        "    # Check for sufficient data\n",
        "    if len(ts) >= 10:\n",
        "        # Check stationarity\n",
        "        is_stationary = check_stationarity(ts, store, dept)\n",
        "        time_series_dict[(store, dept)] = {\n",
        "            'series': ts,\n",
        "            'is_stationary': is_stationary\n",
        "        }\n",
        "\n",
        "# Count stationary and non-stationary series\n",
        "stationary_count = sum(1 for v in time_series_dict.values() if v['is_stationary'])\n",
        "print(f\"Number of stationary time series: {stationary_count}\")\n",
        "print(f\"Number of non-stationary time series: {len(time_series_dict) - stationary_count}\")\n",
        "print(f\"Total Store-Dept pairs prepared: {len(time_series_dict)}\")\n",
        "\n",
        "# Log stationarity check\n",
        "wandb.log({\n",
        "    \"step\": \"stationarity_check\",\n",
        "    \"stationary_series_count\": stationary_count,\n",
        "    \"total_series_count\": len(time_series_dict)\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-SM-5MExY4v",
        "outputId": "892fa146-7152-4f9e-f393-5673bd8f00ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of stationary time series: 2261\n",
            "Number of non-stationary time series: 906\n",
            "Total Store-Dept pairs prepared: 3167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ARIMA Training and Validation\n",
        "# Function to calculate WMAE\n",
        "def calculate_wmae(y_true, y_pred, is_holiday):\n",
        "    weights = np.where(is_holiday, 5, 1)\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "# Initialize storage for models and predictions\n",
        "val_predictions = []\n",
        "arima_models = {}\n",
        "batch_size = 500  # Process pairs in batches to manage runtime\n",
        "arima_orders = [(1,0,2), (0,1,2), (1,1,1), (0,1,1)]  # Orders to test\n",
        "subset_size = 100  # Subset for testing alternative orders\n",
        "\n",
        "# Global mean for fallback\n",
        "global_mean_sales = train_filtered['Weekly_Sales'].mean() if not train_filtered['Weekly_Sales'].isna().all() else 0.0\n",
        "\n",
        "# Process pairs in batches\n",
        "all_pairs = list(time_series_dict.keys())\n",
        "print(f\"Training ARIMA on {len(all_pairs)} Store-Dept pairs\")\n",
        "\n",
        "for batch_start in range(0, len(all_pairs), batch_size):\n",
        "    batch_pairs = all_pairs[batch_start:batch_start + batch_size]\n",
        "    print(f\"Processing batch {batch_start // batch_size + 1}/{len(all_pairs) // batch_size + 1}\")\n",
        "\n",
        "    for store, dept in batch_pairs:\n",
        "        ts = time_series_dict[(store, dept)]['series']\n",
        "\n",
        "        # Split into train (80%) and validation (20%)\n",
        "        dates = ts.index.sort_values()\n",
        "        train_size = int(0.8 * len(dates))\n",
        "        train_dates = dates[:train_size]\n",
        "        val_dates = dates[train_size:]\n",
        "        train_ts = ts[train_dates]\n",
        "        val_ts = ts[val_dates]\n",
        "\n",
        "        if len(train_ts) < 10 or len(val_ts) < 1:\n",
        "            continue\n",
        "\n",
        "        # Test multiple ARIMA orders on subset\n",
        "        orders_to_test = arima_orders if (store, dept) in all_pairs[:subset_size] else [(1,0,2)]\n",
        "\n",
        "        for order in orders_to_test:\n",
        "            try:\n",
        "                # Train ARIMA\n",
        "                model = ARIMA(train_ts, order=order).fit()\n",
        "                if order == (1,0,2):\n",
        "                    arima_models[(store, dept)] = model\n",
        "\n",
        "                # Forecast validation\n",
        "                val_pred = model.forecast(steps=len(val_ts))\n",
        "                val_actual = val_ts.values\n",
        "                val_holidays = train_filtered[(train_filtered['Store'] == store) &\n",
        "                                            (train_filtered['Dept'] == dept) &\n",
        "                                            (train_filtered['Date'].isin(val_dates))]['IsHoliday'].values\n",
        "\n",
        "                if len(val_holidays) != len(val_pred):\n",
        "                    print(f\"Holiday mismatch for Store {store}, Dept {dept}, order {order}\")\n",
        "                    continue\n",
        "\n",
        "                # Calculate WMAE\n",
        "                wmae = calculate_wmae(val_actual, val_pred, val_holidays)\n",
        "\n",
        "                # Log metrics\n",
        "                wandb.log({\n",
        "                    f\"wmae_store_{store}_dept_{dept}_order_{order}\": wmae,\n",
        "                    f\"aic_store_{store}_dept_{dept}_order_{order}\": model.aic\n",
        "                })\n",
        "\n",
        "                # Store predictions for (1,0,2)\n",
        "                if order == (1,0,2):\n",
        "                    for date, pred, actual, holiday in zip(val_ts.index, val_pred, val_actual, val_holidays):\n",
        "                        val_predictions.append({\n",
        "                            'Store': store,\n",
        "                            'Dept': dept,\n",
        "                            'Date': date,\n",
        "                            'Weekly_Sales_Pred': pred,\n",
        "                            'Weekly_Sales_Actual': actual,\n",
        "                            'IsHoliday': holiday\n",
        "                        })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"ARIMA failed for Store {store}, Dept {dept}, order {order}: {e}\")\n",
        "                # Fallback to last value\n",
        "                last_value = train_ts[-1] if len(train_ts) > 0 else global_mean_sales\n",
        "                val_pred = np.full(len(val_ts), last_value)\n",
        "                val_holidays = train_filtered[(train_filtered['Store'] == store) &\n",
        "                                            (train_filtered['Dept'] == dept) &\n",
        "                                            (train_filtered['Date'].isin(val_dates))]['IsHoliday'].values\n",
        "\n",
        "                if len(val_holidays) != len(val_pred):\n",
        "                    print(f\"Holiday mismatch in fallback for Store {store}, Dept {dept}\")\n",
        "                    continue\n",
        "\n",
        "                for date in zip(val_ts.index, val_pred, val_actual, val_holidays):\n",
        "                    val_predictions.append({\n",
        "                        'Store': store,\n",
        "                        'Dept': dept,\n",
        "                        'Date': date,\n",
        "                        'Weekly_Sales_Pred': val_pred,\n",
        "                        'Weekly_Sales_Actual': val_actual,\n",
        "                        'IsHoliday': holiday\n",
        "                    })\n",
        "\n",
        "# Convert predictions to DataFrame\n",
        "val_predictions_df = pd.DataFrame(val_predictions)\n",
        "\n",
        "# Validate and compute overall WMAE\n",
        "if val_predictions_df.empty:\n",
        "    print(\"Error: No valid predictions generated.\")\n",
        "else:\n",
        "    overall_wmae = calculate_wmae(val_predictions_df['Weekly_Sales_Actual'],\n",
        "                                 val_predictions_df['Weekly_Sales_Pred'],\n",
        "                                 val_predictions_df['IsHoliday'])\n",
        "    print(f\"Overall WMAE for ARIMA(1,0,2): {overall_wmae}\")\n",
        "    wandb.log({\"step\": \"arima_validation\", \"arima_order\": \"1,0,2\", \"overall_wmae\": overall_wmae})\n",
        "    print(\"Validation predictions shape:\", val_predictions_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgRNRn78xqLo",
        "outputId": "30574865-cdfb-4d78-eea4-5698444b783e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ARIMA on 3167 Store-Dept pairs\n",
            "Processing batch 1/7\n",
            "Processing batch 2/7\n",
            "Processing batch 3/7\n",
            "Processing batch 4/7\n",
            "Processing batch 5/7\n",
            "Processing batch 6/7\n",
            "Processing batch 7/7\n",
            "Overall WMAE for ARIMA(1,0,2): 1763.8795916807408\n",
            "Validation predictions shape: (85376, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Set Predictions and Submission\n",
        "# Global mean as fallback for missing data\n",
        "global_mean_sales = train_filtered['Weekly_Sales'].mean() if not train_filtered['Weekly_Sales'].isna().all() else 0.0\n",
        "print(f\"Global mean sales (fallback): {global_mean_sales}\")\n",
        "\n",
        "# Check departments in test set not in train set\n",
        "train_depts = set(train_filtered['Dept'].unique())\n",
        "test_depts = set(test_merged['Dept'].unique())\n",
        "missing_depts = test_depts - train_depts\n",
        "print(f\"Departments in test set but not in train set: {missing_depts}\")\n",
        "\n",
        "# Generate test set predictions\n",
        "test_predictions = []\n",
        "\n",
        "# Iterate over test set Store-Dept-Date triplets\n",
        "test_grouped = test_merged.groupby(['Store', 'Dept'])\n",
        "for (store, dept), group in test_grouped:\n",
        "    test_dates = group['Date'].sort_values()\n",
        "\n",
        "    # Initialize predictions\n",
        "    pred = None\n",
        "\n",
        "    # Check if we have a trained model\n",
        "    if (store, dept) in arima_models:\n",
        "        try:\n",
        "            model = arima_models[(store, dept)]\n",
        "            pred = model.forecast(steps=len(test_dates))\n",
        "        except Exception as e:\n",
        "            print(f\"Forecast failed for Store {store}, Dept {dept}: {e}\")\n",
        "\n",
        "    # If no model or forecast failed, use fallback\n",
        "    if pred is None or np.any(np.isnan(pred)) or np.any(np.isinf(pred)):\n",
        "        train_ts = time_series_dict.get((store, dept), {}).get('series', None)\n",
        "        if train_ts is not None and len(train_ts) > 0:\n",
        "            pred = np.full(len(test_dates), train_ts[-1])\n",
        "        else:\n",
        "            dept_mean = train_filtered[train_filtered['Dept'] == dept]['Weekly_Sales'].mean()\n",
        "            pred = np.full(len(test_dates), dept_mean if not np.isnan(dept_mean) else global_mean_sales)\n",
        "\n",
        "    # Ensure valid floats and round to two decimal places\n",
        "    pred = np.where(np.isnan(pred) | np.isinf(pred), global_mean_sales, pred)\n",
        "    pred = np.round(pred, 2).astype(float)\n",
        "\n",
        "    # Store predictions\n",
        "    for date, pred_sales in zip(test_dates, pred):\n",
        "        test_predictions.append({\n",
        "            'Store': store,\n",
        "            'Dept': dept,\n",
        "            'Date': date,\n",
        "            'Weekly_Sales': pred_sales\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame\n",
        "test_predictions_df = pd.DataFrame(test_predictions)\n",
        "\n",
        "# Create Id column in the format Store_Dept_Date\n",
        "test_predictions_df['Id'] = test_predictions_df.apply(\n",
        "    lambda x: f\"{int(x['Store'])}_{int(x['Dept'])}_{x['Date'].strftime('%Y-%m-%d')}\", axis=1)\n",
        "\n",
        "# Prepare submission file\n",
        "submission = test_predictions_df[['Id', 'Weekly_Sales']]\n",
        "submission['Weekly_Sales'] = submission['Weekly_Sales'].round(2)\n",
        "submission['Weekly_Sales'] = submission['Weekly_Sales'].fillna(global_mean_sales)\n",
        "submission['Weekly_Sales'] = submission['Weekly_Sales'].apply(lambda x: global_mean_sales if pd.isna(x) or np.isinf(x) else x)\n",
        "\n",
        "# Verify no invalid values\n",
        "print(\"Submission file shape:\", submission.shape)\n",
        "print(submission.head())\n",
        "print(\"\\nNaN values in Weekly_Sales:\", submission['Weekly_Sales'].isna().sum())\n",
        "print(\"Infinite values in Weekly_Sales:\", np.isinf(submission['Weekly_Sales']).sum())\n",
        "print(\"Empty or non-numeric values in Weekly_Sales:\",\n",
        "      submission['Weekly_Sales'].apply(lambda x: isinstance(x, str) and x.strip() == '').sum())\n",
        "\n",
        "# Check problematic rows from previous submission\n",
        "problem_lines = [4925, 9497, 36405, 47110, 49709, 49710, 49711, 49712, 49713, 49714]\n",
        "problem_rows = [line - 2 for line in problem_lines]\n",
        "print(\"\\nChecking problematic rows in new submission:\")\n",
        "print(submission.iloc[problem_rows])\n",
        "\n",
        "# Save submission file\n",
        "submission.to_csv('/content/submission.csv', index=False)\n",
        "\n",
        "# Log submission to wandb\n",
        "wandb.log({\"step\": \"test_predictions\", \"submission_file\": \"created\"})\n",
        "wandb.save('/content/submission.csv')\n",
        "print(\"Submission file saved: /content/submission.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlRlMpDIyOod",
        "outputId": "456d8e0a-e357-4a6c-c893-35cca9a15f44"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global mean sales (fallback): 13670.121618418989\n",
            "Departments in test set but not in train set: {np.int64(43), np.int64(77), np.int64(39)}\n",
            "Submission file shape: (115064, 2)\n",
            "               Id  Weekly_Sales\n",
            "0  1_1_2012-11-02      40687.77\n",
            "1  1_1_2012-11-09      27693.91\n",
            "2  1_1_2012-11-16      23990.66\n",
            "3  1_1_2012-11-23      23291.52\n",
            "4  1_1_2012-11-30      23159.53\n",
            "\n",
            "NaN values in Weekly_Sales: 0\n",
            "Infinite values in Weekly_Sales: 0\n",
            "Empty or non-numeric values in Weekly_Sales: 0\n",
            "\n",
            "Checking problematic rows in new submission:\n",
            "                     Id  Weekly_Sales\n",
            "4923    2_77_2013-01-11      13670.12\n",
            "9495    4_39_2013-07-12      13670.12\n",
            "36403  14_43_2012-12-07      13670.12\n",
            "47108  18_43_2013-04-26      13670.12\n",
            "49707  19_39_2012-11-02      13670.12\n",
            "49708  19_39_2012-11-09      13670.12\n",
            "49709  19_39_2012-11-16      13670.12\n",
            "49710  19_39_2012-11-23      13670.12\n",
            "49711  19_39_2012-11-30      13670.12\n",
            "49712  19_39_2012-12-07      13670.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file saved: /content/submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Score: 6765.77723"
      ],
      "metadata": {
        "id": "1q4ra5mCP9PF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J0d7A5H61bbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}