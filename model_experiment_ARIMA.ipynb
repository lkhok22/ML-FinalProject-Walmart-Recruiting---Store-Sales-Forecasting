{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyONvLGs9WDpKwLj3/Z5LzZe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkhok22/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_ARIMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PDYnlCfGwHKE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "yaM9pdXpwFlO",
        "outputId": "8c57808f-f069-4ce2-bd60-f259bef8372b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlkhok22\u001b[0m (\u001b[33mlkhok22-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250727_150219-zmo2hlxi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/zmo2hlxi' target=\"_blank\">arima-enhanced</a></strong> to <a href='https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/zmo2hlxi' target=\"_blank\">https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/zmo2hlxi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Data loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries and Initialize wandb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import glob\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initialize wandb\n",
        "import wandb\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"arima-enhanced\")\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Unzip and Load Data\n",
        "zip_path = \"/content/drive/MyDrive/ML-FinalProject/data.zip\"\n",
        "extract_path = \"/content/data\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "for zip_file in glob.glob(f\"{extract_path}/*.csv.zip\"):\n",
        "    with zipfile.ZipFile(zip_file, 'r') as z:\n",
        "        z.extractall(extract_path)\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(f\"{extract_path}/train.csv\")\n",
        "test = pd.read_csv(f\"{extract_path}/test.csv\")\n",
        "stores = pd.read_csv(f\"{extract_path}/stores.csv\")\n",
        "features = pd.read_csv(f\"{extract_path}/features.csv\")\n",
        "\n",
        "# Log data loading completion\n",
        "wandb.log({\"step\": \"data_loading\", \"status\": \"completed\"})\n",
        "print(\"Data loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "# Convert Date column to datetime\n",
        "train['Date'] = pd.to_datetime(train['Date'])\n",
        "test['Date'] = pd.to_datetime(test['Date'])\n",
        "features['Date'] = pd.to_datetime(features['Date'])\n",
        "\n",
        "# Merge datasets\n",
        "train_merged = train.merge(features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "train_merged = train_merged.merge(stores, on='Store', how='left')\n",
        "test_merged = test.merge(features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "test_merged = test_merged.merge(stores, on='Store', how='left')\n",
        "\n",
        "# Impute MarkDown columns with 0 and add missingness indicators\n",
        "markdown_cols = [f'MarkDown{i}' for i in range(1, 6)]\n",
        "for col in markdown_cols:\n",
        "    train_merged[f'{col}_was_missing'] = train_merged[col].isna().astype(int)\n",
        "    test_merged[f'{col}_was_missing'] = test_merged[col].isna().astype(int)\n",
        "    train_merged[col] = train_merged[col].fillna(0)\n",
        "    test_merged[col] = test_merged[col].fillna(0)\n",
        "\n",
        "# Impute numerical columns with ffill/bfill, fallback to mean\n",
        "numerical_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "means = {col: train_merged[col].mean() for col in numerical_cols}\n",
        "for col in numerical_cols:\n",
        "    train_merged[col] = train_merged[col].ffill().bfill()\n",
        "    test_merged[col] = test_merged[col].ffill().bfill()\n",
        "    train_merged[col] = train_merged[col].fillna(means[col])\n",
        "    test_merged[col] = test_merged[col].fillna(means[col])\n",
        "\n",
        "# Sort by Store, Dept, and Date\n",
        "train_merged = train_merged.sort_values(['Store', 'Dept', 'Date'])\n",
        "test_merged = test_merged.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "# Filter Store-Dept pairs with at least 10 observations\n",
        "train_grouped = train_merged.groupby(['Store', 'Dept']).size().reset_index(name='count')\n",
        "valid_pairs = train_grouped[train_grouped['count'] >= 10][['Store', 'Dept']]\n",
        "train_filtered = train_merged.merge(valid_pairs, on=['Store', 'Dept'], how='inner')\n",
        "\n",
        "# Handle missing Weekly_Sales\n",
        "train_filtered['Weekly_Sales'] = train_filtered.groupby(['Store', 'Dept'])['Weekly_Sales'].ffill().bfill()\n",
        "missing_sales = train_filtered['Weekly_Sales'].isna().sum()\n",
        "if missing_sales > 0:\n",
        "    mean_sales = train_filtered.groupby(['Store', 'Dept'])['Weekly_Sales'].transform('mean')\n",
        "    train_filtered['Weekly_Sales'] = train_filtered['Weekly_Sales'].fillna(mean_sales)\n",
        "\n",
        "# Log preprocessing\n",
        "wandb.log({\"step\": \"preprocessing\", \"missing_sales\": missing_sales, \"valid_pairs\": len(valid_pairs)})\n",
        "print(f\"Train filtered shape: {train_filtered.shape}, Valid pairs: {len(valid_pairs)}\")\n",
        "print(f\"Missing Weekly_Sales: {missing_sales}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBE37Vj5wIsh",
        "outputId": "d4bf99ac-d5da-4c18-f7b6-c765d9af5497"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train filtered shape: (420927, 21), Valid pairs: 3167\n",
            "Missing Weekly_Sales: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ARIMA Training, Validation, and Submission\n",
        "# Function to check stationarity\n",
        "def check_stationarity(series, store, dept):\n",
        "    try:\n",
        "        if series.max() == series.min():\n",
        "            wandb.log({f\"stationarity_store_{store}_dept_{dept}\": {\"p_value\": 1.0, \"is_stationary\": False, \"adf_statistic\": 0.0}})\n",
        "            return False\n",
        "        result = adfuller(series.dropna(), autolag='AIC')\n",
        "        p_value = result[1]\n",
        "        is_stationary = p_value < 0.05\n",
        "        wandb.log({f\"stationarity_store_{store}_dept_{dept}\": {\"p_value\": p_value, \"is_stationary\": is_stationary, \"adf_statistic\": result[0]}})\n",
        "        return is_stationary\n",
        "    except Exception as e:\n",
        "        print(f\"Stationarity check failed for Store {store}, Dept {dept}: {e}\")\n",
        "        wandb.log({f\"stationarity_store_{store}_dept_{dept}\": {\"p_value\": 1.0, \"is_stationary\": False, \"adf_statistic\": 0.0}})\n",
        "        return False\n",
        "\n",
        "# Prepare time series\n",
        "time_series_dict = {}\n",
        "for _, row in valid_pairs.iterrows():\n",
        "    store, dept = row['Store'], row['Dept']\n",
        "    ts = train_filtered[(train_filtered['Store'] == store) &\n",
        "                       (train_filtered['Dept'] == dept)][['Date', 'Weekly_Sales']]\n",
        "    ts = ts.set_index('Date')['Weekly_Sales'].asfreq('W-FRI')\n",
        "    if len(ts) >= 10:\n",
        "        is_stationary = check_stationarity(ts, store, dept)\n",
        "        time_series_dict[(store, dept)] = {'series': ts, 'is_stationary': is_stationary}\n",
        "\n",
        "stationary_count = sum(1 for v in time_series_dict.values() if v['is_stationary'])\n",
        "print(f\"Stationary series: {stationary_count}, Non-stationary: {len(time_series_dict) - stationary_count}, Total pairs: {len(time_series_dict)}\")\n",
        "wandb.log({\"step\": \"stationarity_check\", \"stationary_series_count\": stationary_count, \"total_series_count\": len(time_series_dict)})\n",
        "\n",
        "# Function to calculate WMAE\n",
        "def calculate_wmae(y_true, y_pred, is_holiday):\n",
        "    weights = np.where(is_holiday, 5, 1)\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "# Calculate department-specific holiday multipliers\n",
        "holiday_multipliers = {}\n",
        "for dept in train_filtered['Dept'].unique():\n",
        "    dept_data = train_filtered[train_filtered['Dept'] == dept]\n",
        "    holiday_sales = dept_data[dept_data['IsHoliday'] == 1]['Weekly_Sales'].mean()\n",
        "    non_holiday_sales = dept_data[dept_data['IsHoliday'] == 0]['Weekly_Sales'].mean()\n",
        "    if non_holiday_sales > 0:\n",
        "        holiday_multipliers[dept] = holiday_sales / non_holiday_sales if holiday_sales > 0 else 1.5\n",
        "    else:\n",
        "        holiday_multipliers[dept] = 1.5\n",
        "\n",
        "# Train and validate\n",
        "val_predictions = []\n",
        "arima_models = {}\n",
        "batch_size = 500\n",
        "arima_orders = [(1,0,2), (1,0,1), (0,1,2), (1,1,1)]\n",
        "subset_size = 100\n",
        "global_mean_sales = train_filtered['Weekly_Sales'].mean() if not train_filtered['Weekly_Sales'].isna().all() else 0.0\n",
        "\n",
        "all_pairs = list(time_series_dict.keys())\n",
        "print(f\"Training ARIMA on {len(all_pairs)} Store-Dept pairs\")\n",
        "\n",
        "# Date-based validation split\n",
        "validation_cutoff_date = pd.to_datetime('2012-09-01')\n",
        "\n",
        "for batch_start in range(0, len(all_pairs), batch_size):\n",
        "    batch_pairs = all_pairs[batch_start:batch_start + batch_size]\n",
        "    print(f\"Processing batch {batch_start // batch_size + 1}/{len(all_pairs) // batch_size + 1}\")\n",
        "\n",
        "    for store, dept in batch_pairs:\n",
        "        ts = time_series_dict[(store, dept)]['series']\n",
        "        train_ts = ts[ts.index < validation_cutoff_date]\n",
        "        val_ts = ts[ts.index >= validation_cutoff_date]\n",
        "\n",
        "        if len(train_ts) < 10 or len(val_ts) < 1:\n",
        "            continue\n",
        "\n",
        "        orders_to_test = arima_orders if (store, dept) in all_pairs[:subset_size] else [(1,0,2)]\n",
        "\n",
        "        for order in orders_to_test:\n",
        "            try:\n",
        "                model = ARIMA(train_ts, order=order).fit()\n",
        "                if order == (1,0,2):\n",
        "                    arima_models[(store, dept)] = model\n",
        "\n",
        "                val_pred = model.forecast(steps=len(val_ts))\n",
        "                val_actual = val_ts.values\n",
        "                val_dates = val_ts.index\n",
        "                val_holidays = train_filtered[(train_filtered['Store'] == store) &\n",
        "                                            (train_filtered['Dept'] == dept) &\n",
        "                                            (train_filtered['Date'].isin(val_dates))]['IsHoliday'].reindex(val_dates, fill_value=0).values\n",
        "\n",
        "                if len(val_holidays) != len(val_pred):\n",
        "                    print(f\"Holiday mismatch for Store {store}, Dept {dept}, order {order}, lengths: {len(val_holidays)} vs {len(val_pred)}\")\n",
        "                    continue\n",
        "\n",
        "                wmae = calculate_wmae(val_actual, val_pred, val_holidays)\n",
        "                wandb.log({\n",
        "                    f\"wmae_store_{store}_dept_{dept}_order_{order}\": wmae,\n",
        "                    f\"aic_store_{store}_dept_{dept}_order_{order}\": model.aic\n",
        "                })\n",
        "\n",
        "                if order == (1,0,2):\n",
        "                    for date, pred, actual, holiday in zip(val_dates, val_pred, val_actual, val_holidays):\n",
        "                        val_predictions.append({\n",
        "                            'Store': store,\n",
        "                            'Dept': dept,\n",
        "                            'Date': date,\n",
        "                            'Weekly_Sales_Pred': pred,\n",
        "                            'Weekly_Sales_Actual': actual,\n",
        "                            'IsHoliday': holiday\n",
        "                        })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"ARIMA failed for Store {store}, Dept {dept}, order {order}: {e}\")\n",
        "                last_value = train_ts[-1] if len(train_ts) > 0 else global_mean_sales\n",
        "                val_pred = np.full(len(val_ts), last_value)\n",
        "                val_holidays = train_filtered[(train_filtered['Store'] == store) &\n",
        "                                            (train_filtered['Dept'] == dept) &\n",
        "                                            (train_filtered['Date'].isin(val_dates))]['IsHoliday'].reindex(val_dates, fill_value=0).values\n",
        "\n",
        "                if len(val_holidays) != len(val_pred):\n",
        "                    print(f\"Holiday mismatch in fallback for Store {store}, Dept {dept}, lengths: {len(val_holidays)} vs {len(val_pred)}\")\n",
        "                    continue\n",
        "\n",
        "                if order == (1,0,2):\n",
        "                    for date, pred, actual, holiday in zip(val_dates, val_pred, val_actual, val_holidays):\n",
        "                        val_predictions.append({\n",
        "                            'Store': store,\n",
        "                            'Dept': dept,\n",
        "                            'Date': date,\n",
        "                            'Weekly_Sales_Pred': pred,\n",
        "                            'Weekly_Sales_Actual': actual,\n",
        "                            'IsHoliday': holiday\n",
        "                        })\n",
        "\n",
        "# Validate\n",
        "val_predictions_df = pd.DataFrame(val_predictions)\n",
        "if not val_predictions_df.empty:\n",
        "    overall_wmae = calculate_wmae(val_predictions_df['Weekly_Sales_Actual'],\n",
        "                                 val_predictions_df['Weekly_Sales_Pred'],\n",
        "                                 val_predictions_df['IsHoliday'])\n",
        "    print(f\"Overall WMAE for ARIMA(1,0,2): {overall_wmae}\")\n",
        "    wandb.log({\"step\": \"arima_validation\", \"arima_order\": \"1,0,2\", \"overall_wmae\": overall_wmae})\n",
        "else:\n",
        "    print(\"Error: No valid predictions generated.\")\n",
        "\n",
        "# Test set predictions\n",
        "test_predictions = []\n",
        "train_depts = set(train_filtered['Dept'].unique())\n",
        "test_depts = set(test_merged['Dept'].unique())\n",
        "missing_depts = test_depts - train_depts\n",
        "print(f\"Departments in test set but not in train set: {missing_depts}\")\n",
        "\n",
        "test_grouped = test_merged.groupby(['Store', 'Dept'])\n",
        "for (store, dept), group in test_grouped:\n",
        "    test_dates = group['Date'].sort_values()\n",
        "    holiday_adjust = group['IsHoliday'].values\n",
        "    pred = None\n",
        "\n",
        "    if (store, dept) in arima_models:\n",
        "        try:\n",
        "            model = arima_models[(store, dept)]\n",
        "            pred = model.forecast(steps=len(test_dates))\n",
        "        except Exception as e:\n",
        "            print(f\"Forecast failed for Store {store}, Dept {dept}: {e}\")\n",
        "\n",
        "    if pred is None or np.any(np.isnan(pred)) or np.any(np.isinf(pred)):\n",
        "        train_ts = time_series_dict.get((store, dept), {}).get('series', None)\n",
        "        if train_ts is not None and len(train_ts) > 0:\n",
        "            pred = np.full(len(test_dates), train_ts[-1])\n",
        "        else:\n",
        "            dept_median = train_filtered[train_filtered['Dept'] == dept]['Weekly_Sales'].median()\n",
        "            pred = np.full(len(test_dates), dept_median if not np.isnan(dept_median) else global_mean_sales)\n",
        "\n",
        "    # Department-specific holiday adjustment\n",
        "    holiday_multiplier = holiday_multipliers.get(dept, 1.5)\n",
        "    pred = np.where(holiday_adjust, pred * holiday_multiplier, pred)\n",
        "    pred = np.where(np.isnan(pred) | np.isinf(pred), global_mean_sales, pred)\n",
        "    pred = np.round(pred, 2).astype(float)\n",
        "\n",
        "    for date, pred_sales in zip(test_dates, pred):\n",
        "        test_predictions.append({\n",
        "            'Store': store,\n",
        "            'Dept': dept,\n",
        "            'Date': date,\n",
        "            'Weekly_Sales': pred_sales\n",
        "        })\n",
        "\n",
        "# Prepare submission\n",
        "test_predictions_df = pd.DataFrame(test_predictions)\n",
        "test_predictions_df['Id'] = test_predictions_df.apply(\n",
        "    lambda x: f\"{int(x['Store'])}_{int(x['Dept'])}_{x['Date'].strftime('%Y-%m-%d')}\", axis=1)\n",
        "submission = test_predictions_df[['Id', 'Weekly_Sales']]\n",
        "submission['Weekly_Sales'] = submission['Weekly_Sales'].round(2)\n",
        "submission['Weekly_Sales'] = submission['Weekly_Sales'].fillna(global_mean_sales)\n",
        "submission['Weekly_Sales'] = submission['Weekly_Sales'].apply(lambda x: global_mean_sales if pd.isna(x) or np.isinf(x) else x)\n",
        "\n",
        "# Verify submission\n",
        "print(\"Submission file shape:\", submission.shape)\n",
        "print(submission.head())\n",
        "print(\"\\nNaN values in Weekly_Sales:\", submission['Weekly_Sales'].isna().sum())\n",
        "print(\"Infinite values in Weekly_Sales:\", np.isinf(submission['Weekly_Sales']).sum())\n",
        "print(\"Empty or non-numeric values in Weekly_Sales:\",\n",
        "      submission['Weekly_Sales'].apply(lambda x: isinstance(x, str) and x.strip() == '').sum())\n",
        "problem_lines = [4925, 9497, 36405, 47110, 49709, 49710, 49711, 49712, 49713, 49714]\n",
        "problem_rows = [line - 2 for line in problem_lines]\n",
        "print(\"\\nChecking problematic rows:\")\n",
        "print(submission.iloc[problem_rows])\n",
        "\n",
        "# Save submission\n",
        "submission.to_csv('/content/submission.csv', index=False)\n",
        "wandb.log({\"step\": \"arima_submission\", \"submission_file\": \"created\"})\n",
        "wandb.save('/content/submission.csv')\n",
        "print(\"Submission file saved: /content/submission.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-SM-5MExY4v",
        "outputId": "9ec7ae58-175f-44c9-e26f-c2cf859dce00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stationary series: 2275, Non-stationary: 892, Total pairs: 3167\n",
            "Training ARIMA on 3167 Store-Dept pairs\n",
            "Processing batch 1/7\n",
            "Processing batch 2/7\n",
            "Processing batch 3/7\n",
            "Processing batch 4/7\n",
            "Processing batch 5/7\n",
            "Processing batch 6/7\n",
            "Processing batch 7/7\n",
            "Overall WMAE for ARIMA(1,0,2): 1931.0866113120837\n",
            "Departments in test set but not in train set: {np.int64(43), np.int64(77), np.int64(39)}\n",
            "Submission file shape: (115064, 2)\n",
            "               Id  Weekly_Sales\n",
            "0  1_1_2012-11-02      19996.11\n",
            "1  1_1_2012-11-09      22016.22\n",
            "2  1_1_2012-11-16      22484.27\n",
            "3  1_1_2012-11-23      23178.52\n",
            "4  1_1_2012-11-30      22568.45\n",
            "\n",
            "NaN values in Weekly_Sales: 0\n",
            "Infinite values in Weekly_Sales: 0\n",
            "Empty or non-numeric values in Weekly_Sales: 0\n",
            "\n",
            "Checking problematic rows:\n",
            "                     Id  Weekly_Sales\n",
            "4923    2_77_2013-01-11      16005.54\n",
            "9495    4_39_2013-07-12      16005.54\n",
            "36403  14_43_2012-12-07      16005.54\n",
            "47108  18_43_2013-04-26      16005.54\n",
            "49707  19_39_2012-11-02      16005.54\n",
            "49708  19_39_2012-11-09      16005.54\n",
            "49709  19_39_2012-11-16      16005.54\n",
            "49710  19_39_2012-11-23      24008.31\n",
            "49711  19_39_2012-11-30      16005.54\n",
            "49712  19_39_2012-12-07      16005.54\n",
            "Submission file saved: /content/submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Score: 4680.49545\n"
      ],
      "metadata": {
        "id": "1q4ra5mCP9PF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J0d7A5H61bbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}