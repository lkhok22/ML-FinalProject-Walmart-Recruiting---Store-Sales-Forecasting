{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqpnITjoFpooMMlu/2d1lb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkhok22/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_Ensemble_Trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ap6GhiVBmjXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57865ac7-e5ec-46fa-e0d3-8ffa45b679b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabakh22\u001b[0m (\u001b[33mabakh22-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "\n",
        "# Install required libraries\n",
        "!pip install pandas numpy matplotlib seaborn scikit-learn lightgbm xgboost wandb pyyaml --quiet\n",
        "import wandb\n",
        "wandb.login(key=\"eccf2c915699fc032ad678daf0fd4b5ac60bf87c\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive and extract data\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "zip_path = '/content/drive/MyDrive/ML-FinalProject/data.zip'\n",
        "extract_to = '/content/walmart_data/'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "for file_name in os.listdir(extract_to):\n",
        "    if file_name.endswith('.zip'):\n",
        "        with zipfile.ZipFile(os.path.join(extract_to, file_name), 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "print(\"✅ Extracted files:\", os.listdir(extract_to))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pPhBoE0OKrb",
        "outputId": "f93e3552-3e91-4ca7-d3ea-041290986ef2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Extracted files: ['test.csv.zip', 'stores.csv', 'sampleSubmission.csv', 'features.csv', 'test.csv', 'train.csv.zip', 'train.csv', 'sampleSubmission.csv.zip', 'features.csv.zip']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import wandb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "VgvWuWI6OApy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data_path = '/content/walmart_data/'\n",
        "train_df = pd.read_csv(data_path + 'train.csv')\n",
        "test_df = pd.read_csv(data_path + 'test.csv')\n",
        "features_df = pd.read_csv(data_path + 'features.csv')\n",
        "stores_df = pd.read_csv(data_path + 'stores.csv')\n",
        "\n",
        "# Merge datasets, keeping IsHoliday from features.csv\n",
        "train_df = train_df.merge(stores_df, on='Store').merge(\n",
        "    features_df, on=['Store', 'Date'], suffixes=('', '_features'), how='left'\n",
        ")\n",
        "test_df = test_df.merge(stores_df, on='Store').merge(\n",
        "    features_df, on=['Store', 'Date'], suffixes=('', '_features'), how='left'\n",
        ")\n",
        "\n",
        "# Drop redundant IsHoliday and rename\n",
        "train_df = train_df.drop(columns=['IsHoliday'], errors='ignore').rename(\n",
        "    columns={'IsHoliday_features': 'IsHoliday'}\n",
        ")\n",
        "test_df = test_df.drop(columns=['IsHoliday'], errors='ignore').rename(\n",
        "    columns={'IsHoliday_features': 'IsHoliday'}\n",
        ")\n",
        "\n",
        "# Convert Date to datetime\n",
        "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "\n",
        "# Handle missing values in MarkDown columns\n",
        "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "train_df[markdown_cols] = train_df[markdown_cols].fillna(0)\n",
        "test_df[markdown_cols] = test_df[markdown_cols].fillna(0)\n",
        "\n",
        "# Convert IsHoliday to numeric\n",
        "train_df['IsHoliday'] = train_df['IsHoliday'].astype(int)\n",
        "test_df['IsHoliday'] = test_df['IsHoliday'].astype(int)\n",
        "\n",
        "# Fill missing values in other columns\n",
        "numeric_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "for col in numeric_cols:\n",
        "    train_df[col] = train_df[col].fillna(train_df[col].mean())\n",
        "    test_df[col] = test_df[col].fillna(train_df[col].mean())\n",
        "\n",
        "def create_features(df):\n",
        "    \"\"\"Create time-based and lag features for tree-based models\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Time-based features\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Week'] = df['Date'].dt.isocalendar().week\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "    df['Quarter'] = df['Date'].dt.quarter\n",
        "    df['IsYearEnd'] = (df['Date'].dt.month == 12).astype(int)\n",
        "    df['IsYearStart'] = (df['Date'].dt.month == 1).astype(int)\n",
        "    df['IsMonthEnd'] = df['Date'].dt.is_month_end.astype(int)\n",
        "    df['IsMonthStart'] = df['Date'].dt.is_month_start.astype(int)\n",
        "\n",
        "    # Store-Department combination\n",
        "    df['Store_Dept'] = df['Store'].astype(str) + '_' + df['Dept'].astype(str)\n",
        "\n",
        "    # Encode categorical variables\n",
        "    le_type = LabelEncoder()\n",
        "    df['Type_encoded'] = le_type.fit_transform(df['Type'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Create features for both datasets\n",
        "train_df = create_features(train_df)\n",
        "test_df = create_features(test_df)\n",
        "\n",
        "def create_lag_features(df, target_col='Weekly_Sales', lags=[1, 2, 3, 4, 8, 12]):\n",
        "    \"\"\"Create lag features for time series\"\"\"\n",
        "    df = df.copy()\n",
        "    df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "    for lag in lags:\n",
        "        df[f'{target_col}_lag_{lag}'] = df.groupby(['Store', 'Dept'])[target_col].shift(lag)\n",
        "\n",
        "    # Rolling statistics\n",
        "    for window in [3, 4, 8]:\n",
        "        df[f'{target_col}_rolling_mean_{window}'] = df.groupby(['Store', 'Dept'])[target_col].transform(\n",
        "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
        "        )\n",
        "        df[f'{target_col}_rolling_std_{window}'] = df.groupby(['Store', 'Dept'])[target_col].transform(\n",
        "            lambda x: x.rolling(window=window, min_periods=1).std()\n",
        "        )\n",
        "\n",
        "    return df\n",
        "\n",
        "# Create lag features for training data\n",
        "train_df = create_lag_features(train_df)\n",
        "\n",
        "# For test data, we need to append it to train data to create proper lags\n",
        "combined_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)\n",
        "combined_df = combined_df.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "# Create lag features for combined data\n",
        "combined_df = create_lag_features(combined_df, target_col='Weekly_Sales')\n",
        "\n",
        "# Split back into train and test\n",
        "train_with_lags = combined_df[combined_df['Weekly_Sales'].notna()].copy()\n",
        "test_with_lags = combined_df[combined_df['Weekly_Sales'].isna()].copy()\n",
        "\n",
        "# Define feature columns\n",
        "feature_cols = [\n",
        "    'Store', 'Dept', 'Size', 'Type_encoded',\n",
        "    'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "    'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n",
        "    'IsHoliday', 'Year', 'Month', 'Week', 'Day', 'DayOfWeek', 'Quarter',\n",
        "    'IsYearEnd', 'IsYearStart', 'IsMonthEnd', 'IsMonthStart'\n",
        "]\n",
        "\n",
        "# Add lag features to feature columns\n",
        "lag_cols = [col for col in train_with_lags.columns if 'lag_' in col or 'rolling_' in col]\n",
        "feature_cols.extend(lag_cols)\n",
        "\n",
        "# Remove rows with missing lag features for training\n",
        "train_with_lags = train_with_lags.dropna(subset=lag_cols)\n",
        "\n",
        "# Fill missing lag features in test set with 0 (or forward fill)\n",
        "test_with_lags[lag_cols] = test_with_lags[lag_cols].fillna(0)\n",
        "\n",
        "# Prepare training data\n",
        "X_train = train_with_lags[feature_cols]\n",
        "y_train = train_with_lags['Weekly_Sales']\n",
        "\n",
        "# Prepare test data\n",
        "X_test = test_with_lags[feature_cols]\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}\")\n",
        "print(f\"Features: {len(feature_cols)}\")\n",
        "\n",
        "# Time series split for validation\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "train_indices, val_indices = list(tscv.split(X_train))[-1]  # Use last split\n",
        "\n",
        "X_train_split = X_train.iloc[train_indices]\n",
        "y_train_split = y_train.iloc[train_indices]\n",
        "X_val_split = X_train.iloc[val_indices]\n",
        "y_val_split = y_train.iloc[val_indices]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_GZGKIAOD6x",
        "outputId": "580d7c0a-9956-4995-d058-7475e545e435"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (382955, 36)\n",
            "Test data shape: (115064, 36)\n",
            "Features: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'LightGBM': lgb.LGBMRegressor(\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=8,\n",
        "        num_leaves=31,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbose=-1\n",
        "    ),\n",
        "    'XGBoost': xgb.XGBRegressor(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=8,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.1,  # L1 regularization\n",
        "        reg_lambda=0.1,  # L2 regularization\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbosity=0\n",
        "    ),\n",
        "    'RandomForest': RandomForestRegressor(\n",
        "        n_estimators=200,\n",
        "        max_depth=15,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "}\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"walmart-sales-forecasting-trees\", config={\n",
        "    \"models\": list(models.keys()),\n",
        "    \"features\": len(feature_cols),\n",
        "    \"train_size\": len(X_train_split),\n",
        "    \"val_size\": len(X_val_split)\n",
        "})\n",
        "\n",
        "print(\"🚀 Starting model training...\")\n",
        "\n",
        "# Train models and get validation predictions\n",
        "val_predictions = {}\n",
        "model_objects = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "\n",
        "    if name == 'LightGBM':\n",
        "        model.fit(\n",
        "            X_train_split, y_train_split,\n",
        "            eval_set=[(X_val_split, y_val_split)],\n",
        "            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
        "        )\n",
        "    elif name == 'XGBoost':\n",
        "        model.fit(\n",
        "            X_train_split, y_train_split,\n",
        "            eval_set=[(X_val_split, y_val_split)],\n",
        "            verbose=False\n",
        "        )\n",
        "    else:  # RandomForest\n",
        "        model.fit(X_train_split, y_train_split)\n",
        "\n",
        "    # Validation predictions\n",
        "    val_pred = model.predict(X_val_split)\n",
        "    val_predictions[name] = val_pred\n",
        "    model_objects[name] = model\n",
        "\n",
        "    # Calculate validation MAE\n",
        "    val_mae = mean_absolute_error(y_val_split, val_pred)\n",
        "    val_rmse = np.sqrt(mean_squared_error(y_val_split, val_pred))\n",
        "\n",
        "    print(f\"{name} - Validation MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "    # Log to wandb\n",
        "    wandb.log({\n",
        "        f\"val_{name}_MAE\": val_mae,\n",
        "        f\"val_{name}_RMSE\": val_rmse\n",
        "    })\n",
        "\n",
        "# Create ensemble prediction (simple average)\n",
        "ensemble_val_pred = np.mean(list(val_predictions.values()), axis=0)\n",
        "ensemble_mae = mean_absolute_error(y_val_split, ensemble_val_pred)\n",
        "ensemble_rmse = np.sqrt(mean_squared_error(y_val_split, ensemble_val_pred))\n",
        "\n",
        "print(f\"Ensemble - Validation MAE: {ensemble_mae:.4f}, RMSE: {ensemble_rmse:.4f}\")\n",
        "wandb.log({\n",
        "    \"val_ensemble_MAE\": ensemble_mae,\n",
        "    \"val_ensemble_RMSE\": ensemble_rmse\n",
        "})\n",
        "\n",
        "print(\"✅ Validation completed!\")\n",
        "\n",
        "# Train final models on full training data\n",
        "print(\"🚀 Training final models on full dataset...\")\n",
        "\n",
        "final_models = {}\n",
        "final_predictions = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Final training {name}...\")\n",
        "\n",
        "    # Create fresh model instance\n",
        "    if name == 'LightGBM':\n",
        "        final_model = lgb.LGBMRegressor(\n",
        "            n_estimators=1000,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=8,\n",
        "            num_leaves=31,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            verbose=-1\n",
        "        )\n",
        "        final_model.fit(X_train, y_train)\n",
        "    elif name == 'XGBoost':\n",
        "        final_model = xgb.XGBRegressor(\n",
        "            n_estimators=500,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=8,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            reg_alpha=0.1,\n",
        "            reg_lambda=0.1,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            verbosity=0\n",
        "        )\n",
        "        final_model.fit(X_train, y_train)\n",
        "    else:  # RandomForest\n",
        "        final_model = RandomForestRegressor(\n",
        "            n_estimators=200,\n",
        "            max_depth=15,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=2,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        final_model.fit(X_train, y_train)\n",
        "\n",
        "    # Generate test predictions\n",
        "    test_pred = final_model.predict(X_test)\n",
        "    final_predictions[name] = test_pred\n",
        "    final_models[name] = final_model\n",
        "\n",
        "# Create final ensemble prediction\n",
        "final_ensemble_pred = np.mean(list(final_predictions.values()), axis=0)\n",
        "\n",
        "# Prepare submission\n",
        "submission_df = test_df[['Store', 'Dept', 'Date']].copy()\n",
        "\n",
        "# Create submission ID\n",
        "submission_df['Id'] = (submission_df['Store'].astype(str) + '_' +\n",
        "                      submission_df['Dept'].astype(str) + '_' +\n",
        "                      submission_df['Date'].dt.strftime('%Y-%m-%d'))\n",
        "\n",
        "# Add ensemble predictions\n",
        "submission_df['Weekly_Sales'] = final_ensemble_pred\n",
        "\n",
        "# Ensure non-negative predictions\n",
        "submission_df['Weekly_Sales'] = np.maximum(submission_df['Weekly_Sales'], 0)\n",
        "\n",
        "# Final submission format\n",
        "final_submission = submission_df[['Id', 'Weekly_Sales']].copy()\n",
        "\n",
        "print(\"📊 Submission Summary:\")\n",
        "print(f\"Total predictions: {len(final_submission)}\")\n",
        "print(f\"Mean prediction: {final_submission['Weekly_Sales'].mean():.2f}\")\n",
        "print(f\"Std prediction: {final_submission['Weekly_Sales'].std():.2f}\")\n",
        "print(f\"Min prediction: {final_submission['Weekly_Sales'].min():.2f}\")\n",
        "print(f\"Max prediction: {final_submission['Weekly_Sales'].max():.2f}\")\n",
        "\n",
        "# Feature importance analysis\n",
        "print(\"\\n📈 Feature Importance (LightGBM):\")\n",
        "lgb_model = final_models['LightGBM']\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': lgb_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "# Save submission\n",
        "final_submission.to_csv('/content/walmart_submission_trees.csv', index=False)\n",
        "print(\"✅ Submission saved to /content/walmart_submission_trees.csv\")\n",
        "\n",
        "# Log final metrics\n",
        "wandb.log({\n",
        "    \"total_predictions\": len(final_submission),\n",
        "    \"mean_prediction\": final_submission['Weekly_Sales'].mean(),\n",
        "    \"std_prediction\": final_submission['Weekly_Sales'].std(),\n",
        "    \"min_prediction\": final_submission['Weekly_Sales'].min(),\n",
        "    \"max_prediction\": final_submission['Weekly_Sales'].max()\n",
        "})\n",
        "\n",
        "# Display first few predictions\n",
        "print(\"\\n📋 First 10 predictions:\")\n",
        "print(final_submission.head(10))\n",
        "\n",
        "# Save individual model predictions for analysis\n",
        "individual_predictions = pd.DataFrame({\n",
        "    'Id': submission_df['Id'],\n",
        "    'LightGBM': final_predictions['LightGBM'],\n",
        "    'XGBoost': final_predictions['XGBoost'],\n",
        "    'RandomForest': final_predictions['RandomForest'],\n",
        "    'Ensemble': final_ensemble_pred\n",
        "})\n",
        "\n",
        "individual_predictions.to_csv('/content/individual_predictions.csv', index=False)\n",
        "print(\"✅ Individual predictions saved to /content/individual_predictions.csv\")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IVZVJoJnORuv",
        "outputId": "bd27d018-a68b-490e-8495-4867fe342aff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>val_LightGBM_MAE</td><td>▁</td></tr><tr><td>val_LightGBM_RMSE</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>val_LightGBM_MAE</td><td>364.54116</td></tr><tr><td>val_LightGBM_RMSE</td><td>1367.99121</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gentle-night-1</strong> at: <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting-trees/runs/v0acgp4d' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting-trees/runs/v0acgp4d</a><br> View project at: <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting-trees' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting-trees</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250729_131008-v0acgp4d/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250729_131558-i0w258sn</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting-trees/runs/i0w258sn' target=\"_blank\">usual-cloud-2</a></strong> to <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting-trees' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting-trees' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting-trees</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting-trees/runs/i0w258sn' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting-trees/runs/i0w258sn</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting model training...\n",
            "Training LightGBM...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\tvalid_0's l2: 1.8714e+06\n",
            "LightGBM - Validation MAE: 364.5412, RMSE: 1367.9912\n",
            "Training XGBoost...\n",
            "XGBoost - Validation MAE: 334.3273, RMSE: 1577.1431\n",
            "Training RandomForest...\n",
            "RandomForest - Validation MAE: 258.4051, RMSE: 1378.3604\n",
            "Ensemble - Validation MAE: 283.7097, RMSE: 1329.0928\n",
            "✅ Validation completed!\n",
            "🚀 Training final models on full dataset...\n",
            "Final training LightGBM...\n",
            "Final training XGBoost...\n",
            "Final training RandomForest...\n",
            "📊 Submission Summary:\n",
            "Total predictions: 115064\n",
            "Mean prediction: 942.93\n",
            "Std prediction: 6354.92\n",
            "Min prediction: 0.00\n",
            "Max prediction: 193788.49\n",
            "\n",
            "📈 Feature Importance (LightGBM):\n",
            "                        feature  importance\n",
            "30  Weekly_Sales_rolling_mean_3        4657\n",
            "25           Weekly_Sales_lag_2        3796\n",
            "24           Weekly_Sales_lag_1        3768\n",
            "31   Weekly_Sales_rolling_std_3        2761\n",
            "16                         Week        1373\n",
            "17                          Day        1229\n",
            "32  Weekly_Sales_rolling_mean_4        1215\n",
            "26           Weekly_Sales_lag_3        1156\n",
            "1                          Dept        1019\n",
            "33   Weekly_Sales_rolling_std_4         885\n",
            "✅ Submission saved to /content/walmart_submission_trees.csv\n",
            "\n",
            "📋 First 10 predictions:\n",
            "               Id  Weekly_Sales\n",
            "0  1_1_2012-11-02  26005.499110\n",
            "1  1_1_2012-11-09  31825.089923\n",
            "2  1_1_2012-11-16   3577.314543\n",
            "3  1_1_2012-11-23    161.118449\n",
            "4  1_1_2012-11-30    162.392480\n",
            "5  1_1_2012-12-07    155.771383\n",
            "6  1_1_2012-12-14    125.385633\n",
            "7  1_1_2012-12-21     30.076631\n",
            "8  1_1_2012-12-28     77.734902\n",
            "9  1_1_2013-01-04    149.517147\n",
            "✅ Individual predictions saved to /content/individual_predictions.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>max_prediction</td><td>▁</td></tr><tr><td>mean_prediction</td><td>▁</td></tr><tr><td>min_prediction</td><td>▁</td></tr><tr><td>std_prediction</td><td>▁</td></tr><tr><td>total_predictions</td><td>▁</td></tr><tr><td>val_LightGBM_MAE</td><td>▁</td></tr><tr><td>val_LightGBM_RMSE</td><td>▁</td></tr><tr><td>val_RandomForest_MAE</td><td>▁</td></tr><tr><td>val_RandomForest_RMSE</td><td>▁</td></tr><tr><td>val_XGBoost_MAE</td><td>▁</td></tr><tr><td>val_XGBoost_RMSE</td><td>▁</td></tr><tr><td>val_ensemble_MAE</td><td>▁</td></tr><tr><td>val_ensemble_RMSE</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>max_prediction</td><td>193788.49278</td></tr><tr><td>mean_prediction</td><td>942.92583</td></tr><tr><td>min_prediction</td><td>0</td></tr><tr><td>std_prediction</td><td>6354.91883</td></tr><tr><td>total_predictions</td><td>115064</td></tr><tr><td>val_LightGBM_MAE</td><td>364.54116</td></tr><tr><td>val_LightGBM_RMSE</td><td>1367.99121</td></tr><tr><td>val_RandomForest_MAE</td><td>258.40506</td></tr><tr><td>val_RandomForest_RMSE</td><td>1378.36039</td></tr><tr><td>val_XGBoost_MAE</td><td>334.32732</td></tr><tr><td>val_XGBoost_RMSE</td><td>1577.14308</td></tr><tr><td>val_ensemble_MAE</td><td>283.70968</td></tr><tr><td>val_ensemble_RMSE</td><td>1329.09281</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">usual-cloud-2</strong> at: <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting-trees/runs/i0w258sn' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting-trees/runs/i0w258sn</a><br> View project at: <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting-trees' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting-trees</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250729_131558-i0w258sn/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the individual predictions file\n",
        "individual_predictions = pd.read_csv('/content/individual_predictions.csv')\n",
        "\n",
        "# Create final submission with only Id and Ensemble predictions\n",
        "final_submission_clean = individual_predictions[['Id', 'Ensemble']].copy()\n",
        "final_submission_clean = final_submission_clean.rename(columns={'Ensemble': 'Weekly_Sales'})\n",
        "\n",
        "# Save the clean submission file\n",
        "final_submission_clean.to_csv('/content/walmart_submission_final.csv', index=False)\n",
        "\n",
        "print(\"✅ Clean submission file saved to /content/walmart_submission_final.csv\")\n",
        "print(f\"Columns: {list(final_submission_clean.columns)}\")\n",
        "print(f\"Shape: {final_submission_clean.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZq1NfI-PNRI",
        "outputId": "0f0dc371-2043-477e-baab-cbc4d58980cc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Clean submission file saved to /content/walmart_submission_final.csv\n",
            "Columns: ['Id', 'Weekly_Sales']\n",
            "Shape: (115064, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uyUJEAjunO1G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}