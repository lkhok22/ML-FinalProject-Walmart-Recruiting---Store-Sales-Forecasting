{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f3fc7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T17:34:29.393146Z",
     "iopub.status.busy": "2025-07-06T17:34:29.392807Z",
     "iopub.status.idle": "2025-07-06T17:35:02.357841Z",
     "shell.execute_reply": "2025-07-06T17:35:02.356759Z"
    },
    "papermill": {
     "duration": 32.972672,
     "end_time": "2025-07-06T17:35:02.360109",
     "exception": false,
     "start_time": "2025-07-06T17:34:29.387437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "ydata-profiling 4.16.1 requires dacite>=1.8, but you have dacite 1.6.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting mlflow\r\n",
      "  Downloading mlflow-3.1.1-py3-none-any.whl.metadata (29 kB)\r\n",
      "Collecting mlflow-skinny==3.1.1 (from mlflow)\r\n",
      "  Downloading mlflow_skinny-3.1.1-py3-none-any.whl.metadata (30 kB)\r\n",
      "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\r\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.2)\r\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\r\n",
      "Collecting graphene<4 (from mlflow)\r\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\r\n",
      "Collecting gunicorn<24 (from mlflow)\r\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\r\n",
      "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7.2)\r\n",
      "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\r\n",
      "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.3)\r\n",
      "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (19.0.1)\r\n",
      "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.2.2)\r\n",
      "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.2)\r\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.40)\r\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.5.2)\r\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.1.8)\r\n",
      "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.1)\r\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.1->mlflow)\r\n",
      "  Downloading databricks_sdk-0.57.0-py3-none-any.whl.metadata (39 kB)\r\n",
      "Collecting fastapi<1 (from mlflow-skinny==3.1.1->mlflow)\r\n",
      "  Downloading fastapi-0.115.14-py3-none-any.whl.metadata (27 kB)\r\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.44)\r\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.7.0)\r\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (1.31.1)\r\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (1.31.1)\r\n",
      "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (25.0)\r\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.20.3)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.11.4)\r\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (6.0.2)\r\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.32.3)\r\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.5.3)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (4.13.2)\r\n",
      "Collecting uvicorn<1 (from mlflow-skinny==3.1.1->mlflow)\r\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\r\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\r\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\r\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.6)\r\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\r\n",
      "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\r\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.6)\r\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\r\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.57.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.1.0)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.0.9)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2.4.1)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.5.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\r\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\r\n",
      "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (2.40.1)\r\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1->mlflow-skinny==3.1.1->mlflow)\r\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (4.0.12)\r\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.21.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask<4->mlflow) (3.0.2)\r\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow) (1.2.18)\r\n",
      "Collecting importlib_metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==3.1.1->mlflow)\r\n",
      "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\r\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow) (0.52b1)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (2.33.2)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.4.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (2025.4.26)\r\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.1->mlflow) (0.14.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3->mlflow) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3->mlflow) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3->mlflow) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3->mlflow) (2024.2.0)\r\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow) (1.17.2)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (5.0.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.2)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (4.9.1)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3->mlflow) (2024.2.0)\r\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (4.9.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (1.3.1)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.6.1)\r\n",
      "Downloading mlflow-3.1.1-py3-none-any.whl (24.7 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading mlflow_skinny-3.1.1-py3-none-any.whl (1.9 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading databricks_sdk-0.57.0-py3-none-any.whl (733 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading fastapi-0.115.14-py3-none-any.whl (95 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.5/95.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\r\n",
      "Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\r\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: uvicorn, importlib_metadata, gunicorn, graphql-relay, starlette, graphene, fastapi, databricks-sdk, mlflow-skinny, mlflow\r\n",
      "  Attempting uninstall: importlib_metadata\r\n",
      "    Found existing installation: importlib_metadata 8.7.0\r\n",
      "    Uninstalling importlib_metadata-8.7.0:\r\n",
      "      Successfully uninstalled importlib_metadata-8.7.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed databricks-sdk-0.57.0 fastapi-0.115.14 graphene-3.4.3 graphql-relay-3.2.0 gunicorn-23.0.0 importlib_metadata-8.6.1 mlflow-3.1.1 mlflow-skinny-3.1.1 starlette-0.46.2 uvicorn-0.35.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">â—â—â— AUTHORIZATION REQUIRED â—â—â—</span>                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                       \u001b[1mâ—â—â— AUTHORIZATION REQUIRED â—â—â—\u001b[0m                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Open the following link in your browser to authorize the client:\n",
      "https://dagshub.com/login/oauth/authorize?state=1b45fe80-3535-40e4-a101-c9126b3514bd&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=91161416661c9929f1624edbd0399d8c9a4b5e411703f3c635b84122995b32c8\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf04b8264f64259bdb3a12f9a780ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as AleksandreBakhtadze\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as AleksandreBakhtadze\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"AleksandreBakhtadze/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"AleksandreBakhtadze/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository AleksandreBakhtadze/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository AleksandreBakhtadze/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install required logging tools\n",
    "%pip install -q dagshub \n",
    "%pip install -U mlflow\n",
    "\n",
    "# Initialize DagsHub MLflow integration\n",
    "import dagshub\n",
    "dagshub.init(repo_owner='AleksandreBakhtadze', repo_name='ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting', mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be379e6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-06T17:35:02.768351Z",
     "iopub.status.busy": "2025-07-06T17:35:02.768113Z",
     "iopub.status.idle": "2025-07-06T17:35:04.773106Z",
     "shell.execute_reply": "2025-07-06T17:35:04.772013Z"
    },
    "papermill": {
     "duration": 2.403647,
     "end_time": "2025-07-06T17:35:04.774848",
     "exception": false,
     "start_time": "2025-07-06T17:35:02.371201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\n",
      "/kaggle/input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip\n",
      "/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\n",
      "/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\n",
      "/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cdde8fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T17:35:04.792015Z",
     "iopub.status.busy": "2025-07-06T17:35:04.790943Z",
     "iopub.status.idle": "2025-07-06T17:35:08.874508Z",
     "shell.execute_reply": "2025-07-06T17:35:08.873425Z"
    },
    "papermill": {
     "duration": 4.093873,
     "end_time": "2025-07-06T17:35:08.876165",
     "exception": false,
     "start_time": "2025-07-06T17:35:04.782292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab7904c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T17:35:08.891556Z",
     "iopub.status.busy": "2025-07-06T17:35:08.891005Z",
     "iopub.status.idle": "2025-07-06T17:35:09.328245Z",
     "shell.execute_reply": "2025-07-06T17:35:09.327211Z"
    },
    "papermill": {
     "duration": 0.446605,
     "end_time": "2025-07-06T17:35:09.329857",
     "exception": false,
     "start_time": "2025-07-06T17:35:08.883252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to read zipped CSVs\n",
    "def read_zipped_csv(path):\n",
    "    with zipfile.ZipFile(path) as z:\n",
    "        file_name = z.namelist()[0]\n",
    "        return pd.read_csv(z.open(file_name))\n",
    "\n",
    "# Load data\n",
    "train_df = read_zipped_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip')\n",
    "features_df = read_zipped_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip')\n",
    "stores_df = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv')\n",
    "test_df = read_zipped_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c70e7c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T17:35:09.346125Z",
     "iopub.status.busy": "2025-07-06T17:35:09.345289Z",
     "iopub.status.idle": "2025-07-06T17:35:09.355771Z",
     "shell.execute_reply": "2025-07-06T17:35:09.354867Z"
    },
    "papermill": {
     "duration": 0.020165,
     "end_time": "2025-07-06T17:35:09.357106",
     "exception": false,
     "start_time": "2025-07-06T17:35:09.336941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_datasets(X, features_df, stores_df):\n",
    "    df = X.copy()\n",
    "    df_full = df.merge(features_df, on=[\"Store\", \"Date\"], how=\"left\")\n",
    "    df_full = df_full.merge(stores_df, on=\"Store\", how=\"left\")\n",
    "    df_full = df_full.drop(columns=['IsHoliday_y'])\n",
    "    df_full = df_full.rename(columns={'IsHoliday_x': 'IsHoliday'})\n",
    "    return df_full\n",
    "\n",
    "def add_markdown_indicators(df):\n",
    "    df = df.copy()\n",
    "    for i in range(1, 6):\n",
    "        df[f'MarkDown{i}_Missing'] = df[f'MarkDown{i}'].isnull().astype(int)\n",
    "        print(f\"Created column: MarkDown{i}_Missing\")\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df = df.copy()\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Week'] = df['Date'].dt.isocalendar().week.astype(int)\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
    "    df['Type'] = df['Type'].map({'A': 0, 'B': 1, 'C': 2})\n",
    "    df = add_markdown_indicators(df)\n",
    "    return df\n",
    "\n",
    "def convert_date(df):\n",
    "    df = df.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "    return df\n",
    "\n",
    "class ForwardFillImputer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        return X.ffill()\n",
    "\n",
    "# Custom transformer to handle merging with external data\n",
    "class DatasetMerger(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_df, stores_df):\n",
    "        self.features_df = features_df\n",
    "        self.stores_df = stores_df\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return merge_datasets(X, self.features_df, self.stores_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c81de52c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T17:35:09.372534Z",
     "iopub.status.busy": "2025-07-06T17:35:09.372150Z",
     "iopub.status.idle": "2025-07-06T17:35:09.395142Z",
     "shell.execute_reply": "2025-07-06T17:35:09.394034Z"
    },
    "papermill": {
     "duration": 0.032632,
     "end_time": "2025-07-06T17:35:09.396831",
     "exception": false,
     "start_time": "2025-07-06T17:35:09.364199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df['Date'].dtype: object\n",
      "features_df['Date'].dtype: datetime64[ns]\n",
      "test_df['Date'].dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert features_df['Date'] to datetime (auxiliary data, not test set)\n",
    "try:\n",
    "    features_df['Date'] = pd.to_datetime(features_df['Date'], format='%Y-%m-%d')\n",
    "except Exception as e:\n",
    "    print(f\"features_df['Date'] conversion failed: {str(e)}\")\n",
    "    print(\"features_df['Date'].head():\", features_df['Date'].head().tolist())\n",
    "    raise\n",
    "\n",
    "# Verify dtypes\n",
    "print(\"train_df['Date'].dtype:\", train_df['Date'].dtype)\n",
    "print(\"features_df['Date'].dtype:\", features_df['Date'].dtype)\n",
    "print(\"test_df['Date'].dtype:\", test_df['Date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef0dc10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T17:35:09.414091Z",
     "iopub.status.busy": "2025-07-06T17:35:09.413751Z",
     "iopub.status.idle": "2025-07-06T17:35:09.418813Z",
     "shell.execute_reply": "2025-07-06T17:35:09.417864Z"
    },
    "papermill": {
     "duration": 0.015967,
     "end_time": "2025-07-06T17:35:09.420639",
     "exception": false,
     "start_time": "2025-07-06T17:35:09.404672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature columns\n",
    "feature_cols = [\n",
    "    'Store', 'Dept', 'Type', 'Size',\n",
    "    'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
    "    'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n",
    "    'MarkDown1_Missing', 'MarkDown2_Missing', 'MarkDown3_Missing', 'MarkDown4_Missing', 'MarkDown5_Missing',\n",
    "    'IsHoliday', 'Year', 'Month', 'Week'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "452540d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T17:35:09.437079Z",
     "iopub.status.busy": "2025-07-06T17:35:09.436712Z",
     "iopub.status.idle": "2025-07-06T17:35:09.443034Z",
     "shell.execute_reply": "2025-07-06T17:35:09.441676Z"
    },
    "papermill": {
     "duration": 0.017054,
     "end_time": "2025-07-06T17:35:09.445143",
     "exception": false,
     "start_time": "2025-07-06T17:35:09.428089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_ffill', ForwardFillImputer(), ['CPI', 'Unemployment']),\n",
    "        ('markdown_fill', SimpleImputer(strategy='constant', fill_value=0), \n",
    "         ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']),\n",
    "        ('scale', StandardScaler(), ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size']),\n",
    "        ('passthrough', 'passthrough', \n",
    "         ['Store', 'Dept', 'Type', 'IsHoliday', 'Year', 'Month', 'Week',\n",
    "          'MarkDown1_Missing', 'MarkDown2_Missing', 'MarkDown3_Missing', 'MarkDown4_Missing', 'MarkDown5_Missing'])\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01ed87db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T17:35:09.462301Z",
     "iopub.status.busy": "2025-07-06T17:35:09.461884Z",
     "iopub.status.idle": "2025-07-06T17:35:09.467714Z",
     "shell.execute_reply": "2025-07-06T17:35:09.466750Z"
    },
    "papermill": {
     "duration": 0.016866,
     "end_time": "2025-07-06T17:35:09.469149",
     "exception": false,
     "start_time": "2025-07-06T17:35:09.452283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pipeline with all preprocessing steps\n",
    "pipeline = Pipeline([\n",
    "    ('convert_date', FunctionTransformer(convert_date, validate=False)),\n",
    "    ('merge', DatasetMerger(features_df=features_df, stores_df=stores_df)),\n",
    "    ('feature_engineering', FunctionTransformer(feature_engineering, validate=False)),\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d050618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T17:35:09.484562Z",
     "iopub.status.busy": "2025-07-06T17:35:09.483852Z",
     "iopub.status.idle": "2025-07-06T17:35:10.206152Z",
     "shell.execute_reply": "2025-07-06T17:35:10.205238Z"
    },
    "papermill": {
     "duration": 0.731564,
     "end_time": "2025-07-06T17:35:10.207748",
     "exception": false,
     "start_time": "2025-07-06T17:35:09.476184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in train_full before split: ['Store', 'Dept', 'Date', 'IsHoliday']\n",
      "train_full['Date'].dtype: object\n",
      "train_full_for_split['Date'].dtype: datetime64[ns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/1116745263.py:23: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  X_train = train_full[train_idx]\n",
      "/tmp/ipykernel_13/1116745263.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  X_val = train_full[~train_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in X_train: ['Store', 'Dept', 'Date', 'IsHoliday']\n",
      "Created column: MarkDown1_Missing\n",
      "Created column: MarkDown2_Missing\n",
      "Created column: MarkDown3_Missing\n",
      "Created column: MarkDown4_Missing\n",
      "Created column: MarkDown5_Missing\n",
      "Columns in X_train after pipeline transformations: ['Store', 'Dept', 'Date', 'IsHoliday', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size', 'Year', 'Month', 'Week', 'Day', 'MarkDown1_Missing', 'MarkDown2_Missing', 'MarkDown3_Missing', 'MarkDown4_Missing', 'MarkDown5_Missing']\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "train_full = train_df.copy()\n",
    "y = train_full['Weekly_Sales']\n",
    "train_full = train_full.drop(columns=['Weekly_Sales'])\n",
    "\n",
    "# Convert Date for splitting only (minimal preprocessing for training)\n",
    "try:\n",
    "    train_full_for_split = train_full.copy()\n",
    "    train_full_for_split['Date'] = pd.to_datetime(train_full_for_split['Date'], format='%Y-%m-%d')\n",
    "except Exception as e:\n",
    "    print(f\"train_full['Date'] conversion failed: {str(e)}\")\n",
    "    print(\"train_full['Date'].head():\", train_full['Date'].head().tolist())\n",
    "    raise\n",
    "\n",
    "print(\"Columns in train_full before split:\", train_full.columns.tolist())\n",
    "print(\"train_full['Date'].dtype:\", train_full['Date'].dtype)\n",
    "print(\"train_full_for_split['Date'].dtype:\", train_full_for_split['Date'].dtype)\n",
    "\n",
    "# Split data\n",
    "train_full_for_split = train_full_for_split.sort_values('Date')\n",
    "split_date = train_full_for_split['Date'].quantile(0.9)\n",
    "train_idx = train_full_for_split['Date'] < split_date\n",
    "X_train = train_full[train_idx]\n",
    "y_train = np.clip(y[train_idx], 0, y.quantile(0.99))\n",
    "X_val = train_full[~train_idx]\n",
    "y_val = y[~train_idx]\n",
    "is_holiday_val = X_val['IsHoliday']\n",
    "print(\"Columns in X_train:\", X_train.columns.tolist())\n",
    "\n",
    "# Verify columns after pipeline transformation\n",
    "X_train_transformed = pipeline.named_steps['convert_date'].transform(X_train)\n",
    "X_train_transformed = pipeline.named_steps['merge'].transform(X_train_transformed)\n",
    "X_train_transformed = pipeline.named_steps['feature_engineering'].transform(X_train_transformed)\n",
    "print(\"Columns in X_train after pipeline transformations:\", X_train_transformed.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9807144",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T17:35:10.223694Z",
     "iopub.status.busy": "2025-07-06T17:35:10.223392Z",
     "iopub.status.idle": "2025-07-06T17:35:10.228182Z",
     "shell.execute_reply": "2025-07-06T17:35:10.227127Z"
    },
    "papermill": {
     "duration": 0.015238,
     "end_time": "2025-07-06T17:35:10.230015",
     "exception": false,
     "start_time": "2025-07-06T17:35:10.214777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WMAE metric\n",
    "def calculate_wmae(y_true, y_pred, is_holiday):\n",
    "    weights = np.where(is_holiday, 5, 1)\n",
    "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d494080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T17:35:10.245629Z",
     "iopub.status.busy": "2025-07-06T17:35:10.245304Z",
     "iopub.status.idle": "2025-07-06T17:35:10.250314Z",
     "shell.execute_reply": "2025-07-06T17:35:10.249301Z"
    },
    "papermill": {
     "duration": 0.014611,
     "end_time": "2025-07-06T17:35:10.251772",
     "exception": false,
     "start_time": "2025-07-06T17:35:10.237161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MLflow setup\n",
    "os.environ['MLFLOW_TRACKING_URI'] = 'https://dagshub.com/AleksandreBakhtadze/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting.mlflow'\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'AleksandreBakhtadze'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '034b77b38fbceb0a45865e04299f524469d930d4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d3e4d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T17:35:10.267439Z",
     "iopub.status.busy": "2025-07-06T17:35:10.267116Z",
     "iopub.status.idle": "2025-07-06T17:35:19.256521Z",
     "shell.execute_reply": "2025-07-06T17:35:19.254620Z"
    },
    "papermill": {
     "duration": 8.999609,
     "end_time": "2025-07-06T17:35:19.258548",
     "exception": false,
     "start_time": "2025-07-06T17:35:10.258939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created column: MarkDown1_Missing\n",
      "Created column: MarkDown2_Missing\n",
      "Created column: MarkDown3_Missing\n",
      "Created column: MarkDown4_Missing\n",
      "Created column: MarkDown5_Missing\n",
      "Created column: MarkDown1_Missing\n",
      "Created column: MarkDown2_Missing\n",
      "Created column: MarkDown3_Missing\n",
      "Created column: MarkDown4_Missing\n",
      "Created column: MarkDown5_Missing\n",
      "Validation MAE: 3784.87\n",
      "Validation WMAE: 3870.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/06 17:35:16 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to register model in MLflow Model Registry: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n",
      "Model saved as artifact: /kaggle/working/models/xgboost_pipeline_improved.joblib\n",
      "ğŸƒ View run XGBoost_Improved_Training at: https://dagshub.com/AleksandreBakhtadze/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/4/runs/0011b756a4674eaba6d476b9b767123a\n",
      "ğŸ§ª View experiment at: https://dagshub.com/AleksandreBakhtadze/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/4\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"XGBoost_Improved_Pipeline\")\n",
    "# End any active MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"XGBoost_Improved_Training\"):\n",
    "    # Fit pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    wmae = calculate_wmae(y_val, y_pred, is_holiday_val)\n",
    "    print(f\"Validation MAE: {mae:.2f}\")\n",
    "    print(f\"Validation WMAE: {wmae:.2f}\")\n",
    "    \n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"learning_rate\", 0.1)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_metric(\"val_mae\", mae)\n",
    "    mlflow.log_metric(\"val_wmae\", wmae)\n",
    "    \n",
    "    # Attempt to save model to MLflow model registry\n",
    "    model_name = \"XGBoost_Pipeline_Model\"\n",
    "    try:\n",
    "        mlflow.sklearn.log_model(pipeline, \"model\", registered_model_name=model_name)\n",
    "        print(f\"Model successfully registered in MLflow Model Registry as {model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to register model in MLflow Model Registry: {str(e)}\")\n",
    "        # Fallback to saving as joblib artifact\n",
    "        os.makedirs(\"/kaggle/working/models\", exist_ok=True)\n",
    "        model_path = \"/kaggle/working/models/xgboost_pipeline_improved.joblib\"\n",
    "        joblib.dump(pipeline, model_path)\n",
    "        mlflow.log_artifact(model_path)\n",
    "        print(f\"Model saved as artifact: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60d3b3fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T17:35:19.276138Z",
     "iopub.status.busy": "2025-07-06T17:35:19.275780Z",
     "iopub.status.idle": "2025-07-06T17:35:19.567384Z",
     "shell.execute_reply": "2025-07-06T17:35:19.566675Z"
    },
    "papermill": {
     "duration": 0.30226,
     "end_time": "2025-07-06T17:35:19.568940",
     "exception": false,
     "start_time": "2025-07-06T17:35:19.266680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in test_full before pipeline: ['Store', 'Dept', 'Date', 'IsHoliday']\n",
      "test_full['Date'].dtype: object\n",
      "Created column: MarkDown1_Missing\n",
      "Created column: MarkDown2_Missing\n",
      "Created column: MarkDown3_Missing\n",
      "Created column: MarkDown4_Missing\n",
      "Created column: MarkDown5_Missing\n"
     ]
    }
   ],
   "source": [
    "# Test predictions\n",
    "test_full = test_df.copy()\n",
    "print(\"Columns in test_full before pipeline:\", test_full.columns.tolist())\n",
    "print(\"test_full['Date'].dtype:\", test_full['Date'].dtype)\n",
    "test_preds = pipeline.predict(test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "614da15d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T17:35:19.590354Z",
     "iopub.status.busy": "2025-07-06T17:35:19.589729Z",
     "iopub.status.idle": "2025-07-06T17:35:22.171522Z",
     "shell.execute_reply": "2025-07-06T17:35:22.170499Z"
    },
    "papermill": {
     "duration": 2.594835,
     "end_time": "2025-07-06T17:35:22.173649",
     "exception": false,
     "start_time": "2025-07-06T17:35:19.578814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created column: MarkDown1_Missing\n",
      "Created column: MarkDown2_Missing\n",
      "Created column: MarkDown3_Missing\n",
      "Created column: MarkDown4_Missing\n",
      "Created column: MarkDown5_Missing\n"
     ]
    }
   ],
   "source": [
    "# Submission\n",
    "test_full_transformed = pipeline.named_steps['convert_date'].transform(test_full)\n",
    "test_full_transformed = pipeline.named_steps['merge'].transform(test_full_transformed)\n",
    "test_full_transformed = pipeline.named_steps['feature_engineering'].transform(test_full_transformed)\n",
    "test_full_transformed['Id'] = test_full_transformed['Store'].astype(str) + '_' + test_full_transformed['Dept'].astype(str) + '_' + test_full_transformed['Date'].dt.strftime('%Y-%m-%d')\n",
    "submission_df = pd.DataFrame({'Id': test_full_transformed['Id'], 'Weekly_Sales': test_preds})\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "mlflow.log_artifact(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b47d6a",
   "metadata": {
    "papermill": {
     "duration": 0.007525,
     "end_time": "2025-07-06T17:35:22.189247",
     "exception": false,
     "start_time": "2025-07-06T17:35:22.181722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 32105,
     "sourceId": 3816,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 60.029598,
   "end_time": "2025-07-06T17:35:24.526597",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-06T17:34:24.496999",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "7bf04b8264f64259bdb3a12f9a780ee4": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/output",
       "_model_module_version": "1.0.0",
       "_model_name": "OutputModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/output",
       "_view_module_version": "1.0.0",
       "_view_name": "OutputView",
       "layout": "IPY_MODEL_c0df0dd08ef9445bac66761568cc8609",
       "msg_id": "",
       "outputs": [
        {
         "data": {
          "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">â </span> Waiting for authorization\n</pre>\n",
          "text/plain": "\u001b[32mâ \u001b[0m Waiting for authorization\n"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ],
       "tabbable": null,
       "tooltip": null
      }
     },
     "c0df0dd08ef9445bac66761568cc8609": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
