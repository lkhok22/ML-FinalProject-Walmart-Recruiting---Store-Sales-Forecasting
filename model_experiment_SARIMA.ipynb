{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkhok22/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_SARIMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dOp3mvTjWDHU"
      },
      "outputs": [],
      "source": [
        "# ✅ Enhanced SARIMAX Model for Walmart Sales Forecasting\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import time\n",
        "import wandb\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "73aYB6u6aXFm"
      },
      "outputs": [],
      "source": [
        "# ✅ Only install what you need for SARIMAX and logging\n",
        "!pip install statsmodels wandb --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import wandb\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "fDLGL1V8bdtk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ys96BI5qaYiA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import wandb\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwGrXiXscYIl",
        "outputId": "2695e06c-2ed3-415f-953b-2c0d734711d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.0/56.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.6/200.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.0/340.0 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.8/285.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.9/981.9 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabakh22\u001b[0m (\u001b[33mabakh22-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install pandas numpy matplotlib seaborn scikit-learn torch torchvision wandb pyyaml darts --quiet\n",
        "import wandb\n",
        "wandb.login(key=\"eccf2c915699fc032ad678daf0fd4b5ac60bf87c\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjWcYXmtcalW",
        "outputId": "7f6f035d-c7d6-4443-c712-1010e7847384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Extracted files: ['test.csv.zip', 'stores.csv', 'sampleSubmission.csv', 'features.csv', 'test.csv', 'train.csv.zip', 'train.csv', 'sampleSubmission.csv.zip', 'features.csv.zip']\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive and extract data\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "zip_path = '/content/drive/MyDrive/ML-FinalProject/data.zip'\n",
        "extract_to = '/content/walmart_data/'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "for file_name in os.listdir(extract_to):\n",
        "    if file_name.endswith('.zip'):\n",
        "        with zipfile.ZipFile(os.path.join(extract_to, file_name), 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "print(\"✅ Extracted files:\", os.listdir(extract_to))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MaUth4wAoCZB"
      },
      "outputs": [],
      "source": [
        "# ✅ Memory-optimized preprocessing with reduced dtypes\n",
        "dtypes = {\n",
        "    'Store': 'int8', 'Dept': 'int8', 'Weekly_Sales': 'float32',\n",
        "    'Temperature': 'float32', 'Fuel_Price': 'float32', 'CPI': 'float32',\n",
        "    'Unemployment': 'float32', 'Size': 'int32', 'IsHoliday': 'bool',\n",
        "    'MarkDown1': 'float32', 'MarkDown2': 'float32', 'MarkDown3': 'float32',\n",
        "    'MarkDown4': 'float32', 'MarkDown5': 'float32'\n",
        "}\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('/content/walmart_data/train.csv', dtype=dtypes, parse_dates=['Date'])\n",
        "features = pd.read_csv('/content/walmart_data/features.csv', dtype=dtypes, parse_dates=['Date'])\n",
        "stores = pd.read_csv('/content/walmart_data/stores.csv', dtype={'Store': 'int8', 'Size': 'int32'})\n",
        "test = pd.read_csv('/content/walmart_data/test.csv', dtype=dtypes, parse_dates=['Date'])\n",
        "\n",
        "# Merge and keep necessary columns\n",
        "df = pd.merge(train, features, on=['Store', 'Date'], how='left')\n",
        "df = pd.merge(df, stores, on='Store', how='left')\n",
        "df = df.drop(columns=['IsHoliday_x']).rename(columns={'IsHoliday_y': 'IsHoliday'})\n",
        "df = df[['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Temperature',\n",
        "         'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'Type', 'MarkDown1',\n",
        "         'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']].sort_values(by=['Store', 'Dept', 'Date'])\n",
        "\n",
        "test_df = pd.merge(test, features, on=['Store', 'Date'], how='left')\n",
        "test_df = pd.merge(test_df, stores, on='Store', how='left')\n",
        "test_df = test_df.drop(columns=['IsHoliday_x']).rename(columns={'IsHoliday_y': 'IsHoliday'})\n",
        "test_df = test_df[['Store', 'Dept', 'Date', 'IsHoliday', 'Temperature',\n",
        "                   'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'Type',\n",
        "                   'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']].sort_values(by=['Store', 'Dept', 'Date'])\n",
        "\n",
        "# Enhanced preprocessing\n",
        "def enhance_preprocessing(df, test_df):\n",
        "    \"\"\"Enhanced preprocessing with additional features\"\"\"\n",
        "    # Create IsHolidayWeight\n",
        "    df['IsHolidayWeight'] = df['IsHoliday'].map({True: 5, False: 1}).astype('int8')\n",
        "    test_df['IsHolidayWeight'] = test_df['IsHoliday'].map({True: 5, False: 1}).astype('int8')\n",
        "\n",
        "    # Create store type dummy variables\n",
        "    df = pd.get_dummies(df, columns=['Type'], prefix='Type')\n",
        "    test_df = pd.get_dummies(test_df, columns=['Type'], prefix='Type')\n",
        "    for col in ['Type_A', 'Type_B', 'Type_C']:\n",
        "        if col not in df.columns:\n",
        "            df[col] = 0\n",
        "        if col not in test_df.columns:\n",
        "            test_df[col] = 0\n",
        "        df[col] = df[col].astype('int8')\n",
        "        test_df[col] = test_df[col].astype('int8')\n",
        "\n",
        "    # Store-level features\n",
        "    store_stats = df.groupby('Store').agg({\n",
        "        'Weekly_Sales': ['mean', 'std'],\n",
        "        'Size': 'first'\n",
        "    }).round(2)\n",
        "    store_stats.columns = ['Store_Sales_Mean', 'Store_Sales_Std', 'Store_Size']\n",
        "    store_stats['Store_Sales_Std'] = store_stats['Store_Sales_Std'].fillna(0).astype('float32')\n",
        "    store_stats['Store_Sales_Mean'] = store_stats['Store_Sales_Mean'].astype('float32')\n",
        "    store_stats['Store_Size'] = store_stats['Store_Size'].astype('int32')\n",
        "    df = df.merge(store_stats, left_on='Store', right_index=True, how='left')\n",
        "    test_df = test_df.merge(store_stats, left_on='Store', right_index=True, how='left')\n",
        "\n",
        "    # Department-level features\n",
        "    dept_stats = df.groupby('Dept').agg({\n",
        "        'Weekly_Sales': ['mean', 'std']\n",
        "    }).round(2)\n",
        "    dept_stats.columns = ['Dept_Sales_Mean', 'Dept_Sales_Std']\n",
        "    dept_stats['Dept_Sales_Std'] = dept_stats['Dept_Sales_Std'].fillna(0).astype('float32')\n",
        "    dept_stats['Dept_Sales_Mean'] = dept_stats['Dept_Sales_Mean'].astype('float32')\n",
        "    df = df.merge(dept_stats, left_on='Dept', right_index=True, how='left')\n",
        "    test_df = test_df.merge(dept_stats, left_on='Dept', right_index=True, how='left')\n",
        "\n",
        "    # Vectorized holiday features\n",
        "    holiday_dates = {\n",
        "        'SuperBowl': ['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08'],\n",
        "        'LaborDay': ['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06'],\n",
        "        'Thanksgiving': ['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29'],\n",
        "        'Christmas': ['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27']\n",
        "    }\n",
        "    for holiday, dates in holiday_dates.items():\n",
        "        holiday_pd_dates = pd.to_datetime(dates)\n",
        "        df[f'{holiday}_Week'] = 0.0\n",
        "        test_df[f'{holiday}_Week'] = 0.0\n",
        "        for date in holiday_pd_dates:\n",
        "            df[f'{holiday}_Week'] = np.where(df['Date'] == date, 1.0, df[f'{holiday}_Week'])\n",
        "            test_df[f'{holiday}_Week'] = np.where(test_df['Date'] == date, 1.0, test_df[f'{holiday}_Week'])\n",
        "            for i in range(1, 4):\n",
        "                before_date = date - pd.Timedelta(weeks=i)\n",
        "                after_date = date + pd.Timedelta(weeks=i)\n",
        "                weight = 1.0 / (i + 1)\n",
        "                df[f'{holiday}_Week'] = np.where(df['Date'].isin([before_date, after_date]), weight, df[f'{holiday}_Week'])\n",
        "                test_df[f'{holiday}_Week'] = np.where(test_df['Date'].isin([before_date, after_date]), weight, test_df[f'{holiday}_Week'])\n",
        "        df[f'{holiday}_Week'] = df[f'{holiday}_Week'].astype('float32')\n",
        "        test_df[f'{holiday}_Week'] = test_df[f'{holiday}_Week'].astype('float32')\n",
        "\n",
        "    # Time-based and cyclical features\n",
        "    df['WeekOfYear'] = df['Date'].dt.isocalendar().week.astype('int8')\n",
        "    df['Month'] = df['Date'].dt.month.astype('int8')\n",
        "    df['Year'] = (df['Date'].dt.year - df['Date'].dt.year.min()).astype('int8')\n",
        "    df['Week_sin'] = np.sin(2 * np.pi * df['WeekOfYear'] / 52).astype('float32')\n",
        "    df['Week_cos'] = np.cos(2 * np.pi * df['WeekOfYear'] / 52).astype('float32')\n",
        "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12).astype('float32')\n",
        "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12).astype('float32')\n",
        "\n",
        "    test_df['WeekOfYear'] = test_df['Date'].dt.isocalendar().week.astype('int8')\n",
        "    test_df['Month'] = test_df['Date'].dt.month.astype('int8')\n",
        "    test_df['Year'] = (test_df['Date'].dt.year - df['Date'].dt.year.min()).astype('int8')\n",
        "    test_df['Week_sin'] = np.sin(2 * np.pi * test_df['WeekOfYear'] / 52).astype('float32')\n",
        "    test_df['Week_cos'] = np.cos(2 * np.pi * test_df['WeekOfYear'] / 52).astype('float32')\n",
        "    test_df['Month_sin'] = np.sin(2 * np.pi * test_df['Month'] / 12).astype('float32')\n",
        "    test_df['Month_cos'] = np.cos(2 * np.pi * test_df['Month'] / 12).astype('float32')\n",
        "\n",
        "    # Handle missing values\n",
        "    for col in ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']:\n",
        "        df[col] = df.groupby(['Store', 'Month'])[col].transform('mean').fillna(df[col].mean()).astype('float32')\n",
        "        test_df[col] = test_df.groupby(['Store', 'Month'])[col].transform('mean').fillna(test_df[col].mean()).astype('float32')\n",
        "\n",
        "    markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "    for col in markdown_cols:\n",
        "        df[col] = df[col].fillna(0).astype('float32')\n",
        "        test_df[col] = test_df[col].fillna(0).astype('float32')\n",
        "\n",
        "    # Interaction and lagged features\n",
        "    df['Total_Markdown'] = df[markdown_cols].sum(axis=1).astype('float32')\n",
        "    test_df['Total_Markdown'] = test_df[markdown_cols].sum(axis=1).astype('float32')\n",
        "    df['Lagged_Sales'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).astype('float32')\n",
        "    df['Lagged_Sales'] = df['Lagged_Sales'].fillna(df.groupby(['Store', 'Dept'])['Weekly_Sales'].transform('mean')).astype('float32')\n",
        "    test_df['Lagged_Sales'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).reindex(test_df.index, fill_value=df['Weekly_Sales'].mean()).astype('float32')\n",
        "\n",
        "    return df, test_df\n",
        "\n",
        "# Apply enhanced preprocessing\n",
        "df, test_df = enhance_preprocessing(df, test_df)\n",
        "\n",
        "# Compute department-level and store-level means for fallback\n",
        "dept_means = df.groupby(['Dept', 'IsHoliday'])['Weekly_Sales'].mean().unstack().fillna(df['Weekly_Sales'].mean())\n",
        "store_means = df.groupby(['Store', 'IsHoliday'])['Weekly_Sales'].mean().unstack().fillna(df['Weekly_Sales'].mean())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_wmae(y_true, y_pred, weights):\n",
        "    \"\"\"Calculate Weighted Mean Absolute Error\"\"\"\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "def print_memory_usage():\n",
        "    \"\"\"Monitor memory usage\"\"\"\n",
        "    process = psutil.Process()\n",
        "    mem_info = process.memory_info()\n",
        "    print(f\"Memory Usage: {mem_info.rss / 1024**2:.2f} MB\")\n",
        "    wandb.log({\"memory_usage_mb\": mem_info.rss / 1024**2})"
      ],
      "metadata": {
        "id": "SEvf5kDzbQUV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OtdpM_9WEwc"
      },
      "outputs": [],
      "source": [
        "# ✅ Enhanced forecaster with SARIMAX and Random Forest fallback\n",
        "class EnhancedWalmartForecaster:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.store_dept_stats = {}\n",
        "        self.sarimax_features = [\n",
        "            'Total_Markdown', 'Lagged_Sales', 'SuperBowl_Week', 'Thanksgiving_Week',\n",
        "            'Christmas_Week', 'Week_sin', 'Week_cos'\n",
        "        ]\n",
        "        self.fallback_features = [\n",
        "            'Total_Markdown', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "            'IsHoliday', 'WeekOfYear', 'Month', 'Size', 'Type_A', 'Type_B', 'Type_C',\n",
        "            'Week_sin', 'Week_cos', 'Month_sin', 'Month_cos', 'Store_Sales_Mean',\n",
        "            'Dept_Sales_Mean', 'Lagged_Sales'\n",
        "        ]\n",
        "\n",
        "    def create_lag_features(self, df, target_col='Weekly_Sales', lags=[1, 2]):\n",
        "        \"\"\"Create lag features for time series\"\"\"\n",
        "        df_lag = df.copy()\n",
        "        for lag in lags:\n",
        "            df_lag[f'{target_col}_lag_{lag}'] = df_lag.groupby(['Store', 'Dept'])[target_col].shift(lag).astype('float32')\n",
        "        return df_lag\n",
        "\n",
        "    def create_rolling_features(self, df, target_col='Weekly_Sales', windows=[4]):\n",
        "        \"\"\"Create rolling window features\"\"\"\n",
        "        df_roll = df.copy()\n",
        "        for window in windows:\n",
        "            df_roll[f'{target_col}_roll_mean_{window}'] = (\n",
        "                df_roll.groupby(['Store', 'Dept'])[target_col]\n",
        "                .rolling(window=window, min_periods=1).mean()\n",
        "                .reset_index(0, drop=True)\n",
        "            ).astype('float32')\n",
        "        return df_roll\n",
        "\n",
        "    def fit_store_dept_model(self, store, dept, train_data):\n",
        "        \"\"\"Fit model for specific store-department combination\"\"\"\n",
        "        try:\n",
        "            store_dept_data = train_data[\n",
        "                (train_data['Store'] == store) & (train_data['Dept'] == dept)\n",
        "            ].copy().sort_values('Date')\n",
        "\n",
        "            if len(store_dept_data) < 15:\n",
        "                return self._fit_statistical_fallback(store, dept, store_dept_data)\n",
        "\n",
        "            self.store_dept_stats[(store, dept)] = {\n",
        "                'mean': store_dept_data['Weekly_Sales'].mean(),\n",
        "                'std': store_dept_data['Weekly_Sales'].std(),\n",
        "                'holiday_mean': store_dept_data[store_dept_data['IsHoliday']]['Weekly_Sales'].mean(),\n",
        "                'non_holiday_mean': store_dept_data[~store_dept_data['IsHoliday']]['Weekly_Sales'].mean(),\n",
        "                'count': len(store_dept_data)\n",
        "            }\n",
        "\n",
        "            if store_dept_data['Weekly_Sales'].std() < 1.0:\n",
        "                return self._fit_statistical_fallback(store, dept, store_dept_data)\n",
        "\n",
        "            if len(store_dept_data) >= 50 and store_dept_data['Weekly_Sales'].std() > 1000:\n",
        "                success = self._try_sarimax(store, dept, store_dept_data)\n",
        "                if success:\n",
        "                    return True\n",
        "\n",
        "            return self._fit_random_forest_fallback(store, dept, store_dept_data)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fitting model for Store {store}, Dept {dept}: {str(e)}\")\n",
        "            return self._fit_statistical_fallback(store, dept, store_dept_data)\n",
        "\n",
        "    def _try_sarimax(self, store, dept, store_dept_data):\n",
        "        \"\"\"Try to fit SARIMAX model with simplified parameters\"\"\"\n",
        "        try:\n",
        "            y = store_dept_data['Weekly_Sales'].values\n",
        "            exog_data = store_dept_data[self.sarimax_features].copy()\n",
        "            for col in exog_data.columns:\n",
        "                exog_data[col] = exog_data[col].fillna(exog_data[col].mean()).astype('float32')\n",
        "\n",
        "            scaler = StandardScaler()\n",
        "            exog_scaled = scaler.fit_transform(exog_data)\n",
        "            self.scalers[(store, dept)] = scaler\n",
        "\n",
        "            split_idx = int(len(y) * 0.8)\n",
        "            train_y = y[:split_idx]\n",
        "            train_exog = exog_scaled[:split_idx]\n",
        "            val_y = y[split_idx:]\n",
        "            val_exog = exog_scaled[split_idx:]\n",
        "            val_weights = store_dept_data['IsHolidayWeight'].iloc[split_idx:].values\n",
        "\n",
        "            model = SARIMAX(\n",
        "                endog=train_y,\n",
        "                exog=train_exog,\n",
        "                order=(1, 0, 0),\n",
        "                seasonal_order=(0, 0, 0, 52),\n",
        "                enforce_stationarity=False,\n",
        "                enforce_invertibility=False,\n",
        "                simple_differencing=True\n",
        "            )\n",
        "            fitted_model = model.fit(disp=False, maxiter=50, method='lbfgs', gtol=1e-6)\n",
        "\n",
        "            pred = fitted_model.forecast(steps=len(val_y), exog=val_exog)\n",
        "            pred = np.maximum(pred, 0)\n",
        "            wmae = calculate_wmae(val_y, pred, val_weights)\n",
        "\n",
        "            if wmae < 10000:\n",
        "                final_model = SARIMAX(\n",
        "                    endog=y,\n",
        "                    exog=exog_scaled,\n",
        "                    order=(1, 0, 1),\n",
        "                    seasonal_order=(0, 1, 1, 52),\n",
        "                    enforce_stationarity=False,\n",
        "                    enforce_invertibility=False,\n",
        "                    simple_differencing=True\n",
        "                )\n",
        "                fitted_final = final_model.fit(disp=False, maxiter=50)\n",
        "                self.models[(store, dept)] = {\n",
        "                    'type': 'sarimax',\n",
        "                    'model': fitted_final,\n",
        "                    'cv_score': wmae\n",
        "                }\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            return False\n",
        "\n",
        "    def _fit_random_forest_fallback(self, store, dept, store_dept_data):\n",
        "        \"\"\"Fit Random Forest fallback model\"\"\"\n",
        "        try:\n",
        "            enhanced_data = self.create_lag_features(store_dept_data, lags=[1, 2])\n",
        "            enhanced_data = self.create_rolling_features(enhanced_data, windows=[4])\n",
        "            enhanced_data = enhanced_data.dropna()\n",
        "\n",
        "            if len(enhanced_data) < 8:\n",
        "                return self._fit_statistical_fallback(store, dept, store_dept_data)\n",
        "\n",
        "            available_features = [col for col in self.fallback_features if col in enhanced_data.columns]\n",
        "            lag_features = [col for col in enhanced_data.columns if 'lag_' in col or 'roll_' in col]\n",
        "            all_features = available_features + lag_features\n",
        "\n",
        "            X = enhanced_data[all_features].fillna(0)\n",
        "            y = enhanced_data['Weekly_Sales'].values\n",
        "\n",
        "            split_idx = int(len(X) * 0.8)\n",
        "            X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]\n",
        "            y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "            if len(X_train) < 5:\n",
        "                return self._fit_statistical_fallback(store, dept, store_dept_data)\n",
        "\n",
        "            rf_model = RandomForestRegressor(\n",
        "                n_estimators=50, max_depth=6, min_samples_split=5,\n",
        "                min_samples_leaf=3, random_state=42, n_jobs=1, max_features='sqrt'\n",
        "            )\n",
        "            rf_model.fit(X_train, y_train)\n",
        "            self.models[(store, dept)] = {\n",
        "                'type': 'random_forest',\n",
        "                'model': rf_model,\n",
        "                'feature_cols': all_features\n",
        "            }\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            return self._fit_statistical_fallback(store, dept, store_dept_data)\n",
        "\n",
        "    def _fit_statistical_fallback(self, store, dept, store_dept_data):\n",
        "        \"\"\"Fit statistical fallback model\"\"\"\n",
        "        try:\n",
        "            stats = self.store_dept_stats.get((store, dept), {})\n",
        "            holiday_mean = store_dept_data[store_dept_data['IsHoliday']]['Weekly_Sales'].mean()\n",
        "            non_holiday_mean = store_dept_data[~store_dept_data['IsHoliday']]['Weekly_Sales'].mean()\n",
        "            overall_mean = store_dept_data['Weekly_Sales'].mean()\n",
        "\n",
        "            if pd.isna(holiday_mean):\n",
        "                holiday_mean = overall_mean\n",
        "            if pd.isna(non_holiday_mean):\n",
        "                non_holiday_mean = overall_mean\n",
        "            if pd.isna(overall_mean):\n",
        "                overall_mean = 1000\n",
        "\n",
        "            self.models[(store, dept)] = {\n",
        "                'type': 'statistical',\n",
        "                'holiday_mean': holiday_mean,\n",
        "                'non_holiday_mean': non_holiday_mean,\n",
        "                'overall_mean': overall_mean\n",
        "            }\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.models[(store, dept)] = {\n",
        "                'type': 'statistical',\n",
        "                'holiday_mean': 1000,\n",
        "                'non_holiday_mean': 1000,\n",
        "                'overall_mean': 1000\n",
        "            }\n",
        "            return True\n",
        "\n",
        "    def predict_store_dept(self, store, dept, test_group):\n",
        "        \"\"\"Make predictions for specific store-department\"\"\"\n",
        "        try:\n",
        "            if (store, dept) not in self.models:\n",
        "                dept = test_group['Dept'].iloc[0] if not test_group.empty else 1\n",
        "                is_holiday = test_group['IsHoliday'].iloc[0] if not test_group.empty else False\n",
        "                store = test_group['Store'].iloc[0] if not test_group.empty else 1\n",
        "                dept_mean = dept_means.loc[dept, is_holiday] if dept in dept_means.index else 1000\n",
        "                store_mean = store_means.loc[store, is_holiday] if store in store_means.index else 1000\n",
        "                pred_value = 0.6 * dept_mean + 0.4 * store_mean\n",
        "                return np.full(len(test_group), pred_value)\n",
        "\n",
        "            model_info = self.models[(store, dept)]\n",
        "            if model_info['type'] == 'sarimax':\n",
        "                fitted_model = model_info['model']\n",
        "                scaler = self.scalers[(store, dept)]\n",
        "                exog_data = test_group[self.sarimax_features].copy()\n",
        "                for col in exog_data.columns:\n",
        "                    exog_data[col] = exog_data[col].fillna(exog_data[col].mean()).astype('float32')\n",
        "                exog_scaled = scaler.transform(exog_data)\n",
        "                predictions = fitted_model.forecast(steps=len(test_group), exog=exog_scaled)\n",
        "                return np.maximum(predictions, 0)\n",
        "\n",
        "            elif model_info['type'] == 'random_forest':\n",
        "                rf_model = model_info['model']\n",
        "                feature_cols = model_info['feature_cols']\n",
        "                test_group_enhanced = self.create_lag_features(test_group, lags=[1, 2])\n",
        "                test_group_enhanced = self.create_rolling_features(test_group_enhanced, windows=[4])\n",
        "                available_features = [col for col in feature_cols if col in test_group_enhanced.columns]\n",
        "                X_test = test_group_enhanced[available_features].fillna(0)\n",
        "                predictions = rf_model.predict(X_test)\n",
        "                return np.maximum(predictions, 0)\n",
        "\n",
        "            else:\n",
        "                holiday_mean = model_info['holiday_mean']\n",
        "                non_holiday_mean = model_info['non_holiday_mean']\n",
        "                predictions = np.where(test_group['IsHoliday'], holiday_mean, non_holiday_mean)\n",
        "                return np.maximum(predictions, 0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error predicting for Store {store}, Dept {dept}: {str(e)}\")\n",
        "            dept = test_group['Dept'].iloc[0] if not test_group.empty else 1\n",
        "            is_holiday = test_group['IsHoliday'].iloc[0] if not test_group.empty else False\n",
        "            store = test_group['Store'].iloc[0] if not test_group.empty else 1\n",
        "            dept_mean = dept_means.loc[dept, is_holiday] if dept in dept_means.index else 1000\n",
        "            store_mean = store_means.loc[store, is_holiday] if store in store_means.index else 1000\n",
        "            pred_value = 0.6 * dept_mean + 0.4 * store_mean\n",
        "            return np.full(len(test_group), pred_value)\n",
        "\n",
        "def train_and_predict_enhanced_model(train_df, test_df):\n",
        "    \"\"\"Main function to train enhanced model and generate predictions\"\"\"\n",
        "    print(\"🚀 Starting Enhanced Walmart Forecasting...\")\n",
        "    print(\"📊 Enhancing preprocessing...\")\n",
        "    wandb.init(\n",
        "        project=\"walmart-sales-forecasting\",\n",
        "        name=\"sarimax_rf_hybrid\",\n",
        "        config={\n",
        "            \"model\": \"SARIMAX_RF_Ensemble\",\n",
        "            \"features\": \"enhanced_with_store_dept_stats_lagged\",\n",
        "            \"fallback_strategy\": \"sarimax_rf_statistical\",\n",
        "            \"sarimax_order\": (1, 0, 1),\n",
        "            \"sarimax_seasonal_order\": (0, 1, 1, 52),\n",
        "            \"rf_n_estimators\": 50,\n",
        "            \"rf_max_depth\": 6\n",
        "        }\n",
        "    )\n",
        "    forecaster = EnhancedWalmartForecaster()\n",
        "    store_dept_combinations = train_df[['Store', 'Dept']].drop_duplicates()\n",
        "    print(f\"📈 Training models for {len(store_dept_combinations)} store-department combinations...\")\n",
        "\n",
        "    model_type_counts = {'sarimax': 0, 'random_forest': 0, 'statistical': 0}\n",
        "    start_time = time.time()\n",
        "    batch_size = 500\n",
        "    batches = [store_dept_combinations[i:i+batch_size] for i in range(0, len(store_dept_combinations), batch_size)]\n",
        "\n",
        "    for batch_idx, batch in enumerate(batches):\n",
        "        print(f\"Processing batch {batch_idx+1}/{len(batches)}...\")\n",
        "        print_memory_usage()\n",
        "        for idx, (_, row) in enumerate(batch.iterrows()):\n",
        "            store, dept = row['Store'], row['Dept']\n",
        "            if idx % 50 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                rate = (batch_idx * batch_size + idx) / elapsed if elapsed > 0 else 0\n",
        "                eta = (len(store_dept_combinations) - (batch_idx * batch_size + idx)) / rate if rate > 0 else 0\n",
        "                print(f\"Progress: {batch_idx * batch_size + idx}/{len(store_dept_combinations)} \"\n",
        "                      f\"({(batch_idx * batch_size + idx)/len(store_dept_combinations)*100:.1f}%) | \"\n",
        "                      f\"Rate: {rate:.1f}/sec | ETA: {eta/60:.1f} min | \"\n",
        "                      f\"SARIMAX: {model_type_counts['sarimax']}, RF: {model_type_counts['random_forest']}, \"\n",
        "                      f\"Statistical: {model_type_counts['statistical']}\")\n",
        "            success = forecaster.fit_store_dept_model(store, dept, train_df)\n",
        "            if (store, dept) in forecaster.models:\n",
        "                model_type = forecaster.models[(store, dept)]['type']\n",
        "                model_type_counts[model_type] += 1\n",
        "        pd.DataFrame([(k, v) for k, v in forecaster.models.items()]).to_pickle(f'/content/batch_models_{batch_idx}.pkl')\n",
        "        gc.collect()\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"✅ Training completed in {training_time/60:.1f} minutes\")\n",
        "    print(f\"📊 Model distribution: {model_type_counts}\")\n",
        "\n",
        "    print(\"🔮 Generating predictions...\")\n",
        "    predictions = []\n",
        "    test_groups = test_df.groupby(['Store', 'Dept'])\n",
        "    missing_store_depts = set()\n",
        "\n",
        "    for (store, dept), test_group in test_groups:\n",
        "        test_group_sorted = test_group.sort_values('Date')\n",
        "        group_predictions = forecaster.predict_store_dept(store, dept, test_group_sorted)\n",
        "        for idx, (_, test_row) in enumerate(test_group_sorted.iterrows()):\n",
        "            test_id = f\"{store}_{dept}_{test_row['Date'].strftime('%Y-%m-%d')}\"\n",
        "            predictions.append({\n",
        "                'Id': test_id,\n",
        "                'Weekly_Sales': group_predictions[idx]\n",
        "            })\n",
        "\n",
        "    existing_ids = {pred['Id'] for pred in predictions}\n",
        "    overall_mean = train_df['Weekly_Sales'].mean()\n",
        "\n",
        "    for _, test_row in test_df.iterrows():\n",
        "        test_id = f\"{test_row['Store']}_{test_row['Dept']}_{test_row['Date'].strftime('%Y-%m-%d')}\"\n",
        "        if test_id not in existing_ids:\n",
        "            dept = test_row['Dept']\n",
        "            store = test_row['Store']\n",
        "            is_holiday = test_row['IsHoliday']\n",
        "            dept_mean = dept_means.loc[dept, is_holiday] if dept in dept_means.index else overall_mean\n",
        "            store_mean = store_means.loc[store, is_holiday] if store in store_means.index else overall_mean\n",
        "            pred_value = 0.6 * dept_mean + 0.4 * store_mean\n",
        "            predictions.append({\n",
        "                'Id': test_id,\n",
        "                'Weekly_Sales': max(0, pred_value)\n",
        "            })\n",
        "            missing_store_depts.add((store, dept))\n",
        "\n",
        "    submission_df = pd.DataFrame(predictions)\n",
        "    pred_values = submission_df['Weekly_Sales'].values\n",
        "    print(f\"\\n📈 PREDICTION QUALITY CHECKS:\")\n",
        "    print(f\"Total predictions: {len(submission_df)}\")\n",
        "    print(f\"Expected predictions: {len(test_df)}\")\n",
        "    print(f\"Min prediction: {pred_values.min():.2f}\")\n",
        "    print(f\"Max prediction: {pred_values.max():.2f}\")\n",
        "    print(f\"Mean prediction: {pred_values.mean():.2f}\")\n",
        "    print(f\"Negative predictions: {(pred_values < 0).sum()}\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"training_time_minutes\": training_time/60,\n",
        "        \"sarimax_models\": model_type_counts['sarimax'],\n",
        "        \"rf_models\": model_type_counts['random_forest'],\n",
        "        \"statistical_models\": model_type_counts['statistical'],\n",
        "        \"total_predictions\": len(submission_df),\n",
        "        \"mean_prediction\": pred_values.mean(),\n",
        "        \"prediction_std\": pred_values.std(),\n",
        "        \"missing_store_depts\": len(missing_store_depts)\n",
        "    })\n",
        "    return submission_df, forecaster\n",
        "\n",
        "# Run enhanced model\n",
        "print(\"🔄 Starting enhanced model training...\")\n",
        "enhanced_submission, trained_forecaster = train_and_predict_enhanced_model(df, test_df)\n",
        "enhanced_submission.to_csv('/content/sarimax_rf_hybrid_submission.csv', index=False)\n",
        "print(\"✅ Submission saved to /content/sarimax_rf_hybrid_submission.csv\")\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fc0TcA_DWKrF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7R5n2ySOWtHsff13yWDzP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}