{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkhok22/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_SARIMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "73aYB6u6aXFm"
      },
      "outputs": [],
      "source": [
        "# ✅ Only install what you need for SARIMAX and logging\n",
        "!pip install statsmodels wandb --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import wandb\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Ys96BI5qaYiA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install pandas numpy matplotlib seaborn scikit-learn torch torchvision wandb pyyaml darts --quiet\n",
        "import wandb\n",
        "wandb.login(key=\"eccf2c915699fc032ad678daf0fd4b5ac60bf87c\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwGrXiXscYIl",
        "outputId": "97061e48-8aa9-459e-f3b1-30ed96716e6e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.0/56.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.6/200.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.0/340.0 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.8/285.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.9/981.9 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabakh22\u001b[0m (\u001b[33mabakh22-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive and extract data\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "zip_path = '/content/drive/MyDrive/ML-FinalProject/data.zip'\n",
        "extract_to = '/content/walmart_data/'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "for file_name in os.listdir(extract_to):\n",
        "    if file_name.endswith('.zip'):\n",
        "        with zipfile.ZipFile(os.path.join(extract_to, file_name), 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "print(\"✅ Extracted files:\", os.listdir(extract_to))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjWcYXmtcalW",
        "outputId": "250fb0d8-efa6-4c41-ba40-205990507f03"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Extracted files: ['stores.csv', 'sampleSubmission.csv', 'train.csv.zip', 'features.csv', 'test.csv.zip', 'test.csv', 'sampleSubmission.csv.zip', 'train.csv', 'features.csv.zip']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "train = pd.read_csv('/content/walmart_data/train.csv')\n",
        "features = pd.read_csv('/content/walmart_data/features.csv')\n",
        "stores = pd.read_csv('/content/walmart_data/stores.csv')\n",
        "test = pd.read_csv('/content/walmart_data/test.csv')\n",
        "\n",
        "# Merge train with features and stores\n",
        "df = pd.merge(train, features, on=['Store', 'Date'], how='left')\n",
        "df = pd.merge(df, stores, on='Store', how='left')\n",
        "df = df.drop(columns=['IsHoliday_x']).rename(columns={'IsHoliday_y': 'IsHoliday'})\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values(by=['Store', 'Dept', 'Date'])\n",
        "\n",
        "# ✅ IMPROVED PREPROCESSING\n",
        "# Create IsHolidayWeight column\n",
        "df['IsHolidayWeight'] = df['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
        "\n",
        "# ✅ BETTER HOLIDAY FEATURE ENGINEERING\n",
        "holiday_dates = {\n",
        "    'SuperBowl': ['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08'],\n",
        "    'LaborDay': ['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06'],\n",
        "    'Thanksgiving': ['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29'],\n",
        "    'Christmas': ['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27']\n",
        "}\n",
        "\n",
        "# Add more sophisticated holiday features\n",
        "for holiday, dates in holiday_dates.items():\n",
        "    holiday_pd_dates = pd.to_datetime(dates)\n",
        "    df[f'{holiday}_Week'] = 0\n",
        "    for date in holiday_pd_dates:\n",
        "        # Mark the exact holiday week\n",
        "        df.loc[df['Date'] == date, f'{holiday}_Week'] = 1\n",
        "        # Mark weeks before and after with decreasing weights\n",
        "        for i in range(1, 4):  # 3 weeks before/after\n",
        "            before_date = date - pd.Timedelta(weeks=i)\n",
        "            after_date = date + pd.Timedelta(weeks=i)\n",
        "            weight = 1.0 / (i + 1)  # Decreasing weight\n",
        "            df.loc[df['Date'] == before_date, f'{holiday}_Week'] = weight\n",
        "            df.loc[df['Date'] == after_date, f'{holiday}_Week'] = weight\n",
        "\n",
        "# ✅ ENHANCED TIME-BASED FEATURES\n",
        "df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Year'] = df['Date'].dt.year - df['Date'].dt.year.min()\n",
        "df['Quarter'] = df['Date'].dt.quarter\n",
        "\n",
        "# Add cyclical features\n",
        "df['Week_sin'] = np.sin(2 * np.pi * df['WeekOfYear'] / 52)\n",
        "df['Week_cos'] = np.cos(2 * np.pi * df['WeekOfYear'] / 52)\n",
        "df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "\n",
        "# ✅ HANDLE MISSING VALUES BETTER\n",
        "# Fill missing values with more sophisticated methods\n",
        "df['Temperature'].fillna(df.groupby(['Store', 'Month'])['Temperature'].transform('mean'), inplace=True)\n",
        "df['Fuel_Price'].fillna(df.groupby('Store')['Fuel_Price'].transform('mean'), inplace=True)\n",
        "df['CPI'].fillna(df.groupby('Store')['CPI'].transform('mean'), inplace=True)\n",
        "df['Unemployment'].fillna(df.groupby('Store')['Unemployment'].transform('mean'), inplace=True)\n",
        "\n",
        "# Fill markdown columns with 0 (more meaningful than NaN for promotions)\n",
        "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "for col in markdown_cols:\n",
        "    df[col].fillna(0, inplace=True)\n",
        "\n",
        "# ✅ CREATE INTERACTION FEATURES\n",
        "df['Temp_Unemployment'] = df['Temperature'] * df['Unemployment']\n",
        "df['Holiday_Markdown'] = df['IsHoliday'] * (df['MarkDown1'] + df['MarkDown2'] + df['MarkDown3'] + df['MarkDown4'] + df['MarkDown5'])\n",
        "df['Total_Markdown'] = df['MarkDown1'] + df['MarkDown2'] + df['MarkDown3'] + df['MarkDown4'] + df['MarkDown5']\n",
        "\n",
        "# ✅ APPLY SAME PREPROCESSING TO TEST DATA\n",
        "test_df = pd.merge(test, features, on=['Store', 'Date'], how='left')\n",
        "test_df = pd.merge(test_df, stores, on='Store', how='left')\n",
        "test_df = test_df.drop(columns=['IsHoliday_x']).rename(columns={'IsHoliday_y': 'IsHoliday'})\n",
        "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "test_df = test_df.sort_values(by=['Store', 'Dept', 'Date'])\n",
        "\n",
        "test_df['IsHolidayWeight'] = test_df['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
        "\n",
        "# Apply same holiday features to test\n",
        "for holiday, dates in holiday_dates.items():\n",
        "    holiday_pd_dates = pd.to_datetime(dates)\n",
        "    test_df[f'{holiday}_Week'] = 0\n",
        "    for date in holiday_pd_dates:\n",
        "        test_df.loc[test_df['Date'] == date, f'{holiday}_Week'] = 1\n",
        "        for i in range(1, 4):\n",
        "            before_date = date - pd.Timedelta(weeks=i)\n",
        "            after_date = date + pd.Timedelta(weeks=i)\n",
        "            weight = 1.0 / (i + 1)\n",
        "            test_df.loc[test_df['Date'] == before_date, f'{holiday}_Week'] = weight\n",
        "            test_df.loc[test_df['Date'] == after_date, f'{holiday}_Week'] = weight\n",
        "\n",
        "# Apply same time features to test\n",
        "test_df['WeekOfYear'] = test_df['Date'].dt.isocalendar().week\n",
        "test_df['Month'] = test_df['Date'].dt.month\n",
        "test_df['Year'] = test_df['Date'].dt.year - df['Date'].dt.year.min()\n",
        "test_df['Quarter'] = test_df['Date'].dt.quarter\n",
        "test_df['Week_sin'] = np.sin(2 * np.pi * test_df['WeekOfYear'] / 52)\n",
        "test_df['Week_cos'] = np.cos(2 * np.pi * test_df['WeekOfYear'] / 52)\n",
        "test_df['Month_sin'] = np.sin(2 * np.pi * test_df['Month'] / 12)\n",
        "test_df['Month_cos'] = np.cos(2 * np.pi * test_df['Month'] / 12)\n",
        "\n",
        "# Apply same missing value handling to test\n",
        "test_df['Temperature'].fillna(test_df.groupby(['Store', 'Month'])['Temperature'].transform('mean'), inplace=True)\n",
        "test_df['Fuel_Price'].fillna(test_df.groupby('Store')['Fuel_Price'].transform('mean'), inplace=True)\n",
        "test_df['CPI'].fillna(test_df.groupby('Store')['CPI'].transform('mean'), inplace=True)\n",
        "test_df['Unemployment'].fillna(test_df.groupby('Store')['Unemployment'].transform('mean'), inplace=True)\n",
        "\n",
        "for col in markdown_cols:\n",
        "    test_df[col].fillna(0, inplace=True)\n",
        "\n",
        "# Apply same interaction features to test\n",
        "test_df['Temp_Unemployment'] = test_df['Temperature'] * test_df['Unemployment']\n",
        "test_df['Holiday_Markdown'] = test_df['IsHoliday'] * (test_df['MarkDown1'] + test_df['MarkDown2'] + test_df['MarkDown3'] + test_df['MarkDown4'] + test_df['MarkDown5'])\n",
        "test_df['Total_Markdown'] = test_df['MarkDown1'] + test_df['MarkDown2'] + test_df['MarkDown3'] + test_df['MarkDown4'] + test_df['MarkDown5']\n"
      ],
      "metadata": {
        "id": "MaUth4wAoCZB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_wmae(y_true, y_pred, weights):\n",
        "    \"\"\"Calculate Weighted Mean Absolute Error\"\"\"\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "def calculate_mae(y_true, y_pred):\n",
        "    \"\"\"Calculate Mean Absolute Error\"\"\"\n",
        "    return np.mean(np.abs(y_true - y_pred))"
      ],
      "metadata": {
        "id": "GYztIYkSoIDY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ IMPROVED SARIMAX CLASS WITH EXTERNAL REGRESSORS\n",
        "class ImprovedSARIMAX:\n",
        "    def __init__(self, order=(1, 1, 1), seasonal_order=(1, 1, 1, 52)):\n",
        "        self.order = order\n",
        "        self.seasonal_order = seasonal_order\n",
        "        self.models = {}\n",
        "        self.fallback_means = {}\n",
        "        # Define which external variables to use\n",
        "        self.exog_vars = [\n",
        "            'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "            'Total_Markdown', 'Holiday_Markdown',\n",
        "            'SuperBowl_Week', 'LaborDay_Week', 'Thanksgiving_Week', 'Christmas_Week',\n",
        "            'Week_sin', 'Week_cos', 'Month_sin', 'Month_cos'\n",
        "        ]\n",
        "\n",
        "    def _prepare_exog(self, data):\n",
        "        \"\"\"Prepare exogenous variables\"\"\"\n",
        "        try:\n",
        "            exog = data[self.exog_vars].copy()\n",
        "            # Handle any remaining NaN values\n",
        "            exog = exog.fillna(method='ffill').fillna(method='bfill')\n",
        "            return exog.values\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def fit_predict_store_dept(self, store, dept, train_data, test_data):\n",
        "        try:\n",
        "            # Get training data for this store-dept\n",
        "            train_group = train_data[(train_data['Store'] == store) & (train_data['Dept'] == dept)].copy()\n",
        "            test_group = test_data[(test_data['Store'] == store) & (test_data['Dept'] == dept)].copy()\n",
        "\n",
        "            if len(train_group) < 30:  # Need more data for SARIMAX with external regressors\n",
        "                mean_sales = train_group['Weekly_Sales'].mean() if len(train_group) > 0 else 0\n",
        "                self.fallback_means[(store, dept)] = mean_sales\n",
        "                return self._create_fallback_result(mean_sales, train_group, test_group)\n",
        "\n",
        "            # Sort by date\n",
        "            train_group = train_group.sort_values('Date')\n",
        "            test_group = test_group.sort_values('Date')\n",
        "\n",
        "            # Prepare time series and exogenous variables\n",
        "            y_train = train_group['Weekly_Sales'].values\n",
        "            exog_train = self._prepare_exog(train_group)\n",
        "\n",
        "            # Skip if constant or near-constant series\n",
        "            if len(set(y_train)) <= 2 or np.std(y_train) < 1e-3:\n",
        "                mean_sales = np.mean(y_train)\n",
        "                self.fallback_means[(store, dept)] = mean_sales\n",
        "                return self._create_fallback_result(mean_sales, train_group, test_group)\n",
        "\n",
        "            # ✅ ENHANCED SARIMAX with external regressors\n",
        "            model = SARIMAX(\n",
        "                endog=y_train,\n",
        "                exog=exog_train,\n",
        "                order=self.order,\n",
        "                seasonal_order=self.seasonal_order,\n",
        "                enforce_stationarity=False,\n",
        "                enforce_invertibility=False,\n",
        "                simple_differencing=True\n",
        "            )\n",
        "\n",
        "            # Fit with more iterations for better convergence\n",
        "            fitted_model = model.fit(\n",
        "                disp=False,\n",
        "                maxiter=100,\n",
        "                method='lbfgs',\n",
        "                optim_score='harvey'\n",
        "            )\n",
        "\n",
        "            self.models[(store, dept)] = fitted_model\n",
        "\n",
        "            # Validation split\n",
        "            split_idx = int(len(y_train) * 0.8)\n",
        "            val_exog = exog_train[split_idx:] if exog_train is not None else None\n",
        "            val_pred = fitted_model.forecast(steps=len(y_train) - split_idx, exog=val_exog)\n",
        "            val_actual = y_train[split_idx:]\n",
        "            val_weights = train_group['IsHolidayWeight'].iloc[split_idx:].values\n",
        "\n",
        "            # Test prediction\n",
        "            test_pred = None\n",
        "            if len(test_group) > 0:\n",
        "                exog_test = self._prepare_exog(test_group)\n",
        "                test_pred = fitted_model.forecast(steps=len(test_group), exog=exog_test)\n",
        "\n",
        "            return val_pred, val_actual, val_weights, test_pred, test_group\n",
        "\n",
        "        except Exception as e:\n",
        "            # Fallback to mean for problematic series\n",
        "            mean_sales = train_group['Weekly_Sales'].mean() if len(train_group) > 0 else 0\n",
        "            self.fallback_means[(store, dept)] = mean_sales\n",
        "            return self._create_fallback_result(mean_sales, train_group, test_group)\n",
        "\n",
        "    def _create_fallback_result(self, mean_sales, train_group, test_group):\n",
        "        \"\"\"Create fallback result using mean prediction\"\"\"\n",
        "        if len(train_group) == 0:\n",
        "            return None, None, None, None, test_group\n",
        "\n",
        "        split_idx = int(len(train_group) * 0.8)\n",
        "        val_length = len(train_group) - split_idx\n",
        "\n",
        "        val_pred = np.full(val_length, mean_sales)\n",
        "        val_actual = train_group['Weekly_Sales'].iloc[split_idx:].values\n",
        "        val_weights = train_group['IsHolidayWeight'].iloc[split_idx:].values\n",
        "\n",
        "        test_pred = None\n",
        "        if len(test_group) > 0:\n",
        "            test_pred = np.full(len(test_group), mean_sales)\n",
        "\n",
        "        return val_pred, val_actual, val_weights, test_pred, test_group\n"
      ],
      "metadata": {
        "id": "EtZifjN_oKjn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"sarimax-improved-model\", config={\n",
        "    \"model\": \"SARIMAX_with_external_regressors\",\n",
        "    \"seasonal_period\": 52,\n",
        "    \"order\": (1, 1, 1),\n",
        "    \"seasonal_order\": (1, 1, 1, 52),\n",
        "    \"external_regressors\": True,\n",
        "    \"features\": \"enhanced_holidays_cyclical_interactions\"\n",
        "})\n",
        "\n",
        "# ✅ IMPROVED TRAINING LOOP\n",
        "model = ImprovedSARIMAX(order=(1, 1, 1), seasonal_order=(1, 1, 1, 52))\n",
        "\n",
        "val_predictions = []\n",
        "val_actuals = []\n",
        "val_weights = []\n",
        "\n",
        "# Get unique store-dept combinations from TRAINING data\n",
        "store_dept_combinations = df[['Store', 'Dept']].drop_duplicates()\n",
        "\n",
        "print(f\"Training improved models for {len(store_dept_combinations)} store-department combinations...\")\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "fallback_count = 0\n",
        "success_count = 0\n",
        "\n",
        "# Dictionary to store predictions for each store-dept\n",
        "predictions_dict = {}\n",
        "\n",
        "for idx, (_, row) in enumerate(store_dept_combinations.iterrows()):\n",
        "    store, dept = row['Store'], row['Dept']\n",
        "\n",
        "    # Progress updates every 50 iterations (more frequent for debugging)\n",
        "    if idx % 50 == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        rate = idx / elapsed if elapsed > 0 else 0\n",
        "        eta = (len(store_dept_combinations) - idx) / rate if rate > 0 else 0\n",
        "        print(f\"Progress: {idx}/{len(store_dept_combinations)} ({idx/len(store_dept_combinations)*100:.1f}%) | \"\n",
        "              f\"Rate: {rate:.1f}/sec | ETA: {eta/60:.1f} min | Success: {success_count}, Fallback: {fallback_count}\")\n",
        "\n",
        "    result = model.fit_predict_store_dept(store, dept, df, test_df)\n",
        "\n",
        "    if result[0] is None:  # Complete failure case\n",
        "        fallback_count += 1\n",
        "        predictions_dict[(store, dept)] = {'type': 'failed', 'value': 0}\n",
        "    else:\n",
        "        val_pred, val_actual, val_w, test_pred, test_group = result\n",
        "\n",
        "        if val_pred is not None and val_actual is not None:\n",
        "            success_count += 1\n",
        "            val_predictions.extend(val_pred)\n",
        "            val_actuals.extend(val_actual)\n",
        "            val_weights.extend(val_w)\n",
        "        else:\n",
        "            fallback_count += 1\n",
        "\n",
        "        # Store predictions in dictionary\n",
        "        if test_pred is not None:\n",
        "            predictions_dict[(store, dept)] = {'type': 'model', 'predictions': test_pred}\n",
        "        else:\n",
        "            # Use historical mean as fallback\n",
        "            mean_sales = df[(df['Store'] == store) & (df['Dept'] == dept)]['Weekly_Sales'].mean()\n",
        "            predictions_dict[(store, dept)] = {'type': 'mean', 'value': mean_sales if not pd.isna(mean_sales) else 0}\n",
        "\n",
        "print(f\"\\n✅ Model training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EjlLWpLEoNwH",
        "outputId": "686c002d-5624-42ae-da47-165e90346aec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250727_174554-r8lsb35s</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/r8lsb35s' target=\"_blank\">sarimax-improved-model</a></strong> to <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/r8lsb35s' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/r8lsb35s</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training improved models for 3331 store-department combinations...\n",
            "Progress: 0/3331 (0.0%) | Rate: 0.0/sec | ETA: 0.0 min | Success: 0, Fallback: 0\n",
            "Progress: 50/3331 (1.5%) | Rate: 50.6/sec | ETA: 1.1 min | Success: 50, Fallback: 0\n",
            "Progress: 100/3331 (3.0%) | Rate: 71.5/sec | ETA: 0.8 min | Success: 100, Fallback: 0\n",
            "Progress: 150/3331 (4.5%) | Rate: 82.0/sec | ETA: 0.6 min | Success: 150, Fallback: 0\n",
            "Progress: 200/3331 (6.0%) | Rate: 88.5/sec | ETA: 0.6 min | Success: 200, Fallback: 0\n",
            "Progress: 250/3331 (7.5%) | Rate: 93.4/sec | ETA: 0.6 min | Success: 250, Fallback: 0\n",
            "Progress: 300/3331 (9.0%) | Rate: 96.7/sec | ETA: 0.5 min | Success: 300, Fallback: 0\n",
            "Progress: 350/3331 (10.5%) | Rate: 99.4/sec | ETA: 0.5 min | Success: 350, Fallback: 0\n",
            "Progress: 400/3331 (12.0%) | Rate: 101.6/sec | ETA: 0.5 min | Success: 400, Fallback: 0\n",
            "Progress: 450/3331 (13.5%) | Rate: 103.4/sec | ETA: 0.5 min | Success: 450, Fallback: 0\n",
            "Progress: 500/3331 (15.0%) | Rate: 105.0/sec | ETA: 0.4 min | Success: 500, Fallback: 0\n",
            "Progress: 550/3331 (16.5%) | Rate: 106.1/sec | ETA: 0.4 min | Success: 550, Fallback: 0\n",
            "Progress: 600/3331 (18.0%) | Rate: 104.6/sec | ETA: 0.4 min | Success: 600, Fallback: 0\n",
            "Progress: 650/3331 (19.5%) | Rate: 103.0/sec | ETA: 0.4 min | Success: 650, Fallback: 0\n",
            "Progress: 700/3331 (21.0%) | Rate: 101.6/sec | ETA: 0.4 min | Success: 700, Fallback: 0\n",
            "Progress: 750/3331 (22.5%) | Rate: 100.3/sec | ETA: 0.4 min | Success: 750, Fallback: 0\n",
            "Progress: 800/3331 (24.0%) | Rate: 99.8/sec | ETA: 0.4 min | Success: 800, Fallback: 0\n",
            "Progress: 850/3331 (25.5%) | Rate: 99.0/sec | ETA: 0.4 min | Success: 850, Fallback: 0\n",
            "Progress: 900/3331 (27.0%) | Rate: 98.6/sec | ETA: 0.4 min | Success: 900, Fallback: 0\n",
            "Progress: 950/3331 (28.5%) | Rate: 97.5/sec | ETA: 0.4 min | Success: 950, Fallback: 0\n",
            "Progress: 1000/3331 (30.0%) | Rate: 97.4/sec | ETA: 0.4 min | Success: 1000, Fallback: 0\n",
            "Progress: 1050/3331 (31.5%) | Rate: 98.1/sec | ETA: 0.4 min | Success: 1050, Fallback: 0\n",
            "Progress: 1100/3331 (33.0%) | Rate: 98.9/sec | ETA: 0.4 min | Success: 1100, Fallback: 0\n",
            "Progress: 1150/3331 (34.5%) | Rate: 99.5/sec | ETA: 0.4 min | Success: 1150, Fallback: 0\n",
            "Progress: 1200/3331 (36.0%) | Rate: 100.2/sec | ETA: 0.4 min | Success: 1200, Fallback: 0\n",
            "Progress: 1250/3331 (37.5%) | Rate: 100.8/sec | ETA: 0.3 min | Success: 1250, Fallback: 0\n",
            "Progress: 1300/3331 (39.0%) | Rate: 101.4/sec | ETA: 0.3 min | Success: 1300, Fallback: 0\n",
            "Progress: 1350/3331 (40.5%) | Rate: 98.1/sec | ETA: 0.3 min | Success: 1350, Fallback: 0\n",
            "Progress: 1400/3331 (42.0%) | Rate: 89.7/sec | ETA: 0.4 min | Success: 1400, Fallback: 0\n",
            "Progress: 1450/3331 (43.5%) | Rate: 88.0/sec | ETA: 0.4 min | Success: 1450, Fallback: 0\n",
            "Progress: 1500/3331 (45.0%) | Rate: 88.0/sec | ETA: 0.3 min | Success: 1500, Fallback: 0\n",
            "Progress: 1550/3331 (46.5%) | Rate: 88.7/sec | ETA: 0.3 min | Success: 1550, Fallback: 0\n",
            "Progress: 1600/3331 (48.0%) | Rate: 89.3/sec | ETA: 0.3 min | Success: 1600, Fallback: 0\n",
            "Progress: 1650/3331 (49.5%) | Rate: 90.0/sec | ETA: 0.3 min | Success: 1650, Fallback: 0\n",
            "Progress: 1700/3331 (51.0%) | Rate: 90.6/sec | ETA: 0.3 min | Success: 1700, Fallback: 0\n",
            "Progress: 1750/3331 (52.5%) | Rate: 91.1/sec | ETA: 0.3 min | Success: 1750, Fallback: 0\n",
            "Progress: 1800/3331 (54.0%) | Rate: 91.8/sec | ETA: 0.3 min | Success: 1800, Fallback: 0\n",
            "Progress: 1850/3331 (55.5%) | Rate: 92.3/sec | ETA: 0.3 min | Success: 1850, Fallback: 0\n",
            "Progress: 1900/3331 (57.0%) | Rate: 92.2/sec | ETA: 0.3 min | Success: 1900, Fallback: 0\n",
            "Progress: 1950/3331 (58.5%) | Rate: 92.2/sec | ETA: 0.2 min | Success: 1950, Fallback: 0\n",
            "Progress: 2000/3331 (60.0%) | Rate: 92.0/sec | ETA: 0.2 min | Success: 2000, Fallback: 0\n",
            "Progress: 2050/3331 (61.5%) | Rate: 92.0/sec | ETA: 0.2 min | Success: 2050, Fallback: 0\n",
            "Progress: 2100/3331 (63.0%) | Rate: 92.0/sec | ETA: 0.2 min | Success: 2100, Fallback: 0\n",
            "Progress: 2150/3331 (64.5%) | Rate: 91.9/sec | ETA: 0.2 min | Success: 2150, Fallback: 0\n",
            "Progress: 2200/3331 (66.0%) | Rate: 91.9/sec | ETA: 0.2 min | Success: 2200, Fallback: 0\n",
            "Progress: 2250/3331 (67.5%) | Rate: 91.7/sec | ETA: 0.2 min | Success: 2250, Fallback: 0\n",
            "Progress: 2300/3331 (69.0%) | Rate: 91.9/sec | ETA: 0.2 min | Success: 2300, Fallback: 0\n",
            "Progress: 2350/3331 (70.5%) | Rate: 92.3/sec | ETA: 0.2 min | Success: 2350, Fallback: 0\n",
            "Progress: 2400/3331 (72.1%) | Rate: 92.8/sec | ETA: 0.2 min | Success: 2400, Fallback: 0\n",
            "Progress: 2450/3331 (73.6%) | Rate: 93.2/sec | ETA: 0.2 min | Success: 2450, Fallback: 0\n",
            "Progress: 2500/3331 (75.1%) | Rate: 93.6/sec | ETA: 0.1 min | Success: 2500, Fallback: 0\n",
            "Progress: 2550/3331 (76.6%) | Rate: 94.0/sec | ETA: 0.1 min | Success: 2550, Fallback: 0\n",
            "Progress: 2600/3331 (78.1%) | Rate: 94.3/sec | ETA: 0.1 min | Success: 2600, Fallback: 0\n",
            "Progress: 2650/3331 (79.6%) | Rate: 94.5/sec | ETA: 0.1 min | Success: 2650, Fallback: 0\n",
            "Progress: 2700/3331 (81.1%) | Rate: 95.0/sec | ETA: 0.1 min | Success: 2700, Fallback: 0\n",
            "Progress: 2750/3331 (82.6%) | Rate: 95.3/sec | ETA: 0.1 min | Success: 2750, Fallback: 0\n",
            "Progress: 2800/3331 (84.1%) | Rate: 95.8/sec | ETA: 0.1 min | Success: 2800, Fallback: 0\n",
            "Progress: 2850/3331 (85.6%) | Rate: 96.1/sec | ETA: 0.1 min | Success: 2850, Fallback: 0\n",
            "Progress: 2900/3331 (87.1%) | Rate: 96.5/sec | ETA: 0.1 min | Success: 2900, Fallback: 0\n",
            "Progress: 2950/3331 (88.6%) | Rate: 96.8/sec | ETA: 0.1 min | Success: 2950, Fallback: 0\n",
            "Progress: 3000/3331 (90.1%) | Rate: 97.2/sec | ETA: 0.1 min | Success: 3000, Fallback: 0\n",
            "Progress: 3050/3331 (91.6%) | Rate: 97.5/sec | ETA: 0.0 min | Success: 3050, Fallback: 0\n",
            "Progress: 3100/3331 (93.1%) | Rate: 97.7/sec | ETA: 0.0 min | Success: 3100, Fallback: 0\n",
            "Progress: 3150/3331 (94.6%) | Rate: 98.0/sec | ETA: 0.0 min | Success: 3150, Fallback: 0\n",
            "Progress: 3200/3331 (96.1%) | Rate: 98.3/sec | ETA: 0.0 min | Success: 3200, Fallback: 0\n",
            "Progress: 3250/3331 (97.6%) | Rate: 98.6/sec | ETA: 0.0 min | Success: 3250, Fallback: 0\n",
            "Progress: 3300/3331 (99.1%) | Rate: 98.8/sec | ETA: 0.0 min | Success: 3300, Fallback: 0\n",
            "\n",
            "✅ Model training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ GENERATE PREDICTIONS FOR ALL TEST DATA ROWS\n",
        "print(f\"Generating predictions for ALL test data rows...\")\n",
        "\n",
        "# Calculate overall mean as ultimate fallback\n",
        "overall_mean = df['Weekly_Sales'].mean()\n",
        "\n",
        "submission = []\n",
        "missing_store_depts = set()\n",
        "\n",
        "# Process EVERY row in test data\n",
        "for idx, test_row in test_df.iterrows():\n",
        "    store, dept = test_row['Store'], test_row['Dept']\n",
        "\n",
        "    # Create the ID\n",
        "    test_id = f\"{store}_{dept}_{test_row['Date'].strftime('%Y-%m-%d')}\"\n",
        "\n",
        "    # Get prediction for this store-dept combination\n",
        "    if (store, dept) in predictions_dict:\n",
        "        pred_info = predictions_dict[(store, dept)]\n",
        "        if pred_info['type'] == 'model':\n",
        "            # Find which prediction index this test row corresponds to\n",
        "            test_group = test_df[(test_df['Store'] == store) & (test_df['Dept'] == dept)].sort_values('Date')\n",
        "            row_idx = test_group.index.get_loc(idx)\n",
        "            if row_idx < len(pred_info['predictions']):\n",
        "                pred_value = pred_info['predictions'][row_idx]\n",
        "            else:\n",
        "                pred_value = pred_info['predictions'][-1]  # Use last prediction\n",
        "        else:\n",
        "            pred_value = pred_info['value']\n",
        "    else:\n",
        "        # This store-dept combination wasn't in training data\n",
        "        missing_store_depts.add((store, dept))\n",
        "        pred_value = overall_mean  # Use overall mean as fallback\n",
        "\n",
        "    # ✅ ENSURE NON-NEGATIVE PREDICTIONS\n",
        "    pred_value = max(0, pred_value)\n",
        "\n",
        "    submission.append({\n",
        "        'Id': test_id,\n",
        "        'Weekly_Sales': pred_value\n",
        "    })\n",
        "\n",
        "print(f\"✅ Missing store-dept combinations (using overall mean): {len(missing_store_depts)}\")\n",
        "print(f\"✅ Generated predictions for {len(submission)} test rows\")\n",
        "\n",
        "# Verify we have the correct number of predictions\n",
        "expected_rows = len(test_df)\n",
        "actual_rows = len(submission)\n",
        "\n",
        "print(f\"\\n📊 SUBMISSION VERIFICATION:\")\n",
        "print(f\"Expected test rows: {expected_rows}\")\n",
        "print(f\"Generated predictions: {actual_rows}\")\n",
        "print(f\"Match: {'✅ YES' if expected_rows == actual_rows else '❌ NO'}\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n✅ Processing completed in {total_time/60:.1f} minutes\")\n",
        "print(f\"✅ Successful SARIMAX fits: {success_count}\")\n",
        "print(f\"⚠️  Fallback predictions: {fallback_count}\")\n",
        "\n",
        "# Calculate overall WMAE\n",
        "if len(val_predictions) > 0:\n",
        "    overall_wmae = calculate_wmae(np.array(val_actuals), np.array(val_predictions), np.array(val_weights))\n",
        "    wandb.log({\n",
        "        \"overall_wmae\": overall_wmae,\n",
        "        \"success_count\": success_count,\n",
        "        \"fallback_count\": fallback_count,\n",
        "        \"total_time_minutes\": total_time/60,\n",
        "        \"submission_rows\": len(submission),\n",
        "        \"expected_rows\": expected_rows\n",
        "    })\n",
        "    print(f\"✅ Overall Validation WMAE: {overall_wmae:.4f}\")\n",
        "else:\n",
        "    print(\"❌ No validation predictions generated\")\n",
        "\n",
        "# Create submission DataFrame\n",
        "submission_df = pd.DataFrame(submission)\n",
        "print(f\"✅ Generated {len(submission_df)} predictions\")\n",
        "print(f\"✅ Sample predictions:\\n{submission_df.head()}\")\n",
        "\n",
        "# ✅ QUALITY CHECKS\n",
        "print(f\"\\n📈 PREDICTION QUALITY CHECKS:\")\n",
        "pred_values = submission_df['Weekly_Sales'].values\n",
        "print(f\"Min prediction: {pred_values.min():.2f}\")\n",
        "print(f\"Max prediction: {pred_values.max():.2f}\")\n",
        "print(f\"Mean prediction: {pred_values.mean():.2f}\")\n",
        "print(f\"Negative predictions: {(pred_values < 0).sum()}\")\n",
        "\n",
        "# Save submission\n",
        "submission_df.to_csv('/content/sarimax_improved_submission.csv', index=False)\n",
        "print(\"✅ Submission saved to /content/sarimax_improved_submission.csv\")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "6NhwMI19oRQC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "013e5b09-9a8c-4054-9078-a8248e334d46"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions for ALL test data rows...\n",
            "✅ Missing store-dept combinations (using overall mean): 11\n",
            "✅ Generated predictions for 115064 test rows\n",
            "\n",
            "📊 SUBMISSION VERIFICATION:\n",
            "Expected test rows: 115064\n",
            "Generated predictions: 115064\n",
            "Match: ✅ YES\n",
            "\n",
            "✅ Processing completed in 5.0 minutes\n",
            "✅ Successful SARIMAX fits: 3331\n",
            "⚠️  Fallback predictions: 0\n",
            "✅ Overall Validation WMAE: 2379.8436\n",
            "✅ Generated 115064 predictions\n",
            "✅ Sample predictions:\n",
            "               Id  Weekly_Sales\n",
            "0  1_1_2012-11-02  22513.322937\n",
            "1  1_1_2012-11-09  22513.322937\n",
            "2  1_1_2012-11-16  22513.322937\n",
            "3  1_1_2012-11-23  22513.322937\n",
            "4  1_1_2012-11-30  22513.322937\n",
            "\n",
            "📈 PREDICTION QUALITY CHECKS:\n",
            "Min prediction: 0.00\n",
            "Max prediction: 182527.96\n",
            "Mean prediction: 15982.75\n",
            "Negative predictions: 0\n",
            "✅ Submission saved to /content/sarimax_improved_submission.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>expected_rows</td><td>▁</td></tr><tr><td>fallback_count</td><td>▁</td></tr><tr><td>overall_wmae</td><td>▁</td></tr><tr><td>submission_rows</td><td>▁</td></tr><tr><td>success_count</td><td>▁</td></tr><tr><td>total_time_minutes</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>expected_rows</td><td>115064</td></tr><tr><td>fallback_count</td><td>0</td></tr><tr><td>overall_wmae</td><td>2379.84357</td></tr><tr><td>submission_rows</td><td>115064</td></tr><tr><td>success_count</td><td>3331</td></tr><tr><td>total_time_minutes</td><td>4.958</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sarimax-improved-model</strong> at: <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/r8lsb35s' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/r8lsb35s</a><br> View project at: <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250727_174554-r8lsb35s/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# public score 4900"
      ],
      "metadata": {
        "id": "K7umRbV6wUPm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUZy/BPnw1Pz57kpLFyvz/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}