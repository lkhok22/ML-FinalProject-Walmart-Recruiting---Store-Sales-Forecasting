{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AleksandreBakhtadze/ML-abakh22-assignment-1/blob/main/model_experiment_SARIMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "73aYB6u6aXFm"
      },
      "outputs": [],
      "source": [
        "# ✅ Only install what you need for SARIMAX and logging\n",
        "!pip install statsmodels wandb --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import wandb\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Ys96BI5qaYiA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install pandas numpy matplotlib seaborn scikit-learn torch torchvision wandb pyyaml darts --quiet\n",
        "import wandb\n",
        "wandb.login(key=\"eccf2c915699fc032ad678daf0fd4b5ac60bf87c\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwGrXiXscYIl",
        "outputId": "b858be6f-b68d-4f0a-9e59-fd3aa96ef001"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive and extract data\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "zip_path = '/content/drive/MyDrive/ML-FinalProject/data.zip'\n",
        "extract_to = '/content/walmart_data/'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "for file_name in os.listdir(extract_to):\n",
        "    if file_name.endswith('.zip'):\n",
        "        with zipfile.ZipFile(os.path.join(extract_to, file_name), 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "print(\"✅ Extracted files:\", os.listdir(extract_to))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjWcYXmtcalW",
        "outputId": "6d2f054e-893d-4121-e5dc-24fc8d5e254d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Extracted files: ['test.csv.zip', 'features.csv', 'train.csv.zip', 'train.csv', 'features.csv.zip', 'test.csv', 'stores.csv', 'sampleSubmission.csv.zip', 'sampleSubmission.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "train = pd.read_csv('/content/walmart_data/train.csv')\n",
        "features = pd.read_csv('/content/walmart_data/features.csv')\n",
        "stores = pd.read_csv('/content/walmart_data/stores.csv')\n",
        "test = pd.read_csv('/content/walmart_data/test.csv')\n",
        "\n",
        "# Merge train with features and stores\n",
        "df = pd.merge(train, features, on=['Store', 'Date'], how='left')\n",
        "df = pd.merge(df, stores, on='Store', how='left')\n",
        "df = df.drop(columns=['IsHoliday_x']).rename(columns={'IsHoliday_y': 'IsHoliday'})\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values(by=['Store', 'Dept', 'Date'])\n",
        "\n",
        "# ✅ CREATE THE MISSING IsHolidayWeight COLUMN\n",
        "df['IsHolidayWeight'] = df['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
        "\n",
        "# Add holiday-specific features\n",
        "holiday_dates = {\n",
        "    'SuperBowl': ['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08'],\n",
        "    'LaborDay': ['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06'],\n",
        "    'Thanksgiving': ['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29'],\n",
        "    'Christmas': ['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27']\n",
        "}\n",
        "for holiday, dates in holiday_dates.items():\n",
        "    df[holiday] = df['Date'].isin(pd.to_datetime(dates)).astype(int)\n",
        "\n",
        "# Add time-based features\n",
        "df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Year'] = df['Date'].dt.year - df['Date'].dt.year.min()\n",
        "\n",
        "# Add holiday proximity features\n",
        "for holiday, dates in holiday_dates.items():\n",
        "    for date in pd.to_datetime(dates):\n",
        "        df[f'{holiday}_Before'] = ((df['Date'] < date) & (df['Date'] >= date - pd.Timedelta(weeks=2))).astype(int)\n",
        "        df[f'{holiday}_After'] = ((df['Date'] > date) & (df['Date'] <= date + pd.Timedelta(weeks=2))).astype(int)\n",
        "\n",
        "# Load and preprocess test data\n",
        "test_df = pd.merge(test, features, on=['Store', 'Date'], how='left')\n",
        "test_df = pd.merge(test_df, stores, on='Store', how='left')\n",
        "test_df = test_df.drop(columns=['IsHoliday_x']).rename(columns={'IsHoliday_y': 'IsHoliday'})\n",
        "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "test_df = test_df.sort_values(by=['Store', 'Dept', 'Date'])\n",
        "\n",
        "# ✅ ADD IsHolidayWeight TO TEST DATA TOO\n",
        "test_df['IsHolidayWeight'] = test_df['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
        "\n",
        "# Add holiday-specific features to test\n",
        "for holiday, dates in holiday_dates.items():\n",
        "    test_df[holiday] = test_df['Date'].isin(pd.to_datetime(dates)).astype(int)\n",
        "\n",
        "# Add time-based features to test\n",
        "test_df['WeekOfYear'] = test_df['Date'].dt.isocalendar().week\n",
        "test_df['Month'] = test_df['Date'].dt.month\n",
        "# ✅ FIX: Use the same min year as training data\n",
        "test_df['Year'] = test_df['Date'].dt.year - df['Date'].dt.year.min()\n"
      ],
      "metadata": {
        "id": "MaUth4wAoCZB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_wmae(y_true, y_pred, weights):\n",
        "    \"\"\"Calculate Weighted Mean Absolute Error\"\"\"\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "def calculate_mae(y_true, y_pred):\n",
        "    \"\"\"Calculate Mean Absolute Error\"\"\"\n",
        "    return np.mean(np.abs(y_true - y_pred))"
      ],
      "metadata": {
        "id": "GYztIYkSoIDY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ FAST SARIMAX APPROACH with optimizations\n",
        "class FastSARIMAX:\n",
        "    def __init__(self, order=(1, 0, 0), seasonal_order=(0, 0, 0, 52)):  # Simpler default\n",
        "        self.order = order\n",
        "        self.seasonal_order = seasonal_order\n",
        "        self.models = {}\n",
        "        self.fallback_means = {}\n",
        "\n",
        "    def fit_predict_store_dept(self, store, dept, train_data, test_data):\n",
        "        try:\n",
        "            # Get training data for this store-dept\n",
        "            train_group = train_data[(train_data['Store'] == store) & (train_data['Dept'] == dept)].copy()\n",
        "            test_group = test_data[(test_data['Store'] == store) & (test_data['Dept'] == dept)].copy()\n",
        "\n",
        "            if len(train_group) < 20:  # Need more data for SARIMAX\n",
        "                mean_sales = train_group['Weekly_Sales'].mean() if len(train_group) > 0 else 0\n",
        "                self.fallback_means[(store, dept)] = mean_sales\n",
        "                return self._create_fallback_result(mean_sales, train_group, test_group)\n",
        "\n",
        "            # Sort by date\n",
        "            train_group = train_group.sort_values('Date')\n",
        "            test_group = test_group.sort_values('Date')\n",
        "\n",
        "            # Prepare time series\n",
        "            y_train = train_group['Weekly_Sales'].values\n",
        "\n",
        "            # Skip if constant or near-constant series\n",
        "            if len(set(y_train)) <= 2 or np.std(y_train) < 1e-3:\n",
        "                mean_sales = np.mean(y_train)\n",
        "                self.fallback_means[(store, dept)] = mean_sales\n",
        "                return self._create_fallback_result(mean_sales, train_group, test_group)\n",
        "\n",
        "            # ✅ FASTER SARIMAX with reduced complexity and timeout\n",
        "            model = SARIMAX(\n",
        "                endog=y_train,\n",
        "                order=self.order,\n",
        "                seasonal_order=self.seasonal_order,\n",
        "                enforce_stationarity=False,\n",
        "                enforce_invertibility=False,\n",
        "                simple_differencing=True  # Faster differencing\n",
        "            )\n",
        "\n",
        "            # ✅ FASTER FITTING with strict limits\n",
        "            fitted_model = model.fit(\n",
        "                disp=False,\n",
        "                maxiter=50,  # Reduced iterations\n",
        "                method='lbfgs',  # Faster optimizer\n",
        "                optim_score='harvey',  # Faster scoring\n",
        "                low_memory=True\n",
        "            )\n",
        "\n",
        "            self.models[(store, dept)] = fitted_model\n",
        "\n",
        "            # Validation split\n",
        "            split_idx = int(len(y_train) * 0.8)\n",
        "            val_pred = fitted_model.forecast(steps=len(y_train) - split_idx)\n",
        "            val_actual = y_train[split_idx:]\n",
        "            val_weights = train_group['IsHolidayWeight'].iloc[split_idx:].values\n",
        "\n",
        "            # Test prediction\n",
        "            test_pred = None\n",
        "            if len(test_group) > 0:\n",
        "                test_pred = fitted_model.forecast(steps=len(test_group))\n",
        "\n",
        "            return val_pred, val_actual, val_weights, test_pred, test_group\n",
        "\n",
        "        except Exception as e:\n",
        "            # Fallback to mean for problematic series\n",
        "            mean_sales = train_group['Weekly_Sales'].mean() if len(train_group) > 0 else 0\n",
        "            self.fallback_means[(store, dept)] = mean_sales\n",
        "            return self._create_fallback_result(mean_sales, train_group, test_group)\n",
        "\n",
        "    def _create_fallback_result(self, mean_sales, train_group, test_group):\n",
        "        \"\"\"Create fallback result using mean prediction\"\"\"\n",
        "        if len(train_group) == 0:\n",
        "            return None, None, None, None, test_group\n",
        "\n",
        "        split_idx = int(len(train_group) * 0.8)\n",
        "        val_length = len(train_group) - split_idx\n",
        "\n",
        "        val_pred = np.full(val_length, mean_sales)\n",
        "        val_actual = train_group['Weekly_Sales'].iloc[split_idx:].values\n",
        "        val_weights = train_group['IsHolidayWeight'].iloc[split_idx:].values\n",
        "\n",
        "        test_pred = None\n",
        "        if len(test_group) > 0:\n",
        "            test_pred = np.full(len(test_group), mean_sales)\n",
        "\n",
        "        return val_pred, val_actual, val_weights, test_pred, test_group"
      ],
      "metadata": {
        "id": "EtZifjN_oKjn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"sarimax-fast-model\", config={\n",
        "    \"model\": \"SARIMAX\",\n",
        "    \"seasonal_period\": 52,\n",
        "    \"order\": (1, 0, 0),  # Simpler for speed\n",
        "    \"seasonal_order\": (0, 0, 0, 52)  # Non-seasonal for speed\n",
        "})\n",
        "\n",
        "# ✅ FAST TRAINING LOOP with progress tracking\n",
        "model = FastSARIMAX(order=(1, 0, 0), seasonal_order=(0, 0, 0, 52))  # Simpler for speed\n",
        "\n",
        "val_predictions = []\n",
        "val_actuals = []\n",
        "val_weights = []\n",
        "\n",
        "# Get unique store-dept combinations from TRAINING data\n",
        "store_dept_combinations = df[['Store', 'Dept']].drop_duplicates()\n",
        "\n",
        "print(f\"Training models for {len(store_dept_combinations)} store-department combinations...\")\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "fallback_count = 0\n",
        "success_count = 0\n",
        "\n",
        "# Dictionary to store predictions for each store-dept\n",
        "predictions_dict = {}\n",
        "\n",
        "for idx, (_, row) in enumerate(store_dept_combinations.iterrows()):\n",
        "    store, dept = row['Store'], row['Dept']\n",
        "\n",
        "    # Progress updates every 100 iterations\n",
        "    if idx % 100 == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        rate = idx / elapsed if elapsed > 0 else 0\n",
        "        eta = (len(store_dept_combinations) - idx) / rate if rate > 0 else 0\n",
        "        print(f\"Progress: {idx}/{len(store_dept_combinations)} ({idx/len(store_dept_combinations)*100:.1f}%) | \"\n",
        "              f\"Rate: {rate:.1f}/sec | ETA: {eta/60:.1f} min | Success: {success_count}, Fallback: {fallback_count}\")\n",
        "\n",
        "    result = model.fit_predict_store_dept(store, dept, df, test_df)\n",
        "\n",
        "    if result[0] is None:  # Complete failure case\n",
        "        fallback_count += 1\n",
        "        predictions_dict[(store, dept)] = {'type': 'failed', 'value': 0}\n",
        "    else:\n",
        "        val_pred, val_actual, val_w, test_pred, test_group = result\n",
        "\n",
        "        if val_pred is not None and val_actual is not None:\n",
        "            success_count += 1\n",
        "            val_predictions.extend(val_pred)\n",
        "            val_actuals.extend(val_actual)\n",
        "            val_weights.extend(val_w)\n",
        "\n",
        "            # Log less frequently to save time\n",
        "            if idx % 500 == 0:\n",
        "                val_mae_score = calculate_mae(val_actual, val_pred)\n",
        "                wandb.log({\n",
        "                    f\"Store_{store}_Dept_{dept}_val_mae\": val_mae_score,\n",
        "                    \"store\": store,\n",
        "                    \"dept\": dept\n",
        "                })\n",
        "        else:\n",
        "            fallback_count += 1\n",
        "\n",
        "        # Store predictions in dictionary\n",
        "        if test_pred is not None:\n",
        "            predictions_dict[(store, dept)] = {'type': 'model', 'predictions': test_pred}\n",
        "        else:\n",
        "            # Use historical mean as fallback\n",
        "            mean_sales = df[(df['Store'] == store) & (df['Dept'] == dept)]['Weekly_Sales'].mean()\n",
        "            predictions_dict[(store, dept)] = {'type': 'mean', 'value': mean_sales if not pd.isna(mean_sales) else 0}\n",
        "\n",
        "print(f\"\\n✅ Model training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "EjlLWpLEoNwH",
        "outputId": "5da178a8-2221-4cde-c52f-38607b1e03d6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250727_162844-3ys37h57</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/3ys37h57' target=\"_blank\">sarimax-fast-model</a></strong> to <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/3ys37h57' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/3ys37h57</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training models for 3331 store-department combinations...\n",
            "Progress: 0/3331 (0.0%) | Rate: 0.0/sec | ETA: 0.0 min | Success: 0, Fallback: 0\n",
            "Progress: 100/3331 (3.0%) | Rate: 13.5/sec | ETA: 4.0 min | Success: 100, Fallback: 0\n",
            "Progress: 200/3331 (6.0%) | Rate: 16.7/sec | ETA: 3.1 min | Success: 200, Fallback: 0\n",
            "Progress: 300/3331 (9.0%) | Rate: 20.7/sec | ETA: 2.4 min | Success: 300, Fallback: 0\n",
            "Progress: 400/3331 (12.0%) | Rate: 24.7/sec | ETA: 2.0 min | Success: 400, Fallback: 0\n",
            "Progress: 500/3331 (15.0%) | Rate: 27.0/sec | ETA: 1.7 min | Success: 500, Fallback: 0\n",
            "Progress: 600/3331 (18.0%) | Rate: 28.5/sec | ETA: 1.6 min | Success: 600, Fallback: 0\n",
            "Progress: 700/3331 (21.0%) | Rate: 30.8/sec | ETA: 1.4 min | Success: 700, Fallback: 0\n",
            "Progress: 800/3331 (24.0%) | Rate: 32.7/sec | ETA: 1.3 min | Success: 800, Fallback: 0\n",
            "Progress: 900/3331 (27.0%) | Rate: 34.5/sec | ETA: 1.2 min | Success: 900, Fallback: 0\n",
            "Progress: 1000/3331 (30.0%) | Rate: 35.7/sec | ETA: 1.1 min | Success: 1000, Fallback: 0\n",
            "Progress: 1100/3331 (33.0%) | Rate: 37.0/sec | ETA: 1.0 min | Success: 1100, Fallback: 0\n",
            "Progress: 1200/3331 (36.0%) | Rate: 38.2/sec | ETA: 0.9 min | Success: 1200, Fallback: 0\n",
            "Progress: 1300/3331 (39.0%) | Rate: 38.4/sec | ETA: 0.9 min | Success: 1300, Fallback: 0\n",
            "Progress: 1400/3331 (42.0%) | Rate: 38.7/sec | ETA: 0.8 min | Success: 1400, Fallback: 0\n",
            "Progress: 1500/3331 (45.0%) | Rate: 39.6/sec | ETA: 0.8 min | Success: 1500, Fallback: 0\n",
            "Progress: 1600/3331 (48.0%) | Rate: 40.2/sec | ETA: 0.7 min | Success: 1600, Fallback: 0\n",
            "Progress: 1700/3331 (51.0%) | Rate: 41.1/sec | ETA: 0.7 min | Success: 1700, Fallback: 0\n",
            "Progress: 1800/3331 (54.0%) | Rate: 41.8/sec | ETA: 0.6 min | Success: 1800, Fallback: 0\n",
            "Progress: 1900/3331 (57.0%) | Rate: 42.5/sec | ETA: 0.6 min | Success: 1900, Fallback: 0\n",
            "Progress: 2000/3331 (60.0%) | Rate: 42.7/sec | ETA: 0.5 min | Success: 2000, Fallback: 0\n",
            "Progress: 2100/3331 (63.0%) | Rate: 42.5/sec | ETA: 0.5 min | Success: 2100, Fallback: 0\n",
            "Progress: 2200/3331 (66.0%) | Rate: 43.1/sec | ETA: 0.4 min | Success: 2200, Fallback: 0\n",
            "Progress: 2300/3331 (69.0%) | Rate: 43.4/sec | ETA: 0.4 min | Success: 2300, Fallback: 0\n",
            "Progress: 2400/3331 (72.1%) | Rate: 43.9/sec | ETA: 0.4 min | Success: 2400, Fallback: 0\n",
            "Progress: 2500/3331 (75.1%) | Rate: 44.5/sec | ETA: 0.3 min | Success: 2500, Fallback: 0\n",
            "Progress: 2600/3331 (78.1%) | Rate: 44.9/sec | ETA: 0.3 min | Success: 2600, Fallback: 0\n",
            "Progress: 2700/3331 (81.1%) | Rate: 45.5/sec | ETA: 0.2 min | Success: 2700, Fallback: 0\n",
            "Progress: 2800/3331 (84.1%) | Rate: 45.5/sec | ETA: 0.2 min | Success: 2800, Fallback: 0\n",
            "Progress: 2900/3331 (87.1%) | Rate: 45.4/sec | ETA: 0.2 min | Success: 2900, Fallback: 0\n",
            "Progress: 3000/3331 (90.1%) | Rate: 45.8/sec | ETA: 0.1 min | Success: 3000, Fallback: 0\n",
            "Progress: 3100/3331 (93.1%) | Rate: 46.1/sec | ETA: 0.1 min | Success: 3100, Fallback: 0\n",
            "Progress: 3200/3331 (96.1%) | Rate: 46.3/sec | ETA: 0.0 min | Success: 3200, Fallback: 0\n",
            "Progress: 3300/3331 (99.1%) | Rate: 46.7/sec | ETA: 0.0 min | Success: 3300, Fallback: 0\n",
            "\n",
            "✅ Model training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ NOW GENERATE PREDICTIONS FOR ALL TEST DATA ROWS\n",
        "print(f\"Generating predictions for ALL test data rows...\")\n",
        "\n",
        "# Calculate overall mean as ultimate fallback\n",
        "overall_mean = df['Weekly_Sales'].mean()\n",
        "\n",
        "submission = []\n",
        "missing_store_depts = set()\n",
        "\n",
        "# Process EVERY row in test data\n",
        "for idx, test_row in test_df.iterrows():\n",
        "    store, dept = test_row['Store'], test_row['Dept']\n",
        "\n",
        "    # Create the ID\n",
        "    test_id = f\"{store}_{dept}_{test_row['Date'].strftime('%Y-%m-%d')}\"\n",
        "\n",
        "    # Get prediction for this store-dept combination\n",
        "    if (store, dept) in predictions_dict:\n",
        "        pred_info = predictions_dict[(store, dept)]\n",
        "        if pred_info['type'] == 'model':\n",
        "            # Find which prediction index this test row corresponds to\n",
        "            test_group = test_df[(test_df['Store'] == store) & (test_df['Dept'] == dept)].sort_values('Date')\n",
        "            row_idx = test_group.index.get_loc(idx)\n",
        "            if row_idx < len(pred_info['predictions']):\n",
        "                pred_value = pred_info['predictions'][row_idx]\n",
        "            else:\n",
        "                pred_value = pred_info['predictions'][-1]  # Use last prediction\n",
        "        else:\n",
        "            pred_value = pred_info['value']\n",
        "    else:\n",
        "        # This store-dept combination wasn't in training data\n",
        "        missing_store_depts.add((store, dept))\n",
        "        pred_value = overall_mean  # Use overall mean as fallback\n",
        "\n",
        "    submission.append({\n",
        "        'Id': test_id,\n",
        "        'Weekly_Sales': pred_value\n",
        "    })\n",
        "\n",
        "print(f\"✅ Missing store-dept combinations (using overall mean): {len(missing_store_depts)}\")\n",
        "if missing_store_depts:\n",
        "    print(f\"Examples: {list(missing_store_depts)[:5]}\")\n",
        "\n",
        "print(f\"✅ Generated predictions for {len(submission)} test rows\")\n",
        "\n",
        "# Verify we have the correct number of predictions\n",
        "expected_rows = len(test_df)\n",
        "actual_rows = len(submission)\n",
        "\n",
        "print(f\"\\n📊 SUBMISSION VERIFICATION:\")\n",
        "print(f\"Expected test rows: {expected_rows}\")\n",
        "print(f\"Generated predictions: {actual_rows}\")\n",
        "print(f\"Match: {'✅ YES' if expected_rows == actual_rows else '❌ NO'}\")\n",
        "\n",
        "if expected_rows != actual_rows:\n",
        "    print(f\"❌ Missing {expected_rows - actual_rows} predictions!\")\n",
        "\n",
        "    # Find missing test rows\n",
        "    test_ids = set(f\"{row['Store']}_{row['Dept']}_{row['Date'].strftime('%Y-%m-%d')}\"\n",
        "                   for _, row in test_df.iterrows())\n",
        "    submission_ids = set(row['Id'] for row in submission)\n",
        "    missing_ids = test_ids - submission_ids\n",
        "\n",
        "    print(f\"Missing IDs: {len(missing_ids)}\")\n",
        "    if missing_ids:\n",
        "        print(f\"Examples: {list(missing_ids)[:5]}\")\n",
        "\n",
        "        # Add missing predictions with overall mean\n",
        "        for missing_id in missing_ids:\n",
        "            submission.append({\n",
        "                'Id': missing_id,\n",
        "                'Weekly_Sales': overall_mean\n",
        "            })\n",
        "\n",
        "        print(f\"✅ Added {len(missing_ids)} missing predictions\")\n",
        "        print(f\"✅ Final submission size: {len(submission)}\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n✅ Processing completed in {total_time/60:.1f} minutes\")\n",
        "print(f\"✅ Successful SARIMAX fits: {success_count}\")\n",
        "print(f\"⚠️  Fallback predictions: {fallback_count}\")\n",
        "\n",
        "# Calculate overall WMAE\n",
        "if len(val_predictions) > 0:\n",
        "    overall_wmae = calculate_wmae(np.array(val_actuals), np.array(val_predictions), np.array(val_weights))\n",
        "    wandb.log({\n",
        "        \"overall_wmae\": overall_wmae,\n",
        "        \"success_count\": success_count,\n",
        "        \"fallback_count\": fallback_count,\n",
        "        \"total_time_minutes\": total_time/60,\n",
        "        \"submission_rows\": len(submission),\n",
        "        \"expected_rows\": expected_rows\n",
        "    })\n",
        "    print(f\"✅ Overall Validation WMAE: {overall_wmae:.4f}\")\n",
        "else:\n",
        "    print(\"❌ No validation predictions generated\")\n",
        "\n",
        "# Create submission DataFrame\n",
        "submission_df = pd.DataFrame(submission)\n",
        "print(f\"✅ Generated {len(submission_df)} predictions\")\n",
        "print(f\"✅ Sample predictions:\\n{submission_df.head()}\")\n",
        "\n",
        "# Save submission\n",
        "submission_df.to_csv('/content/sarimax_submission.csv', index=False)\n",
        "print(\"✅ Submission saved to /content/sarimax_submission.csv\")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "6NhwMI19oRQC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0050e44c-1060-49a5-fb6c-e747a74b1782"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions for ALL test data rows...\n",
            "✅ Missing store-dept combinations (using overall mean): 11\n",
            "Examples: [(18, 43), (24, 43), (36, 30), (37, 29), (25, 99)]\n",
            "✅ Generated predictions for 115064 test rows\n",
            "\n",
            "📊 SUBMISSION VERIFICATION:\n",
            "Expected test rows: 115064\n",
            "Generated predictions: 115064\n",
            "Match: ✅ YES\n",
            "\n",
            "✅ Processing completed in 4.1 minutes\n",
            "✅ Successful SARIMAX fits: 3331\n",
            "⚠️  Fallback predictions: 0\n",
            "✅ Overall Validation WMAE: 5034.1327\n",
            "✅ Generated 115064 predictions\n",
            "✅ Sample predictions:\n",
            "               Id  Weekly_Sales\n",
            "0  1_1_2012-11-02  25478.484605\n",
            "1  1_1_2012-11-09  23699.670720\n",
            "2  1_1_2012-11-16  22045.047064\n",
            "3  1_1_2012-11-23  20505.943134\n",
            "4  1_1_2012-11-30  19074.293766\n",
            "✅ Submission saved to /content/sarimax_submission.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Store_14_Dept_13_val_mae</td><td>▁</td></tr><tr><td>Store_1_Dept_1_val_mae</td><td>▁</td></tr><tr><td>Store_20_Dept_51_val_mae</td><td>▁</td></tr><tr><td>Store_27_Dept_10_val_mae</td><td>▁</td></tr><tr><td>Store_33_Dept_97_val_mae</td><td>▁</td></tr><tr><td>Store_41_Dept_6_val_mae</td><td>▁</td></tr><tr><td>Store_7_Dept_52_val_mae</td><td>▁</td></tr><tr><td>dept</td><td>▁▅▂▅▂█▁</td></tr><tr><td>expected_rows</td><td>▁</td></tr><tr><td>fallback_count</td><td>▁</td></tr><tr><td>overall_wmae</td><td>▁</td></tr><tr><td>store</td><td>▁▂▃▄▆▇█</td></tr><tr><td>submission_rows</td><td>▁</td></tr><tr><td>success_count</td><td>▁</td></tr><tr><td>total_time_minutes</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Store_14_Dept_13_val_mae</td><td>7953.76273</td></tr><tr><td>Store_1_Dept_1_val_mae</td><td>8880.30376</td></tr><tr><td>Store_20_Dept_51_val_mae</td><td>11.06424</td></tr><tr><td>Store_27_Dept_10_val_mae</td><td>5431.11492</td></tr><tr><td>Store_33_Dept_97_val_mae</td><td>1640.43571</td></tr><tr><td>Store_41_Dept_6_val_mae</td><td>4382.32999</td></tr><tr><td>Store_7_Dept_52_val_mae</td><td>475.3113</td></tr><tr><td>dept</td><td>6</td></tr><tr><td>expected_rows</td><td>115064</td></tr><tr><td>fallback_count</td><td>0</td></tr><tr><td>overall_wmae</td><td>5034.13274</td></tr><tr><td>store</td><td>41</td></tr><tr><td>submission_rows</td><td>115064</td></tr><tr><td>success_count</td><td>3331</td></tr><tr><td>total_time_minutes</td><td>4.05435</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sarimax-fast-model</strong> at: <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/3ys37h57' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/3ys37h57</a><br> View project at: <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250727_162844-3ys37h57/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# public score 7900"
      ],
      "metadata": {
        "id": "K7umRbV6wUPm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMuzItw1a1CATbrrkfV28i4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}