{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:41:09.004788Z","iopub.execute_input":"2025-07-28T17:41:09.005141Z","iopub.status.idle":"2025-07-28T17:41:11.591813Z","shell.execute_reply.started":"2025-07-28T17:41:09.005108Z","shell.execute_reply":"2025-07-28T17:41:11.590366Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\n/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import rcParams\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import tree","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:42:09.810715Z","iopub.execute_input":"2025-07-28T17:42:09.811099Z","iopub.status.idle":"2025-07-28T17:42:11.443316Z","shell.execute_reply.started":"2025-07-28T17:42:09.811070Z","shell.execute_reply":"2025-07-28T17:42:11.442313Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"features = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/features.csv.zip')\ntrain = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/train.csv.zip')\ntest = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/test.csv.zip')\nstores = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/stores.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:42:17.631871Z","iopub.execute_input":"2025-07-28T17:42:17.632390Z","iopub.status.idle":"2025-07-28T17:42:18.073083Z","shell.execute_reply.started":"2025-07-28T17:42:17.632360Z","shell.execute_reply":"2025-07-28T17:42:18.072116Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# merging data","metadata":{}},{"cell_type":"code","source":"df = pd.merge(train, features, how = \"left\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:43:09.077695Z","iopub.execute_input":"2025-07-28T17:43:09.078102Z","iopub.status.idle":"2025-07-28T17:43:09.239318Z","shell.execute_reply.started":"2025-07-28T17:43:09.078072Z","shell.execute_reply":"2025-07-28T17:43:09.238037Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df = pd.merge(df, stores, how = \"left\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:43:18.998645Z","iopub.execute_input":"2025-07-28T17:43:18.999051Z","iopub.status.idle":"2025-07-28T17:43:19.094969Z","shell.execute_reply.started":"2025-07-28T17:43:18.999015Z","shell.execute_reply":"2025-07-28T17:43:19.093986Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# transform data to datetime","metadata":{}},{"cell_type":"code","source":"df.Date = pd.to_datetime(df.Date)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:43:26.996055Z","iopub.execute_input":"2025-07-28T17:43:26.996369Z","iopub.status.idle":"2025-07-28T17:43:27.060746Z","shell.execute_reply.started":"2025-07-28T17:43:26.996345Z","shell.execute_reply":"2025-07-28T17:43:27.059932Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df.set_index(keys = \"Date\", inplace = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:43:34.300041Z","iopub.execute_input":"2025-07-28T17:43:34.300368Z","iopub.status.idle":"2025-07-28T17:43:34.306646Z","shell.execute_reply.started":"2025-07-28T17:43:34.300341Z","shell.execute_reply":"2025-07-28T17:43:34.305710Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df.drop(axis = 1, columns = [\"MarkDown1\", \"MarkDown2\",\"MarkDown3\",\"MarkDown4\", \"MarkDown5\"], inplace = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:43:46.884395Z","iopub.execute_input":"2025-07-28T17:43:46.884890Z","iopub.status.idle":"2025-07-28T17:43:46.905030Z","shell.execute_reply.started":"2025-07-28T17:43:46.884851Z","shell.execute_reply":"2025-07-28T17:43:46.903938Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# handle categoricals ","metadata":{}},{"cell_type":"code","source":"df.Store = pd.Categorical(df.Store)\ndf.Dept = pd.Categorical(df.Dept)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:43:55.037640Z","iopub.execute_input":"2025-07-28T17:43:55.037969Z","iopub.status.idle":"2025-07-28T17:43:55.054411Z","shell.execute_reply.started":"2025-07-28T17:43:55.037942Z","shell.execute_reply":"2025-07-28T17:43:55.053382Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df.Weekly_Sales[df.Weekly_Sales == max(df.Weekly_Sales)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:45:47.121667Z","iopub.execute_input":"2025-07-28T17:45:47.122116Z","iopub.status.idle":"2025-07-28T17:45:47.171603Z","shell.execute_reply.started":"2025-07-28T17:45:47.122089Z","shell.execute_reply":"2025-07-28T17:45:47.170678Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Date\n2010-11-26    693099.36\nName: Weekly_Sales, dtype: float64"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"df.IsHoliday = pd.Categorical(df.IsHoliday)\ndf.Type = pd.Categorical(df.Type)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:46:47.639056Z","iopub.execute_input":"2025-07-28T17:46:47.639391Z","iopub.status.idle":"2025-07-28T17:46:47.670904Z","shell.execute_reply.started":"2025-07-28T17:46:47.639366Z","shell.execute_reply":"2025-07-28T17:46:47.669963Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"df_X = df.drop(\"Weekly_Sales\", axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:46:57.095596Z","iopub.execute_input":"2025-07-28T17:46:57.095899Z","iopub.status.idle":"2025-07-28T17:46:57.106310Z","shell.execute_reply.started":"2025-07-28T17:46:57.095876Z","shell.execute_reply":"2025-07-28T17:46:57.105179Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"df_Y = df.Weekly_Sales","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:47:04.559812Z","iopub.execute_input":"2025-07-28T17:47:04.560695Z","iopub.status.idle":"2025-07-28T17:47:04.565678Z","shell.execute_reply.started":"2025-07-28T17:47:04.560667Z","shell.execute_reply":"2025-07-28T17:47:04.564634Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"df_X = pd.get_dummies(df_X, drop_first = True)\ndf_X.reset_index(inplace = True)\ndf_X = df_X.iloc[:,1:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:47:19.409697Z","iopub.execute_input":"2025-07-28T17:47:19.410055Z","iopub.status.idle":"2025-07-28T17:47:19.570271Z","shell.execute_reply.started":"2025-07-28T17:47:19.410026Z","shell.execute_reply":"2025-07-28T17:47:19.569291Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf_X_train, df_X_validation, df_Y_train, df_Y_validation = train_test_split(df_X , df_Y, test_size = 0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:47:29.968312Z","iopub.execute_input":"2025-07-28T17:47:29.968865Z","iopub.status.idle":"2025-07-28T17:47:30.174107Z","shell.execute_reply.started":"2025-07-28T17:47:29.968838Z","shell.execute_reply":"2025-07-28T17:47:30.173180Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:47:57.202739Z","iopub.execute_input":"2025-07-28T17:47:57.203090Z","iopub.status.idle":"2025-07-28T17:47:57.207800Z","shell.execute_reply.started":"2025-07-28T17:47:57.203067Z","shell.execute_reply":"2025-07-28T17:47:57.206806Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:48:16.348287Z","iopub.execute_input":"2025-07-28T17:48:16.348689Z","iopub.status.idle":"2025-07-28T17:48:16.354556Z","shell.execute_reply.started":"2025-07-28T17:48:16.348657Z","shell.execute_reply":"2025-07-28T17:48:16.353453Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from lightgbm import LGBMRegressor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:48:42.621825Z","iopub.execute_input":"2025-07-28T17:48:42.622208Z","iopub.status.idle":"2025-07-28T17:48:49.002568Z","shell.execute_reply.started":"2025-07-28T17:48:42.622162Z","shell.execute_reply":"2025-07-28T17:48:49.001715Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"lgbm = LGBMRegressor()\nlgbm_model = lgbm.fit(df_X_train, df_Y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:48:53.888553Z","iopub.execute_input":"2025-07-28T17:48:53.889237Z","iopub.status.idle":"2025-07-28T17:48:56.386173Z","shell.execute_reply.started":"2025-07-28T17:48:53.889209Z","shell.execute_reply":"2025-07-28T17:48:56.385398Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007171 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 337256, number of used features: 130\n[LightGBM] [Info] Start training from score 15971.940286\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"lgbm_model.score(df_X_validation, df_Y_validation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:49:09.593579Z","iopub.execute_input":"2025-07-28T17:49:09.593904Z","iopub.status.idle":"2025-07-28T17:49:10.161778Z","shell.execute_reply.started":"2025-07-28T17:49:09.593880Z","shell.execute_reply":"2025-07-28T17:49:10.160655Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"0.868223750985688"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"y_pred = lgbm_model.predict(df_X_validation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:49:21.453711Z","iopub.execute_input":"2025-07-28T17:49:21.454074Z","iopub.status.idle":"2025-07-28T17:49:22.159113Z","shell.execute_reply.started":"2025-07-28T17:49:21.454047Z","shell.execute_reply":"2025-07-28T17:49:22.157480Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(df_Y_validation, y_pred))\nrmse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:49:35.269209Z","iopub.execute_input":"2025-07-28T17:49:35.269614Z","iopub.status.idle":"2025-07-28T17:49:35.279184Z","shell.execute_reply.started":"2025-07-28T17:49:35.269583Z","shell.execute_reply":"2025-07-28T17:49:35.278288Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"8305.268278450305"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"lgbm_grid = {'n_estimators': [20, 40, 100, 200, 500, 1000],\n             'learning_rate': [0.1, 0.01, 0.5]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:49:42.859231Z","iopub.execute_input":"2025-07-28T17:49:42.859586Z","iopub.status.idle":"2025-07-28T17:49:42.865029Z","shell.execute_reply.started":"2025-07-28T17:49:42.859560Z","shell.execute_reply":"2025-07-28T17:49:42.863990Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# use grind search for best params","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:49:49.640354Z","iopub.execute_input":"2025-07-28T17:49:49.640644Z","iopub.status.idle":"2025-07-28T17:49:49.645168Z","shell.execute_reply.started":"2025-07-28T17:49:49.640620Z","shell.execute_reply":"2025-07-28T17:49:49.644243Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"lgbm = LGBMRegressor()\nlgbm_cv_model = GridSearchCV(lgbm, lgbm_grid, cv=5, n_jobs = -1, verbose = 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:50:17.322637Z","iopub.execute_input":"2025-07-28T17:50:17.323000Z","iopub.status.idle":"2025-07-28T17:50:17.330144Z","shell.execute_reply.started":"2025-07-28T17:50:17.322973Z","shell.execute_reply":"2025-07-28T17:50:17.328035Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"lgbm_cv_model.fit(df_X_train, df_Y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:50:18.060402Z","iopub.execute_input":"2025-07-28T17:50:18.060867Z","iopub.status.idle":"2025-07-28T17:58:55.364697Z","shell.execute_reply.started":"2025-07-28T17:50:18.060834Z","shell.execute_reply":"2025-07-28T17:58:55.363489Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 18 candidates, totalling 90 fits\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088509 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15972.514868\n[CV] END .................learning_rate=0.1, n_estimators=20; total time=   4.7s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022484 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15972.514868\n[CV] END .................learning_rate=0.1, n_estimators=40; total time=   4.6s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041997 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269804, number of used features: 130\n[LightGBM] [Info] Start training from score 15983.097555\n[CV] END ................learning_rate=0.1, n_estimators=100; total time=   7.5s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012732 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1301\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15951.704985\n[CV] END ................learning_rate=0.1, n_estimators=100; total time=   8.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059476 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 16002.759134\n[CV] END ................learning_rate=0.1, n_estimators=200; total time=  20.7s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022852 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 16002.759134\n[CV] END ................learning_rate=0.1, n_estimators=500; total time=  30.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059344 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15972.514868\n[CV] END ...............learning_rate=0.1, n_estimators=1000; total time=  59.6s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017095 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15972.514868\n[CV] END ................learning_rate=0.01, n_estimators=20; total time=   3.6s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011767 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15949.624931\n[CV] END ................learning_rate=0.01, n_estimators=20; total time=   3.8s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033492 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269804, number of used features: 130\n[LightGBM] [Info] Start training from score 15983.097555\n[CV] END ................learning_rate=0.01, n_estimators=40; total time=   7.4s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009209 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15949.624931\n[CV] END ................learning_rate=0.01, n_estimators=40; total time=   4.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065811 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269804, number of used features: 130\n[LightGBM] [Info] Start training from score 15983.097555\n[CV] END ...............learning_rate=0.01, n_estimators=100; total time=  11.1s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016615 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1301\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15951.704985\n[CV] END ...............learning_rate=0.01, n_estimators=100; total time=  30.4s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062225 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15972.514868\n[CV] END ...............learning_rate=0.01, n_estimators=500; total time=  40.0s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056406 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1301\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15951.704985\n[CV] END ...............learning_rate=0.01, n_estimators=500; total time=  45.8s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027424 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15949.624931\n[CV] END ..............learning_rate=0.01, n_estimators=1000; total time= 1.1min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059343 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269804, number of used features: 130\n[LightGBM] [Info] Start training from score 15983.097555\n[CV] END .................learning_rate=0.5, n_estimators=40; total time=   9.0s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020017 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269804, number of used features: 130[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079045 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15949.624931\n[CV] END .................learning_rate=0.1, n_estimators=20; total time=   4.2s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023032 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1301\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15951.704985\n[CV] END .................learning_rate=0.1, n_estimators=20; total time=   4.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062447 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15949.624931\n[CV] END .................learning_rate=0.1, n_estimators=40; total time=   8.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084626 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15949.624931\n[CV] END ................learning_rate=0.1, n_estimators=100; total time=   9.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042781 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15949.624931\n[CV] END ................learning_rate=0.1, n_estimators=200; total time=  13.6s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054787 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15972.514868\n[CV] END ................learning_rate=0.1, n_estimators=500; total time=  27.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059050 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269804, number of used features: 130\n[LightGBM] [Info] Start training from score 15983.097555\n[CV] END ...............learning_rate=0.1, n_estimators=1000; total time=  49.3s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011682 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1301\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15951.704985\n[CV] END ...............learning_rate=0.1, n_estimators=1000; total time=  49.4s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067724 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15949.624931\n[CV] END ...............learning_rate=0.01, n_estimators=100; total time=  13.4s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048928 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 16002.759134\n[CV] END ...............learning_rate=0.01, n_estimators=200; total time=  14.7s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008455 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269804, number of used features: 130\n[LightGBM] [Info] Start training from score 15983.097555\n[CV] END ...............learning_rate=0.01, n_estimators=500; total time=  44.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051883 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269804, number of used features: 130\n[LightGBM] [Info] Start training from score 15983.097555\n[CV] END ..............learning_rate=0.01, n_estimators=1000; total time= 1.6min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073241 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269804, number of used features: 130\n[LightGBM] [Info] Start training from score 15983.097555\n[CV] END .................learning_rate=0.5, n_estimators=20; total time=   3.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066280 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15972.514868\n[CV] END .................learning_rate=0.5, n_estimators=20; total time=   4.4s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060907 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 16002.759134\n[CV] END .................learning_rate=0.5, n_estimators=20; total time=   3.9s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020496 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15949.624931\n[CV] END .................learning_rate=0.5, n_estimators=20; total time=   3.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067249 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15972.514868\n[CV] END .................learning_rate=0.5, n_estimators=40; total time=   4.6s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047863 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15949.624931\n[CV] END .................learning_rate=0.5, n_estimators=40; total time=  10.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058473 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092280 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269804, number of used features: 130\n[LightGBM] [Info] Start training from score 15983.097555\n[CV] END .................learning_rate=0.1, n_estimators=20; total time=   4.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057931 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269804, number of used features: 130\n[LightGBM] [Info] Start training from score 15983.097555\n[CV] END .................learning_rate=0.1, n_estimators=40; total time=   5.1s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011941 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1301\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15951.704985\n[CV] END .................learning_rate=0.1, n_estimators=40; total time=   4.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069391 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 16002.759134\n[CV] END ................learning_rate=0.1, n_estimators=100; total time=   9.0s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065975 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15972.514868\n[CV] END ................learning_rate=0.1, n_estimators=200; total time=  13.8s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013340 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1301\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15951.704985\n[CV] END ................learning_rate=0.1, n_estimators=200; total time=  12.6s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016286 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15949.624931\n[CV] END ................learning_rate=0.1, n_estimators=500; total time=  31.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061876 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 16002.759134\n[CV] END ...............learning_rate=0.1, n_estimators=1000; total time=  58.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059435 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 16002.759134\n[CV] END ................learning_rate=0.01, n_estimators=20; total time=   3.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064977 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1301\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15951.704985\n[CV] END ................learning_rate=0.01, n_estimators=20; total time=  13.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061192 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15972.514868\n[CV] END ...............learning_rate=0.01, n_estimators=100; total time=  10.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052641 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269804, number of used features: 130\n[LightGBM] [Info] Start training from score 15983.097555\n[CV] END ...............learning_rate=0.01, n_estimators=200; total time=  22.4s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014269 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1301\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15951.704985\n[CV] END ...............learning_rate=0.01, n_estimators=200; total time=  23.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057436 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15949.624931\n[CV] END ...............learning_rate=0.01, n_estimators=500; total time=  48.0s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011997 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 16002.759134\n[CV] END ..............learning_rate=0.01, n_estimators=1000; total time= 1.5min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059165 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1301\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15951.704985\n[CV] END .................learning_rate=0.5, n_estimators=20; total time=   3.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.098549 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 16002.759134\n[CV] END .................learning_rate=0.5, n_estimators=40; total time=   6.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054577 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1301\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15951.704985\n[CV] END .................learning_rate=0.5, n_estimators=40; total time=   9.3s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012235 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094203 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 16002.759134\n[CV] END .................learning_rate=0.1, n_estimators=20; total time=   5.6s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065788 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 16002.759134\n[CV] END .................learning_rate=0.1, n_estimators=40; total time=   7.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055645 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15972.514868\n[CV] END ................learning_rate=0.1, n_estimators=100; total time=   8.5s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056790 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269804, number of used features: 130\n[LightGBM] [Info] Start training from score 15983.097555\n[CV] END ................learning_rate=0.1, n_estimators=200; total time=  16.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049034 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269804, number of used features: 130\n[LightGBM] [Info] Start training from score 15983.097555\n[CV] END ................learning_rate=0.1, n_estimators=500; total time=  25.7s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017999 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1301\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15951.704985\n[CV] END ................learning_rate=0.1, n_estimators=500; total time=  22.9s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012024 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15949.624931\n[CV] END ...............learning_rate=0.1, n_estimators=1000; total time=  44.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049322 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269804, number of used features: 130\n[LightGBM] [Info] Start training from score 15983.097555\n[CV] END ................learning_rate=0.01, n_estimators=20; total time=  12.6s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011983 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15972.514868\n[CV] END ................learning_rate=0.01, n_estimators=40; total time=   4.7s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011585 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 16002.759134\n[CV] END ................learning_rate=0.01, n_estimators=40; total time=   4.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076797 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1301\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15951.704985\n[CV] END ................learning_rate=0.01, n_estimators=40; total time=   4.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068609 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 16002.759134\n[CV] END ...............learning_rate=0.01, n_estimators=100; total time=  11.0s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011327 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15972.514868\n[CV] END ...............learning_rate=0.01, n_estimators=200; total time=  14.0s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011624 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15949.624931\n[CV] END ...............learning_rate=0.01, n_estimators=200; total time=  15.3s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014714 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 16002.759134\n[CV] END ...............learning_rate=0.01, n_estimators=500; total time=  55.0s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069674 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15972.514868\n[CV] END ..............learning_rate=0.01, n_estimators=1000; total time= 1.3min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060367 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1301\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 15951.704985\n[CV] END ..............learning_rate=0.01, n_estimators=1000; total time= 1.2min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065409 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1299\n[LightGBM] [Info] Number of data points in the train set: 269805, number of used features: 130\n[LightGBM] [Info] Start training from score 16002.759134\n[CV] END ................learning_rate=0.5, n_estimators=500; total time=  32.6s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070893 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005676 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 337256, number of used features: 130\n[LightGBM] [Info] Start training from score 15971.940286\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"GridSearchCV(cv=5, estimator=LGBMRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.1, 0.01, 0.5],\n                         'n_estimators': [20, 40, 100, 200, 500, 1000]},\n             verbose=2)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LGBMRegressor(), n_jobs=-1,\n             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.01, 0.5],\n                         &#x27;n_estimators&#x27;: [20, 40, 100, 200, 500, 1000]},\n             verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LGBMRegressor(), n_jobs=-1,\n             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.01, 0.5],\n                         &#x27;n_estimators&#x27;: [20, 40, 100, 200, 500, 1000]},\n             verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"lgbm_cv_model.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:58:55.366126Z","iopub.execute_input":"2025-07-28T17:58:55.366497Z","iopub.status.idle":"2025-07-28T17:58:55.372850Z","shell.execute_reply.started":"2025-07-28T17:58:55.366465Z","shell.execute_reply":"2025-07-28T17:58:55.371972Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"{'learning_rate': 0.5, 'n_estimators': 1000}"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"lgbm_tuned = LGBMRegressor(learning_rate = 0.5, n_estimators = 1000)\n\nlgbm_tuned = lgbm_tuned.fit(df_X_train, df_Y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:58:55.373579Z","iopub.execute_input":"2025-07-28T17:58:55.373847Z","iopub.status.idle":"2025-07-28T17:59:06.258748Z","shell.execute_reply.started":"2025-07-28T17:58:55.373826Z","shell.execute_reply":"2025-07-28T17:59:06.257964Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005536 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1300\n[LightGBM] [Info] Number of data points in the train set: 337256, number of used features: 130\n[LightGBM] [Info] Start training from score 15971.940286\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"y_pred = lgbm_tuned.predict(df_X_validation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:59:06.260279Z","iopub.execute_input":"2025-07-28T17:59:06.260639Z","iopub.status.idle":"2025-07-28T17:59:10.449344Z","shell.execute_reply.started":"2025-07-28T17:59:06.260604Z","shell.execute_reply":"2025-07-28T17:59:10.448486Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"# final score","metadata":{}},{"cell_type":"code","source":"np.sqrt(mean_squared_error(df_Y_validation, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:59:10.450255Z","iopub.execute_input":"2025-07-28T17:59:10.450480Z","iopub.status.idle":"2025-07-28T17:59:10.458677Z","shell.execute_reply.started":"2025-07-28T17:59:10.450462Z","shell.execute_reply":"2025-07-28T17:59:10.457942Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"4969.7372343221805"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"lgbm_tuned.score(df_X_validation, df_Y_validation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:59:10.460052Z","iopub.execute_input":"2025-07-28T17:59:10.460442Z","iopub.status.idle":"2025-07-28T17:59:14.788037Z","shell.execute_reply.started":"2025-07-28T17:59:10.460413Z","shell.execute_reply":"2025-07-28T17:59:14.787267Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"0.9528157926665131"},"metadata":{}}],"execution_count":35}]}