{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqI5Eb2k5Rf4hVIuS3yY54",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkhok22/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_TFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7quW0irj8e9K"
      },
      "outputs": [],
      "source": [
        "# ✅ Only install what you need for SARIMAX and logging\n",
        "!pip install statsmodels wandb --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import wandb\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import gc\n",
        "import psutil\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "ySWFUyYx8iRG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install pandas numpy matplotlib seaborn scikit-learn torch torchvision wandb pyyaml darts --quiet\n",
        "import wandb\n",
        "wandb.login(key=\"eccf2c915699fc032ad678daf0fd4b5ac60bf87c\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVgddH4Y8jMP",
        "outputId": "c5ec1e16-5091-45c8-8fa8-120e5158e8d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.0/56.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.6/200.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.0/340.0 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.8/285.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.9/981.9 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabakh22\u001b[0m (\u001b[33mabakh22-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive and extract data\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "zip_path = '/content/drive/MyDrive/ML-FinalProject/data.zip'\n",
        "extract_to = '/content/walmart_data/'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "for file_name in os.listdir(extract_to):\n",
        "    if file_name.endswith('.zip'):\n",
        "        with zipfile.ZipFile(os.path.join(extract_to, file_name), 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "print(\"✅ Extracted files:\", os.listdir(extract_to))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jgh8r5r8koj",
        "outputId": "3a5e149d-868d-46c2-dccf-e998a8ca9d1c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Extracted files: ['test.csv.zip', 'stores.csv', 'sampleSubmission.csv', 'features.csv', 'test.csv', 'train.csv.zip', 'train.csv', 'sampleSubmission.csv.zip', 'features.csv.zip']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data with optimized dtypes\n",
        "train = pd.read_csv('/content/walmart_data/train.csv')\n",
        "features = pd.read_csv('/content/walmart_data/features.csv')\n",
        "stores = pd.read_csv('/content/walmart_data/stores.csv')\n",
        "test = pd.read_csv('/content/walmart_data/test.csv')\n",
        "\n",
        "# Merge and keep only necessary columns\n",
        "df = pd.merge(train, features, on=['Store', 'Date'], how='left')\n",
        "df = pd.merge(df, stores, on='Store', how='left')\n",
        "df = df.drop(columns=['IsHoliday_x']).rename(columns={'IsHoliday_y': 'IsHoliday'})\n",
        "train_merged = df[['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Temperature',\n",
        "         'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'MarkDown1', 'MarkDown2',\n",
        "         'MarkDown3', 'MarkDown4', 'MarkDown5']].sort_values(by=['Store', 'Dept', 'Date'])\n",
        "\n",
        "test_df = pd.merge(test, features, on=['Store', 'Date'], how='left')\n",
        "test_df = pd.merge(test_df, stores, on='Store', how='left')\n",
        "test_df = test_df.drop(columns=['IsHoliday_x']).rename(columns={'IsHoliday_y': 'IsHoliday'})\n",
        "test_merged = test_df[['Store', 'Dept', 'Date', 'IsHoliday', 'Temperature',\n",
        "                   'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'MarkDown1',\n",
        "                   'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']].sort_values(by=['Store', 'Dept', 'Date'])\n",
        "\n",
        "# Convert Date to datetime\n",
        "train_merged['Date'] = pd.to_datetime(train_merged['Date'])\n",
        "test_merged['Date'] = pd.to_datetime(test_merged['Date'])\n",
        "\n",
        "print(\"✅ Merged train shape:\", train_merged.shape)\n",
        "print(\"✅ Merged test shape:\", test_merged.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw18IuhH8l2p",
        "outputId": "7713cc27-f093-4d7c-cf4a-91872c68d01a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Merged train shape: (421570, 15)\n",
            "✅ Merged test shape: (115064, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect column names to diagnose the issue\n",
        "print(\"✅ Stores columns:\", stores.columns.tolist())\n",
        "print(\"✅ Train merged columns:\", train_merged.columns.tolist())\n",
        "print(\"✅ Test merged columns:\", test_merged.columns.tolist())\n",
        "\n",
        "# Handle missing values\n",
        "# MarkDown1-5: Fill NA with 0 (no markdown event)\n",
        "for col in ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']:\n",
        "    train_merged[col] = train_merged[col].fillna(0)\n",
        "    test_merged[col] = test_merged[col].fillna(0)\n",
        "\n",
        "# Other numerical features: Fill NA with median\n",
        "for col in ['CPI', 'Unemployment']:\n",
        "    train_merged[col] = train_merged[col].fillna(train_merged[col].median())\n",
        "    test_merged[col] = test_merged[col].fillna(test_merged[col].median())\n",
        "\n",
        "# Encode categorical variables\n",
        "# Check if 'Type' exists, and use the correct column name\n",
        "type_col = 'Type' if 'Type' in train_merged.columns else None\n",
        "if type_col:\n",
        "    train_merged[type_col] = train_merged[type_col].astype('category').cat.codes\n",
        "    test_merged[type_col] = test_merged[type_col].astype('category').cat.codes\n",
        "else:\n",
        "    print(\"⚠️ Column 'Type' not found in merged data. Skipping encoding for 'Type'.\")\n",
        "    # Optionally, add a placeholder column if needed\n",
        "    train_merged['Type'] = 0\n",
        "    test_merged['Type'] = 0\n",
        "\n",
        "# Encode IsHoliday\n",
        "train_merged['IsHoliday'] = train_merged['IsHoliday'].astype(int)\n",
        "test_merged['IsHoliday'] = test_merged['IsHoliday'].astype(int)\n",
        "\n",
        "print(\"✅ Missing values handled and categoricals encoded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-8CmTcF9D7c",
        "outputId": "ba7f7a3f-dafe-448c-b418-7ea2c10bba6d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Stores columns: ['Store', 'Type', 'Size']\n",
            "✅ Train merged columns: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
            "✅ Test merged columns: ['Store', 'Dept', 'Date', 'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
            "⚠️ Column 'Type' not found in merged data. Skipping encoding for 'Type'.\n",
            "✅ Missing values handled and categoricals encoded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from darts import TimeSeries\n",
        "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
        "\n",
        "# Create TimeSeries objects for each store-department\n",
        "def create_timeseries(df, target_col='Weekly_Sales', is_train=True, min_length=16):\n",
        "    series_dict = {}\n",
        "    for (store, dept), group in df.groupby(['Store', 'Dept']):\n",
        "        group = group.sort_values('Date')\n",
        "        # Ensure group has enough data\n",
        "        if len(group) < min_length:\n",
        "            print(f\"⚠️ Skipping Store {store}, Dept {dept}: Only {len(group)} time steps (min required: {min_length})\")\n",
        "            continue\n",
        "\n",
        "        # Target series (only for training data)\n",
        "        target_series = None\n",
        "        if is_train:\n",
        "            if target_col in group.columns:\n",
        "                target_series = TimeSeries.from_dataframe(\n",
        "                    group, time_col='Date', value_cols=[target_col], freq='W-FRI'\n",
        "                )\n",
        "            else:\n",
        "                print(f\"⚠️ Target column '{target_col}' not found for Store {store}, Dept {dept}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "        # Past covariates (e.g., Temperature, Fuel_Price, MarkDowns, CPI, Unemployment, Type, IsHoliday)\n",
        "        past_cov_columns = ['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n",
        "                           'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'IsHoliday']\n",
        "        # Filter columns that exist in the DataFrame\n",
        "        past_cov_columns = [col for col in past_cov_columns if col in group.columns]\n",
        "        past_cov = TimeSeries.from_dataframe(\n",
        "            group, time_col='Date', value_cols=past_cov_columns, freq='W-FRI'\n",
        "        )\n",
        "\n",
        "        # Future covariates (known in advance: IsHoliday, Date-based features)\n",
        "        future_cov = TimeSeries.from_dataframe(\n",
        "            group, time_col='Date', value_cols=['IsHoliday'], freq='W-FRI'\n",
        "        )\n",
        "\n",
        "        # Add time-based features (month one-hot, week as numerical)\n",
        "        time_index = TimeSeries.from_dataframe(group, time_col='Date', value_cols=['IsHoliday'], freq='W-FRI')\n",
        "        month_features = datetime_attribute_timeseries(\n",
        "            time_index, attribute='month', one_hot=True\n",
        "        )\n",
        "        week_features = datetime_attribute_timeseries(\n",
        "            time_index, attribute='week', one_hot=False  # Use numerical week number\n",
        "        )\n",
        "        time_features = month_features.concatenate(week_features, axis=1)\n",
        "        future_cov = future_cov.concatenate(time_features, axis=1)\n",
        "\n",
        "        # Store series\n",
        "        series_dict[(store, dept)] = {\n",
        "            'target': target_series,\n",
        "            'past_cov': past_cov,\n",
        "            'future_cov': future_cov\n",
        "        }\n",
        "    return series_dict\n",
        "\n",
        "# Create training and test series\n",
        "train_series = create_timeseries(train_merged, target_col='Weekly_Sales', is_train=True, min_length=16)\n",
        "test_series = create_timeseries(test_merged, is_train=False, min_length=1)  # Test set needs at least 1 time step\n",
        "\n",
        "print(f\"✅ Created {len(train_series)} training time series\")\n",
        "print(f\"✅ Created {len(test_series)} test time series\")"
      ],
      "metadata": {
        "id": "e0TOV6n99HTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from darts.models import TFTModel\n",
        "from darts.metrics import mae\n",
        "import torch\n",
        "import threadpoolctl\n",
        "import numpy as np\n",
        "\n",
        "# Suppress threadpoolctl warnings\n",
        "threadpoolctl.threadpool_limits(1, \"blas\")  # Limit to 1 thread to avoid dlopen issues\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"walmart_sales_forecasting\", name=\"tft_simple_model\")\n",
        "\n",
        "# Define WMAE metric\n",
        "def wmae(y_true, y_pred, is_holiday):\n",
        "    weights = 5 * is_holiday + (1 - is_holiday)\n",
        "    return np.mean(weights * np.abs(y_true - y_pred))\n",
        "\n",
        "# TFT model configuration\n",
        "model = TFTModel(\n",
        "    input_chunk_length=12,  # Look back 12 weeks\n",
        "    output_chunk_length=4,  # Predict 4 weeks ahead\n",
        "    hidden_size=16,         # Small for efficiency\n",
        "    lstm_layers=1,\n",
        "    num_attention_heads=4,\n",
        "    dropout=0.1,\n",
        "    batch_size=32,\n",
        "    n_epochs=10,            # Adjust based on performance\n",
        "    add_relative_index=True,\n",
        "    random_state=42,\n",
        "    pl_trainer_kwargs={\"accelerator\": \"gpu\" if torch.cuda.is_available() else \"cpu\"}\n",
        ")\n",
        "\n",
        "# Train model on all series\n",
        "train_targets = [series['target'] for series in train_series.values() if series['target'] is not None]\n",
        "train_past_covs = [series['past_cov'] for series in train_series.values() if series['target'] is not None]\n",
        "train_future_covs = [series['future_cov'] for series in train_series.values() if series['target'] is not None]\n",
        "\n",
        "# Log number of valid series\n",
        "wandb.log({\"num_training_series\": len(train_targets)})\n",
        "\n",
        "model.fit(\n",
        "    series=train_targets,\n",
        "    past_covariates=train_past_covs,\n",
        "    future_covariates=train_future_covs,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Log training completion\n",
        "wandb.log({\"status\": \"Training completed\"})"
      ],
      "metadata": {
        "id": "w0I-12dU9Kth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "predictions = {}\n",
        "for (store, dept), series in test_series.items():\n",
        "    pred = model.predict(\n",
        "        n=len(series['target']),\n",
        "        series=series['target'],\n",
        "        past_covariates=series['past_cov'],\n",
        "        future_covariates=series['future_cov']\n",
        "    )\n",
        "    predictions[(store, dept)] = pred\n",
        "\n",
        "# Format submission\n",
        "submission = []\n",
        "for (store, dept), pred in predictions.items():\n",
        "    dates = pred.time_index\n",
        "    values = pred.values().flatten()\n",
        "    for date, value in zip(dates, values):\n",
        "        id_str = f\"{store}_{dept}_{date.strftime('%Y-%m-%d')}\"\n",
        "        submission.append({'Id': id_str, 'Weekly_Sales': max(0, value)})  # Ensure non-negative sales\n",
        "\n",
        "submission_df = pd.DataFrame(submission)\n",
        "submission_df.to_csv('/content/walmart_submission.csv', index=False)\n",
        "\n",
        "# Log submission file to wandb\n",
        "wandb.save('/content/walmart_submission.csv')\n",
        "print(\"✅ Submission file created: walmart_submission.csv\")"
      ],
      "metadata": {
        "id": "Fds-JeX_9LND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation (optional, if you want to compute WMAE on a validation split)\n",
        "# Split train data into train/validation (last 4 weeks for validation)\n",
        "val_targets = []\n",
        "val_preds = []\n",
        "val_holidays = []\n",
        "for (store, dept), series in train_series.items():\n",
        "    train_split = series['target'][:-4]\n",
        "    val_split = series['target'][-4:]\n",
        "    past_cov_split = series['past_cov'][:-4]\n",
        "    future_cov_split = series['future_cov']\n",
        "    pred = model.predict(\n",
        "        n=4,\n",
        "        series=train_split,\n",
        "        past_covariates=past_cov_split,\n",
        "        future_covariates=future_cov_split\n",
        "    )\n",
        "    val_targets.append(val_split.values().flatten())\n",
        "    val_preds.append(pred.values().flatten())\n",
        "    val_holidays.append(series['future_cov']['IsHoliday'][-4:].values().flatten())\n",
        "\n",
        "# Compute WMAE\n",
        "val_targets = np.concatenate(val_targets)\n",
        "val_preds = np.concatenate(val_preds)\n",
        "val_holidays = np.concatenate(val_holidays)\n",
        "wmae_score = wmae(val_targets, val_preds, val_holidays)\n",
        "wandb.log({\"validation_wmae\": wmae_score})\n",
        "print(f\"✅ Validation WMAE: {wmae_score}\")"
      ],
      "metadata": {
        "id": "Uie598HQ9OSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f59zc6o-9VCE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}