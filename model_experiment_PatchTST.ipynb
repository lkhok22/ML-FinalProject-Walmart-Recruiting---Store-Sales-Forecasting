{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMiKNdizgOqHkAAXcGavaxa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkhok22/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_PatchTST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "5n9R-y6b5MuE",
        "outputId": "01c946b4-2260-4179-d076-59f35f684eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlkhok22\u001b[0m (\u001b[33mlkhok22-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250727_181425-2tpsdcsc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/2tpsdcsc' target=\"_blank\">vibrant-donkey-23</a></strong> to <a href='https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/2tpsdcsc' target=\"_blank\">https://wandb.ai/lkhok22-free-university-of-tbilisi-/walmart-sales-forecasting/runs/2tpsdcsc</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Unzip dataset\n",
        "zip_path = \"/content/drive/MyDrive/ML-FinalProject/data.zip\"\n",
        "extract_path = \"/content/data\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "for zip_file in glob.glob(f\"{extract_path}/*.csv.zip\"):\n",
        "    with zipfile.ZipFile(zip_file, 'r') as z:\n",
        "        z.extractall(extract_path)\n",
        "\n",
        "# Set random seed\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"walmart-sales-forecasting\", config={\n",
        "    \"learning_rate\": 0.0005,\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 30,\n",
        "    \"patch_length\": 4,\n",
        "    \"n_patches\": 13,\n",
        "    \"d_model\": 256,\n",
        "    \"n_heads\": 8,\n",
        "    \"n_layers\": 4\n",
        "})\n",
        "config = wandb.config\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train = pd.read_csv(f\"{extract_path}/train.csv\")\n",
        "test = pd.read_csv(f\"{extract_path}/test.csv\")\n",
        "stores = pd.read_csv(f\"{extract_path}/stores.csv\")\n",
        "features = pd.read_csv(f\"{extract_path}/features.csv\")\n",
        "\n",
        "# Rename IsHoliday in features to avoid merge conflicts\n",
        "features = features.rename(columns={\"IsHoliday\": \"IsHoliday_features\"})\n",
        "\n",
        "# Merge datasets\n",
        "train = train.merge(stores, on=\"Store\", how=\"left\").merge(features, on=[\"Store\", \"Date\"], how=\"left\")\n",
        "test = test.merge(stores, on=\"Store\", how=\"left\").merge(features, on=[\"Store\", \"Date\"], how=\"left\")\n",
        "\n",
        "# Convert Date to datetime\n",
        "train[\"Date\"] = pd.to_datetime(train[\"Date\"])\n",
        "test[\"Date\"] = pd.to_datetime(test[\"Date\"])\n",
        "\n",
        "# Verify merges\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "print(\"Test columns:\", test.columns.tolist())\n",
        "print(f\"Train rows: {len(train)}, Test rows: {len(test)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqFm9ZAx5N-A",
        "outputId": "c4cdb278-e38d-4bee-f5c7-ed324d7558ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train columns: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Type', 'Size', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_features']\n",
            "Test columns: ['Store', 'Dept', 'Date', 'IsHoliday', 'Type', 'Size', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_features']\n",
            "Train rows: 421570, Test rows: 115064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values\n",
        "for col in [\"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\"]:\n",
        "    train[col] = train[col].fillna(0)\n",
        "    test[col] = test[col].fillna(0)\n",
        "train[[\"CPI\", \"Unemployment\"]] = train[[\"CPI\", \"Unemployment\"]].ffill()\n",
        "test[[\"CPI\", \"Unemployment\"]] = test[[\"CPI\", \"Unemployment\"]].ffill()\n",
        "\n",
        "# Encode categorical variables\n",
        "le_store = LabelEncoder()\n",
        "le_dept = LabelEncoder()\n",
        "le_type = LabelEncoder()\n",
        "train[\"Store\"] = le_store.fit_transform(train[\"Store\"])\n",
        "test[\"Store\"] = le_store.transform(test[\"Store\"])\n",
        "train[\"Dept\"] = le_dept.fit_transform(train[\"Dept\"])\n",
        "test[\"Dept_original\"] = test[\"Dept\"]  # Preserve original Dept for Ids\n",
        "test[\"Dept\"] = test[\"Dept\"].apply(lambda x: x if x in le_dept.classes_ else le_dept.classes_[-1])\n",
        "test[\"Dept\"] = le_dept.transform(test[\"Dept\"])\n",
        "train[\"Type\"] = le_type.fit_transform(train[\"Type\"])\n",
        "test[\"Type\"] = le_type.transform(test[\"Type\"])\n",
        "\n",
        "# Add holiday indicators\n",
        "holiday_dates = {\n",
        "    \"Super Bowl\": [\"2010-02-12\", \"2011-02-11\", \"2012-02-10\", \"2013-02-08\"],\n",
        "    \"Labor Day\": [\"2010-09-10\", \"2011-09-09\", \"2012-09-07\", \"2013-09-06\"],\n",
        "    \"Thanksgiving\": [\"2010-11-26\", \"2011-11-25\", \"2012-11-23\", \"2013-11-29\"],\n",
        "    \"Christmas\": [\"2010-12-31\", \"2011-12-30\", \"2012-12-28\", \"2013-12-27\"]\n",
        "}\n",
        "for holiday, dates in holiday_dates.items():\n",
        "    dates = pd.to_datetime(dates)\n",
        "    for df in [train, test]:\n",
        "        df[holiday] = df[\"Date\"].isin(dates).astype(int)\n",
        "\n",
        "# Extract date features and holiday weight\n",
        "for df in [train, test]:\n",
        "    df[\"Year\"] = df[\"Date\"].dt.year\n",
        "    df[\"Month\"] = df[\"Date\"].dt.month\n",
        "    df[\"Week\"] = df[\"Date\"].dt.isocalendar().week\n",
        "    df[\"IsHolidayWeight\"] = df[\"IsHoliday\"].apply(lambda x: 5 if x else 1)\n",
        "\n",
        "# Add lag-52 feature\n",
        "train[\"Lag_52\"] = train.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(52)\n",
        "# Create a lagged date column in train\n",
        "train[\"Date_lag\"] = train[\"Date\"] + pd.Timedelta(weeks=52)\n",
        "test = test.merge(\n",
        "    train[[\"Store\", \"Dept\", \"Date_lag\", \"Weekly_Sales\"]].copy().rename(columns={\"Weekly_Sales\": \"Weekly_Sales_lag\"}),\n",
        "    left_on=[\"Store\", \"Dept\", \"Date\"],\n",
        "    right_on=[\"Store\", \"Dept\", \"Date_lag\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "train[\"Lag_52\"] = train[\"Lag_52\"].fillna(train[\"Weekly_Sales\"].mean())\n",
        "test[\"Lag_52\"] = test[\"Weekly_Sales_lag\"].fillna(train[\"Weekly_Sales\"].mean())\n",
        "# Drop temporary column\n",
        "train = train.drop(columns=[\"Date_lag\"])\n",
        "test = test.drop(columns=[\"Weekly_Sales_lag\", \"Date_lag\"], errors=\"ignore\")\n",
        "\n",
        "# Define features\n",
        "num_features = [\"Size\", \"Temperature\", \"Fuel_Price\", \"MarkDown1\", \"MarkDown2\", \"MarkDown3\",\n",
        "                \"MarkDown4\", \"MarkDown5\", \"CPI\", \"Unemployment\", \"Year\", \"Month\", \"Week\",\n",
        "                \"Super Bowl\", \"Labor Day\", \"Thanksgiving\", \"Christmas\", \"Lag_52\"]\n",
        "cat_features = [\"Store\", \"Dept\", \"Type\"]\n",
        "target = \"Weekly_Sales\"\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "train[num_features] = scaler.fit_transform(train[num_features])\n",
        "test[num_features] = scaler.transform(test[num_features])\n",
        "\n",
        "# Scale target\n",
        "target_scaler = StandardScaler()\n",
        "train[\"Weekly_Sales\"] = target_scaler.fit_transform(train[[\"Weekly_Sales\"]]).flatten()\n",
        "\n",
        "# Verify preprocessing\n",
        "print(f\"Train columns after preprocessing: {train.columns.tolist()}\")\n",
        "print(f\"Test columns after preprocessing: {test.columns.tolist()}\")\n",
        "print(f\"Train NaNs:\\n{train[num_features].isnull().sum()}\")\n",
        "print(f\"Test NaNs:\\n{test[num_features].isnull().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTAGH1A55lUm",
        "outputId": "827df033-0999-4d25-de48-f6a4516ce028"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train columns after preprocessing: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Type', 'Size', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_features', 'Super Bowl', 'Labor Day', 'Thanksgiving', 'Christmas', 'Year', 'Month', 'Week', 'IsHolidayWeight', 'Lag_52']\n",
            "Test columns after preprocessing: ['Store', 'Dept', 'Date', 'IsHoliday', 'Type', 'Size', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_features', 'Dept_original', 'Super Bowl', 'Labor Day', 'Thanksgiving', 'Christmas', 'Year', 'Month', 'Week', 'IsHolidayWeight', 'Lag_52']\n",
            "Train NaNs:\n",
            "Size            0\n",
            "Temperature     0\n",
            "Fuel_Price      0\n",
            "MarkDown1       0\n",
            "MarkDown2       0\n",
            "MarkDown3       0\n",
            "MarkDown4       0\n",
            "MarkDown5       0\n",
            "CPI             0\n",
            "Unemployment    0\n",
            "Year            0\n",
            "Month           0\n",
            "Week            0\n",
            "Super Bowl      0\n",
            "Labor Day       0\n",
            "Thanksgiving    0\n",
            "Christmas       0\n",
            "Lag_52          0\n",
            "dtype: int64\n",
            "Test NaNs:\n",
            "Size            0\n",
            "Temperature     0\n",
            "Fuel_Price      0\n",
            "MarkDown1       0\n",
            "MarkDown2       0\n",
            "MarkDown3       0\n",
            "MarkDown4       0\n",
            "MarkDown5       0\n",
            "CPI             0\n",
            "Unemployment    0\n",
            "Year            0\n",
            "Month           0\n",
            "Week            0\n",
            "Super Bowl      0\n",
            "Labor Day       0\n",
            "Thanksgiving    0\n",
            "Christmas       0\n",
            "Lag_52          0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(train_df, test_df, seq_length=52):\n",
        "    train_sequences = []\n",
        "    train_targets = []\n",
        "    train_weights = []\n",
        "    test_sequences = []\n",
        "    test_ids = []\n",
        "    n_features = len(num_features + cat_features)\n",
        "\n",
        "    # Training sequences\n",
        "    for (store, dept), group in train_df.groupby([\"Store\", \"Dept\"]):\n",
        "        group = group.sort_values(\"Date\")\n",
        "        features = group[num_features + cat_features].values.astype(np.float64)\n",
        "        sales = group[target].values.astype(np.float64)\n",
        "        weights = group[\"IsHolidayWeight\"].values.astype(np.float64)\n",
        "        for i in range(len(group) - seq_length):\n",
        "            train_sequences.append(features[i:i+seq_length])\n",
        "            train_targets.append(sales[i+seq_length])\n",
        "            train_weights.append(weights[i+seq_length])\n",
        "\n",
        "    # Mean features for padding\n",
        "    mean_features = train_df[num_features + cat_features].mean().values.astype(np.float64)\n",
        "\n",
        "    # Test sequences\n",
        "    for idx, row in test_df.iterrows():\n",
        "        store = row[\"Store\"]\n",
        "        dept = row[\"Dept\"]\n",
        "        date = row[\"Date\"]\n",
        "        test_features = row[num_features + cat_features].values.astype(np.float64)\n",
        "        group = train_df[(train_df[\"Store\"] == store) & (train_df[\"Dept\"] == dept)].sort_values(\"Date\")\n",
        "        historical = group[group[\"Date\"] < date][num_features + cat_features].values.astype(np.float64)\n",
        "\n",
        "        seq = np.zeros((seq_length, n_features), dtype=np.float64)\n",
        "        seq[:] = mean_features\n",
        "        if len(historical) > 0:\n",
        "            start_idx = max(0, len(historical) - (seq_length - 1))\n",
        "            seq[-(len(historical) - start_idx + 1):-1] = historical[start_idx:]\n",
        "        seq[-1] = test_features\n",
        "        test_sequences.append(seq)\n",
        "        test_ids.append(f\"{int(row['Store'])}_{int(row['Dept_original'])}_{date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "    train_sequences = np.array(train_sequences, dtype=np.float64)\n",
        "    train_targets = np.array(train_targets, dtype=np.float64)\n",
        "    train_weights = np.array(train_weights, dtype=np.float64)\n",
        "    test_sequences = np.array(test_sequences, dtype=np.float64)\n",
        "\n",
        "    print(f\"train_sequences shape: {train_sequences.shape}\")\n",
        "    print(f\"test_sequences shape: {test_sequences.shape}\")\n",
        "    print(f\"Number of unique test_ids: {len(set(test_ids))}\")\n",
        "\n",
        "    return train_sequences, train_targets, train_weights, test_sequences, test_ids\n",
        "\n",
        "seq_length = config.patch_length * config.n_patches  # 4 * 13 = 52\n",
        "train_sequences, train_targets, train_weights, test_sequences, test_ids = create_sequences(train, test)\n",
        "\n",
        "# Convert to tensors\n",
        "train_sequences = torch.FloatTensor(train_sequences)\n",
        "train_targets = torch.FloatTensor(train_targets)\n",
        "train_weights = torch.FloatTensor(train_weights)\n",
        "test_sequences = torch.FloatTensor(test_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anHTIyxE6Khj",
        "outputId": "168c2e27-cfd8-4fd1-9f4e-d3fe69582ad1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_sequences shape: (261083, 52, 21)\n",
            "test_sequences shape: (115064, 52, 21)\n",
            "Number of unique test_ids: 115064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchTST(nn.Module):\n",
        "    def __init__(self, seq_length, patch_length, n_patches, d_model, n_heads, n_layers, n_features):\n",
        "        super(PatchTST, self).__init__()\n",
        "        self.patch_length = patch_length\n",
        "        self.n_patches = n_patches\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert seq_length >= n_patches * patch_length, f\"Sequence length ({seq_length}) must be >= n_patches ({n_patches}) * patch_length ({patch_length})\"\n",
        "\n",
        "        self.patch_embedding = nn.Linear(patch_length * n_features, d_model)\n",
        "        self.position_embedding = nn.Parameter(torch.randn(1, n_patches, d_model))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        self.fc = nn.Linear(d_model * n_patches, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x[:, :self.n_patches * self.patch_length, :].view(batch_size, self.n_patches, self.patch_length * x.size(-1))\n",
        "        x = self.patch_embedding(x) + self.position_embedding\n",
        "        x = self.transformer(x)\n",
        "        x = x.view(batch_size, -1)\n",
        "        x = self.fc(x)\n",
        "        return x.squeeze(-1)\n",
        "\n",
        "class SalesDataset(Dataset):\n",
        "    def __init__(self, sequences, targets, weights):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "        self.weights = weights\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.targets[idx], self.weights[idx]"
      ],
      "metadata": {
        "id": "jaqgGc316Nyo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train/validation\n",
        "val_size = int(0.2 * len(train_sequences))\n",
        "train_seq, val_seq = train_sequences[:-val_size], train_sequences[-val_size:]\n",
        "train_tgt, val_tgt = train_targets[:-val_size], train_targets[-val_size:]\n",
        "train_wgt, val_wgt = train_weights[:-val_size], train_weights[-val_size:]\n",
        "\n",
        "train_dataset = SalesDataset(train_seq, train_tgt, train_wgt)\n",
        "val_dataset = SalesDataset(val_seq, val_tgt, val_wgt)\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config.batch_size)\n",
        "\n",
        "# WMAE Loss\n",
        "def wmae_loss(pred, target, weight):\n",
        "    return torch.mean(weight * torch.abs(pred - target))\n",
        "\n",
        "# Initialize model\n",
        "n_features = len(num_features + cat_features)\n",
        "model = PatchTST(seq_length=seq_length, patch_length=config.patch_length, n_patches=config.n_patches,\n",
        "                 d_model=config.d_model, n_heads=config.n_heads, n_layers=config.n_layers,\n",
        "                 n_features=n_features)\n",
        "model = model.cuda() if torch.cuda.is_available() else model\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop\n",
        "best_val_wmae = float(\"inf\")\n",
        "for epoch in range(config.epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for seq, tgt, wgt in train_loader:\n",
        "        seq, tgt, wgt = seq.to(device), tgt.to(device), wgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(seq)\n",
        "        loss = wmae_loss(pred, tgt, wgt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * seq.size(0)\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for seq, tgt, wgt in val_loader:\n",
        "            seq, tgt, wgt = seq.to(device), tgt.to(device), wgt.to(device)\n",
        "            pred = model(seq)\n",
        "            loss = wmae_loss(pred, tgt, wgt)\n",
        "            val_loss += loss.item() * seq.size(0)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "    wandb.log({\"epoch\": epoch+1, \"train_wmae\": train_loss, \"val_wmae\": val_loss, \"lr\": optimizer.param_groups[0]['lr']})\n",
        "\n",
        "    if val_loss < best_val_wmae:\n",
        "        best_val_wmae = val_loss\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{config.epochs}, Train WMAE: {train_loss:.4f}, Val WMAE: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lqDnSkbGcwt",
        "outputId": "cad42e6a-b728-4cf0-edb0-48560ece83fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Train WMAE: 0.3314, Val WMAE: 0.2009\n",
            "Epoch 2/30, Train WMAE: 0.1996, Val WMAE: 0.1847\n",
            "Epoch 3/30, Train WMAE: 0.1874, Val WMAE: 0.1450\n",
            "Epoch 4/30, Train WMAE: 0.1818, Val WMAE: 0.1383\n",
            "Epoch 5/30, Train WMAE: 0.1807, Val WMAE: 0.1627\n",
            "Epoch 6/30, Train WMAE: 0.1785, Val WMAE: 0.1535\n",
            "Epoch 7/30, Train WMAE: 0.1722, Val WMAE: 0.1621\n",
            "Epoch 8/30, Train WMAE: 0.1754, Val WMAE: 0.1886\n",
            "Epoch 9/30, Train WMAE: 0.1762, Val WMAE: 0.1869\n",
            "Epoch 10/30, Train WMAE: 0.1814, Val WMAE: 0.2197\n",
            "Epoch 11/30, Train WMAE: 0.1655, Val WMAE: 0.1659\n",
            "Epoch 12/30, Train WMAE: 0.1609, Val WMAE: 0.1404\n",
            "Epoch 13/30, Train WMAE: 0.1586, Val WMAE: 0.1310\n",
            "Epoch 14/30, Train WMAE: 0.1564, Val WMAE: 0.1391\n",
            "Epoch 15/30, Train WMAE: 0.1546, Val WMAE: 0.1614\n",
            "Epoch 16/30, Train WMAE: 0.1550, Val WMAE: 0.1377\n",
            "Epoch 17/30, Train WMAE: 0.1565, Val WMAE: 0.1476\n",
            "Epoch 18/30, Train WMAE: 0.1673, Val WMAE: 0.1503\n",
            "Epoch 19/30, Train WMAE: 0.1539, Val WMAE: 0.1589\n",
            "Epoch 20/30, Train WMAE: 0.1478, Val WMAE: 0.1615\n",
            "Epoch 21/30, Train WMAE: 0.1468, Val WMAE: 0.1392\n",
            "Epoch 22/30, Train WMAE: 0.1456, Val WMAE: 0.1396\n",
            "Epoch 23/30, Train WMAE: 0.1446, Val WMAE: 0.1632\n",
            "Epoch 24/30, Train WMAE: 0.1439, Val WMAE: 0.1442\n",
            "Epoch 25/30, Train WMAE: 0.1436, Val WMAE: 0.1397\n",
            "Epoch 26/30, Train WMAE: 0.1401, Val WMAE: 0.1513\n",
            "Epoch 27/30, Train WMAE: 0.1399, Val WMAE: 0.1541\n",
            "Epoch 28/30, Train WMAE: 0.1393, Val WMAE: 0.1656\n",
            "Epoch 29/30, Train WMAE: 0.1387, Val WMAE: 0.1516\n",
            "Epoch 30/30, Train WMAE: 0.1383, Val WMAE: 0.1665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions for test set\n",
        "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_sequences), config.batch_size):\n",
        "        batch_seq = test_sequences[i:i+config.batch_size].to(device)\n",
        "        pred = model(batch_seq)\n",
        "        pred = target_scaler.inverse_transform(pred.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "        test_predictions.extend(pred)\n",
        "\n",
        "# Load test.csv to get expected Ids\n",
        "test_csv = pd.read_csv(f\"{extract_path}/test.csv\")\n",
        "test_csv[\"Date\"] = pd.to_datetime(test_csv[\"Date\"])\n",
        "test_csv[\"Id\"] = test_csv.apply(lambda row: f\"{int(row['Store'])}_{int(row['Dept'])}_{row['Date'].strftime('%Y-%m-%d')}\", axis=1)\n",
        "\n",
        "# Create submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    \"Id\": test_csv[\"Id\"],\n",
        "    \"Weekly_Sales\": [test_predictions[i] if i < len(test_predictions) else train[\"Weekly_Sales\"].mean() for i in range(len(test_csv))]\n",
        "})\n",
        "\n",
        "# Clip negative predictions\n",
        "submission[\"Weekly_Sales\"] = submission[\"Weekly_Sales\"].clip(lower=0)\n",
        "\n",
        "# Ensure correct number of entries and no duplicates\n",
        "assert len(submission) == len(test_csv), f\"Submission has {len(submission)} rows, expected {len(test_csv)}\"\n",
        "assert len(set(submission[\"Id\"])) == len(submission), f\"Found {len(submission) - len(set(submission['Id']))} duplicate Ids\"\n",
        "\n",
        "# Sort by Id\n",
        "submission = submission.sort_values(\"Id\")\n",
        "\n",
        "# Verify date range\n",
        "submission[\"Date\"] = submission[\"Id\"].str.split(\"_\").str[2]\n",
        "submission[\"Date\"] = pd.to_datetime(submission[\"Date\"])\n",
        "assert submission[\"Date\"].min() == pd.to_datetime(\"2012-11-02\"), f\"Submission starts at {submission['Date'].min()}, expected 2012-11-02\"\n",
        "assert submission[\"Date\"].max() <= pd.to_datetime(\"2013-07-26\"), f\"Submission ends at {submission['Date'].max()}, expected <= 2013-07-26\"\n",
        "\n",
        "# Save submission\n",
        "submission[[\"Id\", \"Weekly_Sales\"]].to_csv(\"submission.csv\", index=False)\n",
        "wandb.save(\"submission.csv\")\n",
        "\n",
        "# Verify submission\n",
        "print(f\"Submission rows: {len(submission)}\")\n",
        "print(f\"Unique Ids: {len(set(submission['Id']))}\")\n",
        "print(submission.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m26rc1eWHySX",
        "outputId": "b7402581-4cd1-4ce0-dd6f-d99435438df7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission rows: 115064\n",
            "Unique Ids: 115064\n",
            "                     Id  Weekly_Sales       Date\n",
            "24245  10_10_2012-11-02  46189.890625 2012-11-02\n",
            "24246  10_10_2012-11-09  49435.960938 2012-11-09\n",
            "24247  10_10_2012-11-16  48769.394531 2012-11-16\n",
            "24248  10_10_2012-11-23  58300.500000 2012-11-23\n",
            "24249  10_10_2012-11-30  48250.031250 2012-11-30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Score: 5475.74688"
      ],
      "metadata": {
        "id": "l1EQtdzXSvjY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EHp0qzSkOG3A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}