{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3816,"databundleVersionId":32105,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":65.097773,"end_time":"2025-07-06T15:08:18.075283","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-07-06T15:07:12.977510","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"2b8a9e285c354ead992787182d5ef8fb":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a773be6cc9b64dafa29f32083ba840d0":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_2b8a9e285c354ead992787182d5ef8fb","msg_id":"","outputs":[{"data":{"text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠼</span> Waiting for authorization\n</pre>\n","text/plain":"\u001b[32m⠼\u001b[0m Waiting for authorization\n"},"metadata":{},"output_type":"display_data"}],"tabbable":null,"tooltip":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"470e8759-1d1a-4249-8620-070a148dba61","cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import FunctionTransformer, OrdinalEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nimport joblib\nimport zipfile\nimport os\nimport mlflow\n\n# Set sklearn to output pandas DataFrames\nfrom sklearn import set_config\nset_config(transform_output=\"pandas\")\n\n# Helper function to read zipped CSVs\ndef read_zipped_csv(path):\n    with zipfile.ZipFile(path) as z:\n        file_name = z.namelist()[0]\n        return pd.read_csv(z.open(file_name))\n\n# Custom transformer for merging dataframes\ndef merge_dataframes(X, features_df=None, stores_df=None):\n    df = X.copy()\n    # Ensure Date columns are in datetime format\n    df['Date'] = pd.to_datetime(df['Date'])\n    features_df = features_df.copy()\n    features_df['Date'] = pd.to_datetime(features_df['Date'])\n    df = df.merge(features_df, on=[\"Store\", \"Date\"], how=\"left\")\n    df = df.merge(stores_df, on=\"Store\", how=\"left\")\n    df = df.drop(columns=['IsHoliday_y'], errors='ignore')\n    df = df.rename(columns={'IsHoliday_x': 'IsHoliday'})\n    return df\n\n# Custom transformer for date features\ndef add_date_features(X):\n    df = X.copy()\n    df['Date'] = pd.to_datetime(df['Date'])\n    df['Year'] = df['Date'].dt.year\n    df['Month'] = df['Date'].dt.month\n    df['Week'] = df['Date'].dt.isocalendar().week.astype(int)\n    df['Day'] = df['Date'].dt.day\n    df['IsHoliday'] = df['IsHoliday'].astype(int)\n    return df\n\n# Custom transformer for CPI and Unemployment imputation\ndef impute_cpi_unemployment(X):\n    df = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X.copy()\n    # Ensure required columns exist\n    if 'CPI' in df.columns:\n        df['CPI'] = df.groupby('Store')['CPI'].ffill().bfill()\n    if 'Unemployment' in df.columns:\n        df['Unemployment'] = df.groupby('Store')['Unemployment'].ffill().bfill()\n    return df\n\n# Custom transformer for lag and rolling features\ndef add_lag_rolling_features(X, y=None, train_full=None):\n    df = X.copy()\n    # Ensure Weekly_Sales is available for training\n    if y is not None and 'Weekly_Sales' not in df.columns:\n        df['Weekly_Sales'] = y.values if isinstance(y, pd.Series) else y\n    if train_full is not None:\n        train_full = train_full.copy()\n        if 'Weekly_Sales' not in train_full.columns:\n            train_full['Weekly_Sales'] = np.nan\n        combined = pd.concat([train_full, df], sort=False)\n    else:\n        combined = df\n    combined = combined.sort_values(['Store', 'Dept', 'Date'])\n    # Compute lag and rolling features\n    if 'Weekly_Sales' in combined.columns:\n        combined['Sales_Lag_1'] = combined.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1)\n        combined['Sales_Lag_2'] = combined.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(2)\n        combined['Sales_Rolling_Mean_3'] = combined.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).rolling(3).mean()\n        combined['Sales_Rolling_Mean_5'] = combined.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).rolling(5).mean()\n    if train_full is not None:\n        result = combined[combined['Weekly_Sales'].isna()]\n        result = result.drop(columns=['Weekly_Sales'], errors='ignore')\n        return result\n    return combined.drop(columns=['Weekly_Sales'], errors='ignore')\n\n# Function to calculate WMAE (Weighted Mean Absolute Error)\ndef calculate_wmae(y_true, y_pred, is_holiday):\n    weights = np.where(is_holiday, 5, 1)\n    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T18:00:54.032145Z","iopub.execute_input":"2025-07-06T18:00:54.032507Z","iopub.status.idle":"2025-07-06T18:00:54.050021Z","shell.execute_reply.started":"2025-07-06T18:00:54.032482Z","shell.execute_reply":"2025-07-06T18:00:54.048566Z"}},"outputs":[],"execution_count":69},{"id":"c531fd66-8157-4467-98a4-a5d343b4a222","cell_type":"code","source":"\n# Load datasets from Kaggle input directory\ntrain_df = read_zipped_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip')\ntest_df = read_zipped_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip')\nfeatures_df = read_zipped_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip')\nstores_df = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T18:00:54.652724Z","iopub.execute_input":"2025-07-06T18:00:54.653815Z","iopub.status.idle":"2025-07-06T18:00:55.001515Z","shell.execute_reply.started":"2025-07-06T18:00:54.653785Z","shell.execute_reply":"2025-07-06T18:00:55.000274Z"}},"outputs":[],"execution_count":70},{"id":"1b521a94-9e2f-4f12-90ee-1d6672b87987","cell_type":"code","source":"\n# Define feature columns for the model (excluding Date and Day)\nfeature_cols = [\n    'Store', 'Dept', 'Type', 'Size',\n    'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n    'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n    'IsHoliday', 'Year', 'Month', 'Week',\n    'Sales_Lag_1', 'Sales_Lag_2', 'Sales_Rolling_Mean_3', 'Sales_Rolling_Mean_5'\n]\n\n# Create pipeline with preprocessing and RandomForest model\npipeline = Pipeline([\n    ('merge_data', FunctionTransformer(merge_dataframes, kw_args={'features_df': features_df, 'stores_df': stores_df})),\n    ('date_features', FunctionTransformer(add_date_features)),\n    ('lag_rolling_features', FunctionTransformer(add_lag_rolling_features, kw_args={'train_full': None}, validate=False)),\n    ('encode_type', ColumnTransformer([\n        ('type_encoder', OrdinalEncoder(categories=[['A', 'B', 'C']]), ['Type']),\n        ('passthrough', 'passthrough', [col for col in feature_cols if col != 'Type'])\n    ], remainder='drop')),\n    ('impute_cpi_unemployment', FunctionTransformer(impute_cpi_unemployment)),\n    ('impute_markdowns', SimpleImputer(strategy='constant', fill_value=0)),\n    ('impute_lags', SimpleImputer(strategy='mean')),\n    ('rf', RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1))\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T18:00:57.307072Z","iopub.execute_input":"2025-07-06T18:00:57.307425Z","iopub.status.idle":"2025-07-06T18:00:57.315829Z","shell.execute_reply.started":"2025-07-06T18:00:57.307403Z","shell.execute_reply":"2025-07-06T18:00:57.314619Z"}},"outputs":[],"execution_count":71},{"id":"558aeaa3-d86e-425c-88d9-979b0efd9d2d","cell_type":"code","source":"\n# Merge and preprocess training data\ntrain_full = train_df.merge(features_df, on=[\"Store\", \"Date\"], how=\"left\")\ntrain_full = train_full.merge(stores_df, on=\"Store\", how=\"left\")\ntrain_full = train_full.drop(columns=['IsHoliday_y'], errors='ignore').rename(columns={'IsHoliday_x': 'IsHoliday'})\ntrain_full['Date'] = pd.to_datetime(train_full['Date'])\n\n# Select input features and target\nX_train = train_full[['Store', 'Dept', 'Date']]\ny_train = train_full['Weekly_Sales']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T18:00:57.621716Z","iopub.execute_input":"2025-07-06T18:00:57.622048Z","iopub.status.idle":"2025-07-06T18:00:57.919034Z","shell.execute_reply.started":"2025-07-06T18:00:57.622024Z","shell.execute_reply":"2025-07-06T18:00:57.917836Z"}},"outputs":[],"execution_count":72},{"id":"f57f4f4f-b37b-483e-b833-0dfb0d1fdf8c","cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\n\n# Split data for validation\nX_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Fit pipeline on training split, passing y for lag/rolling features\npipeline.set_params(lag_rolling_features__kw_args={'train_full': None, 'y': y_train_split})\npipeline.fit(X_train_split, y_train_split)\n\n# Predict on validation set\npipeline.set_params(lag_rolling_features__kw_args={'train_full': train_full, 'y': None})\ny_pred = pipeline.predict(X_val)\n\n# Calculate and print validation metrics\n# Get IsHoliday from transformed validation data\nX_val_transformed = pipeline.named_steps['merge_data'].transform(X_val)\nX_val_transformed = pipeline.named_steps['date_features'].transform(X_val_transformed)\nmae = mean_absolute_error(y_val, y_pred)\nwmae = calculate_wmae(y_val, y_pred, X_val_transformed['IsHoliday'])\nprint(f\"Validation MAE: {mae:.2f}\")\nprint(f\"Validation WMAE: {wmae:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T18:01:00.253489Z","iopub.execute_input":"2025-07-06T18:01:00.253860Z","iopub.status.idle":"2025-07-06T18:04:32.466251Z","shell.execute_reply.started":"2025-07-06T18:01:00.253836Z","shell.execute_reply":"2025-07-06T18:04:32.465165Z"}},"outputs":[{"name":"stdout","text":"Validation MAE: 16105.53\nValidation WMAE: 16339.83\n","output_type":"stream"}],"execution_count":73},{"id":"5be57033-c7de-487b-a998-68e7758df593","cell_type":"code","source":"\n# Save the trained pipeline\nos.makedirs(\"/kaggle/working/models\", exist_ok=True)\nmodel_path = \"/kaggle/working/models/walmart_sales_pipeline.pkl\"\njoblib.dump(pipeline, model_path)\n\n# Set MLflow tracking\nos.environ['MLFLOW_TRACKING_URI'] = 'https://dagshub.com/lkhok22/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting.mlflow'\nos.environ['MLFLOW_TRACKING_USERNAME'] = 'lkhok22'\nos.environ['MLFLOW_TRACKING_PASSWORD'] = 'd529c095d3cbb382ae3b78c03c1c2abafabf5d17'\n\n# Log parameters and artifact to MLflow\nmlflow.set_experiment(\"Walmart_Sales_Pipeline\")\nwith mlflow.start_run(run_name=\"Pipeline_Training\"):\n    mlflow.log_param(\"model_type\", \"RandomForest\")\n    mlflow.log_param(\"n_estimators\", 100)\n    mlflow.log_param(\"max_depth\", 20)\n    mlflow.log_metric(\"val_mae\", mae)\n    mlflow.log_metric(\"val_wmae\", wmae)\n    mlflow.log_artifact(model_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T18:04:32.468205Z","iopub.execute_input":"2025-07-06T18:04:32.468584Z","iopub.status.idle":"2025-07-06T18:04:40.476493Z","shell.execute_reply.started":"2025-07-06T18:04:32.468551Z","shell.execute_reply":"2025-07-06T18:04:40.475393Z"}},"outputs":[{"name":"stderr","text":"2025/07/06 18:04:32 INFO mlflow.tracking.fluent: Experiment with name 'Walmart_Sales_Pipeline' does not exist. Creating a new experiment.\n","output_type":"stream"},{"name":"stdout","text":"🏃 View run Pipeline_Training at: https://dagshub.com/lkhok22/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/1/runs/025cd499527b45bdb9dc72bbe66d37f2\n🧪 View experiment at: https://dagshub.com/lkhok22/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/1\n","output_type":"stream"}],"execution_count":74},{"id":"d6e848c7-f074-4327-bf9c-8a603e438c5c","cell_type":"code","source":"\n# Prepare test data\ntest_full = test_df.copy()\ntest_full['Date'] = pd.to_datetime(test_full['Date'])\nX_test = test_full[['Store', 'Dept', 'Date']]\n\n# Predict on test data with train_full for lag/rolling features\npipeline.set_params(lag_rolling_features__kw_args={'train_full': train_full, 'y': None})\npredictions = pipeline.predict(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T18:05:43.799765Z","iopub.execute_input":"2025-07-06T18:05:43.800257Z","iopub.status.idle":"2025-07-06T18:05:45.020348Z","shell.execute_reply.started":"2025-07-06T18:05:43.800220Z","shell.execute_reply":"2025-07-06T18:05:45.019148Z"}},"outputs":[],"execution_count":75},{"id":"615dc7a5-e78b-4534-9179-dc4e37e9203b","cell_type":"code","source":"\n# Create submission dataframe\nsubmission = test_df[['Store', 'Dept', 'Date']].copy()\nsubmission['Id'] = submission['Store'].astype(str) + '_' + submission['Dept'].astype(str) + '_' + submission['Date'].astype(str)\nsubmission['Weekly_Sales'] = predictions\nsubmission = submission[['Id', 'Weekly_Sales']]\n\n# Save submission\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\nprint(\"Submission file saved to /kaggle/working/submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T18:05:46.874684Z","iopub.execute_input":"2025-07-06T18:05:46.875032Z","iopub.status.idle":"2025-07-06T18:05:47.307375Z","shell.execute_reply.started":"2025-07-06T18:05:46.874984Z","shell.execute_reply":"2025-07-06T18:05:47.306099Z"}},"outputs":[{"name":"stdout","text":"Submission file saved to /kaggle/working/submission.csv\n","output_type":"stream"}],"execution_count":76},{"id":"e5b7d941-a6e5-401d-8844-52c2b8e6324e","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}