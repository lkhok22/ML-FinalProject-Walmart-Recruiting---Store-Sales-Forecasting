{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+unwE2hk2nZZVMV5McAQ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkhok22/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_DLinear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iglDJqL0GHnc"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install pandas numpy matplotlib seaborn scikit-learn torch torchvision wandb pyyaml darts --quiet\n",
        "import wandb\n",
        "wandb.login(key=\"eccf2c915699fc032ad678daf0fd4b5ac60bf87c\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU7ZjnIdW4P3",
        "outputId": "e550991b-f407-459e-8e2b-fd2cb297b00b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Extracted files: ['test.csv.zip', 'features.csv', 'train.csv.zip', 'train.csv', 'features.csv.zip', 'test.csv', 'stores.csv', 'sampleSubmission.csv.zip', 'sampleSubmission.csv']\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive and extract data\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "zip_path = '/content/drive/MyDrive/ML-FinalProject/data.zip'\n",
        "extract_to = '/content/walmart_data/'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "for file_name in os.listdir(extract_to):\n",
        "    if file_name.endswith('.zip'):\n",
        "        with zipfile.ZipFile(os.path.join(extract_to, file_name), 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "print(\"✅ Extracted files:\", os.listdir(extract_to))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BiUyAJEaXcKg"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from datetime import timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIjvZvFUYsME"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "train = pd.read_csv('/content/walmart_data/train.csv')\n",
        "features = pd.read_csv('/content/walmart_data/features.csv')\n",
        "stores = pd.read_csv('/content/walmart_data/stores.csv')\n",
        "test = pd.read_csv('/content/walmart_data/test.csv')\n",
        "\n",
        "# Merge train with features and stores\n",
        "df = pd.merge(train, features, on=['Store', 'Date'], how='left')\n",
        "df = pd.merge(df, stores, on='Store', how='left')\n",
        "df = df.drop(columns=['IsHoliday_x']).rename(columns={'IsHoliday_y': 'IsHoliday'})\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values(by=['Store', 'Dept', 'Date'])\n",
        "\n",
        "# Add holiday-specific features\n",
        "holiday_dates = {\n",
        "    'SuperBowl': ['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08'],\n",
        "    'LaborDay': ['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06'],\n",
        "    'Thanksgiving': ['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29'],\n",
        "    'Christmas': ['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27']\n",
        "}\n",
        "for holiday, dates in holiday_dates.items():\n",
        "    df[holiday] = df['Date'].isin(pd.to_datetime(dates)).astype(int)\n",
        "\n",
        "# Add time-based features\n",
        "df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Year'] = df['Date'].dt.year - df['Date'].dt.year.min()\n",
        "\n",
        "# Add holiday proximity features\n",
        "for holiday, dates in holiday_dates.items():\n",
        "    for date in pd.to_datetime(dates):\n",
        "        df[f'{holiday}_Before'] = ((df['Date'] < date) & (df['Date'] >= date - pd.Timedelta(weeks=2))).astype(int)\n",
        "        df[f'{holiday}_After'] = ((df['Date'] > date) & (df['Date'] <= date + pd.Timedelta(weeks=2))).astype(int)\n",
        "\n",
        "# Add lagged sales\n",
        "df['Lag1'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1)\n",
        "df['Lag2'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(2)\n",
        "df['Lag4'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(4)\n",
        "\n",
        "# Handle missing values\n",
        "df['MarkDown1'].fillna(0, inplace=True)\n",
        "df['MarkDown2'].fillna(0, inplace=True)\n",
        "df['MarkDown3'].fillna(0, inplace=True)\n",
        "df['MarkDown4'].fillna(0, inplace=True)\n",
        "df['MarkDown5'].fillna(0, inplace=True)\n",
        "df['CPI'].fillna(df['CPI'].mean(), inplace=True)\n",
        "df['Unemployment'].fillna(df['Unemployment'].mean(), inplace=True)\n",
        "df['Temperature'].fillna(df['Temperature'].mean(), inplace=True)\n",
        "df['Fuel_Price'].fillna(df['Fuel_Price'].mean(), inplace=True)\n",
        "df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "df['Type'] = df['Type'].map({'A': 0, 'B': 1, 'C': 2})\n",
        "df['Size'].fillna(df['Size'].mean(), inplace=True)\n",
        "df[['Lag1', 'Lag2', 'Lag4']] = df[['Lag1', 'Lag2', 'Lag4']].fillna(0)\n",
        "\n",
        "# Define feature columns\n",
        "feature_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2',\n",
        "                'MarkDown3', 'MarkDown4', 'MarkDown5', 'Size', 'Type', 'IsHoliday',\n",
        "                'SuperBowl', 'LaborDay', 'Thanksgiving', 'Christmas',\n",
        "                'WeekOfYear', 'Month', 'Year', 'Lag1', 'Lag2', 'Lag4',\n",
        "                'SuperBowl_Before', 'SuperBowl_After', 'LaborDay_Before', 'LaborDay_After']\n",
        "assert len(feature_cols) == 26, f\"Expected 26 features, got {len(feature_cols)}\"\n",
        "\n",
        "# Check for NaN or inf\n",
        "assert not df[['Weekly_Sales'] + feature_cols].isna().any().any(), \"NaN values found in data\"\n",
        "assert not df[['Weekly_Sales'] + feature_cols].isin([np.inf, -np.inf]).any().any(), \"Inf values found in data\"\n",
        "\n",
        "# Normalize features\n",
        "scaler_sales = StandardScaler()\n",
        "scaler_features = StandardScaler()\n",
        "df['Weekly_Sales'] = scaler_sales.fit_transform(df[['Weekly_Sales']].clip(lower=-1e5, upper=1e5))\n",
        "df[feature_cols] = scaler_features.fit_transform(df[feature_cols].clip(lower=-1e5, upper=1e5))\n",
        "\n",
        "# Create time series dictionary\n",
        "store_dept_pairs = df[['Store', 'Dept']].drop_duplicates()\n",
        "time_series_dict = {}\n",
        "for _, row in store_dept_pairs.iterrows():\n",
        "    store, dept = row['Store'], row['Dept']\n",
        "    sub_df = df[(df['Store'] == store) & (df['Dept'] == dept)].sort_values('Date')\n",
        "\n",
        "    date_range = pd.date_range(start=sub_df['Date'].min(), end=sub_df['Date'].max(), freq='W-FRI')\n",
        "    sub_df = sub_df.set_index('Date').reindex(date_range, method='ffill').reset_index()\n",
        "    sub_df['Store'] = store\n",
        "    sub_df['Dept'] = dept\n",
        "    sub_df['Weekly_Sales'].fillna(0, inplace=True)\n",
        "    sub_df['IsHoliday'].fillna(0, inplace=True)\n",
        "    sub_df['Type'].fillna(df[df['Store'] == store]['Type'].iloc[0], inplace=True)\n",
        "    sub_df['Size'].fillna(df[df['Store'] == store]['Size'].iloc[0], inplace=True)\n",
        "    for holiday in ['SuperBowl', 'LaborDay', 'Thanksgiving', 'Christmas',\n",
        "                    'SuperBowl_Before', 'SuperBowl_After', 'LaborDay_Before', 'LaborDay_After',\n",
        "                    'Thanksgiving_Before', 'Thanksgiving_After', 'Christmas_Before', 'Christmas_After']:\n",
        "        sub_df[holiday].fillna(0, inplace=True)\n",
        "    sub_df[['WeekOfYear', 'Month', 'Year', 'Lag1', 'Lag2', 'Lag4']] = sub_df[['WeekOfYear', 'Month', 'Year', 'Lag1', 'Lag2', 'Lag4']].fillna(0)\n",
        "    sub_df[feature_cols] = sub_df[feature_cols].fillna(0)\n",
        "\n",
        "    assert not sub_df[['Weekly_Sales'] + feature_cols].isna().any().any(), f\"NaN in sub_df for Store {store}, Dept {dept}\"\n",
        "    assert not sub_df[['Weekly_Sales'] + feature_cols].isin([np.inf, -np.inf]).any().any(), f\"Inf in sub_df for Store {store}, Dept {dept}\"\n",
        "\n",
        "    time_series_dict[(store, dept)] = {\n",
        "        'sales': sub_df['Weekly_Sales'].values.astype(np.float32),\n",
        "        'features': sub_df[feature_cols].values.astype(np.float32),\n",
        "        'dates': sub_df['index'].values,\n",
        "        'is_holiday': sub_df['IsHoliday'].values.astype(np.float32)\n",
        "    }\n",
        "print(f\"Created time series for {len(time_series_dict)} store-department pairs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AAi6dU74ah_Z"
      },
      "outputs": [],
      "source": [
        "class WalmartSalesDataset(Dataset):\n",
        "    def __init__(self, time_series_dict, seq_len=36, pred_len=6, train=True):\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.data = []\n",
        "        self.holiday_weights = []\n",
        "\n",
        "        for (store, dept), ts_data in time_series_dict.items():\n",
        "            sales = ts_data['sales']\n",
        "            features = ts_data['features']\n",
        "            is_holiday = ts_data['is_holiday']\n",
        "            n = len(sales)\n",
        "            if n < seq_len + pred_len:\n",
        "                continue\n",
        "            split_idx = int(0.8 * (n - seq_len - pred_len + 1)) if train else 0\n",
        "            start_idx = 0 if train else split_idx\n",
        "            end_idx = split_idx if train else n - seq_len - pred_len + 1\n",
        "            for i in range(start_idx, end_idx):\n",
        "                x_sales = sales[i:i + seq_len]\n",
        "                x_features = features[i:i + seq_len]\n",
        "                y = sales[i + seq_len:i + seq_len + pred_len]\n",
        "                w = is_holiday[i + seq_len:i + seq_len + pred_len] * 4 + 1\n",
        "                # Validate data\n",
        "                if np.any(np.isnan(x_sales)) or np.any(np.isnan(x_features)) or np.any(np.isnan(y)) or np.any(np.isnan(w)):\n",
        "                    continue\n",
        "                if np.any(np.isinf(x_sales)) or np.any(np.isinf(x_features)) or np.any(np.isinf(y)) or np.any(np.isinf(w)):\n",
        "                    continue\n",
        "                self.data.append((x_sales, x_features, y))\n",
        "                self.holiday_weights.append(w)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_sales, x_features, y = self.data[idx]\n",
        "        w = self.holiday_weights[idx]\n",
        "        return (torch.tensor(x_sales, dtype=torch.float32).unsqueeze(-1),\n",
        "                torch.tensor(x_features, dtype=torch.float32),\n",
        "                torch.tensor(y, dtype=torch.float32),\n",
        "                torch.tensor(w, dtype=torch.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeiHZFfJagmI",
        "outputId": "8d8b0487-e35d-4ce9-c9a7-1bcd3c5947db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 251133, Val samples: 316241\n"
          ]
        }
      ],
      "source": [
        "# Create train and validation datasets\n",
        "train_dataset = WalmartSalesDataset(time_series_dict, seq_len=36, pred_len=6, train=True)\n",
        "val_dataset = WalmartSalesDataset(time_series_dict, seq_len=36, pred_len=6, train=False)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qfFJ0fHBazNy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, feature_dim, attention_dim=128):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.query = nn.Linear(feature_dim, attention_dim)\n",
        "        self.key = nn.Linear(feature_dim, attention_dim)\n",
        "        self.value = nn.Linear(feature_dim, feature_dim)\n",
        "        self.attention_dim = attention_dim\n",
        "        self.scale = 1 / (attention_dim ** 0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, n_features)\n",
        "        query = self.query(x)  # (batch_size, seq_len, attention_dim)\n",
        "        key = self.key(x)      # (batch_size, seq_len, attention_dim)\n",
        "        value = self.value(x)  # (batch_size, seq_len, n_features)\n",
        "\n",
        "        scores = torch.bmm(query, key.transpose(1, 2)) * self.scale  # (batch_size, seq_len, seq_len)\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        output = torch.bmm(weights, value)  # (batch_size, seq_len, n_features)\n",
        "        return output\n",
        "\n",
        "class EnhancedDLinear(nn.Module):\n",
        "    def __init__(self, seq_len, pred_len, n_features, dropout=0.2, kernel_size=7):\n",
        "        super(EnhancedDLinear, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.n_features = n_features\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        # Moving average for trend decomposition\n",
        "        self.avg_pool = nn.AvgPool1d(kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "\n",
        "        # Trend component\n",
        "        self.Trend_MLP = nn.Sequential(\n",
        "            nn.Linear(seq_len, seq_len),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(seq_len, seq_len // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(seq_len // 2, pred_len)\n",
        "        )\n",
        "\n",
        "        # Seasonal component\n",
        "        self.Seasonal_MLP = nn.Sequential(\n",
        "            nn.Linear(seq_len, seq_len),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(seq_len, seq_len // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(seq_len // 2, pred_len)\n",
        "        )\n",
        "\n",
        "        # Attention for exogenous features\n",
        "        self.Attention = AttentionLayer(n_features, attention_dim=128)\n",
        "\n",
        "        # Exogenous feature interaction layer\n",
        "        self.Interaction_MLP = nn.Sequential(\n",
        "            nn.Linear(seq_len * n_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, pred_len)\n",
        "        )\n",
        "\n",
        "        # Holiday-specific branch\n",
        "        self.Holiday_MLP = nn.Sequential(\n",
        "            nn.Linear(seq_len * 5, 256),  # 5 holiday features\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, pred_len)\n",
        "        )\n",
        "\n",
        "        # Final combination layer\n",
        "        self.Combine = nn.Linear(pred_len * 4, pred_len)\n",
        "\n",
        "    def forward(self, x_sales, x_features):\n",
        "        # x_sales: (batch_size, seq_len, 1)\n",
        "        # x_features: (batch_size, seq_len, n_features)\n",
        "        x_sales = x_sales.squeeze(-1)  # (batch_size, seq_len)\n",
        "\n",
        "        # Trend decomposition with moving average\n",
        "        trend = self.avg_pool(x_sales.unsqueeze(1)).squeeze(1)  # (batch_size, seq_len)\n",
        "        seasonal = x_sales - trend  # Residual for seasonality\n",
        "\n",
        "        # Trend and seasonal predictions\n",
        "        trend_pred = self.Trend_MLP(trend)  # (batch_size, pred_len)\n",
        "        seasonal_pred = self.Seasonal_MLP(seasonal)  # (batch_size, pred_len)\n",
        "\n",
        "        # Exogenous features with attention\n",
        "        attended_features = self.Attention(x_features)  # (batch_size, seq_len, n_features)\n",
        "        attended_features = attended_features.reshape(attended_features.size(0), -1)  # (batch_size, seq_len * n_features)\n",
        "        exogenous_pred = self.Interaction_MLP(attended_features)  # (batch_size, pred_len)\n",
        "\n",
        "        # Holiday-specific prediction (use holiday features)\n",
        "        holiday_features = x_features[:, :, -5:]  # Last 5 features: IsHoliday, SuperBowl, LaborDay, Thanksgiving, Christmas\n",
        "        holiday_features = holiday_features.reshape(holiday_features.size(0), -1)  # (batch_size, seq_len * 5)\n",
        "        holiday_pred = self.Holiday_MLP(holiday_features)  # (batch_size, pred_len)\n",
        "\n",
        "        # Combine all components\n",
        "        combined = torch.cat([trend_pred, seasonal_pred, exogenous_pred, holiday_pred], dim=-1)  # (batch_size, pred_len * 4)\n",
        "        output = self.Combine(combined)  # (batch_size, pred_len)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom WMAE loss\n",
        "def wmae_loss(preds, targets, weights):\n",
        "    return torch.mean(weights * torch.abs(preds - targets))"
      ],
      "metadata": {
        "id": "aGTCsD-99qQI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EnhancedDLinear(seq_len=36, pred_len=6, n_features=26, dropout=0.2, kernel_size=7).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "criterion_mse = nn.MSELoss()\n",
        "criterion_wmae = wmae_loss\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.init(project=\"walmart-dlinear\", name=\"enhanced-dlinear-run-v3\", config={\n",
        "    \"seq_len\": 36, \"pred_len\": 6, \"batch_size\": 32, \"epochs\": 50,\n",
        "    \"learning_rate\": 0.0001, \"model\": \"EnhancedDLinear\", \"n_features\": 26\n",
        "})\n",
        "\n",
        "# Training loop\n",
        "best_val_wmae = float('inf')\n",
        "best_model_state = None\n",
        "patience = 10\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    train_loss_mse, train_loss_wmae, train_wmae_unscaled = 0.0, 0.0, 0.0\n",
        "    train_batches = 0\n",
        "    for xb_sales, xb_features, yb, wb in train_loader:\n",
        "        xb_sales, xb_features, yb, wb = xb_sales.to(device), xb_features.to(device), yb.to(device), wb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb_sales, xb_features)\n",
        "        loss_mse = criterion_mse(preds, yb)\n",
        "        loss_wmae = criterion_wmae(preds, yb, wb)\n",
        "        loss = loss_mse + 3 * loss_wmae\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            continue\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        train_loss_mse += loss_mse.item() * xb_sales.size(0)\n",
        "        train_loss_wmae += loss_wmae.item() * xb_sales.size(0)\n",
        "        preds_unscaled = scaler_sales.inverse_transform(preds.detach().cpu().numpy())\n",
        "        yb_unscaled = scaler_sales.inverse_transform(yb.detach().cpu().numpy())\n",
        "        train_wmae_unscaled += np.mean(wb.cpu().numpy() * np.abs(preds_unscaled - yb_unscaled)) * xb_sales.size(0)\n",
        "        train_batches += xb_sales.size(0)\n",
        "\n",
        "    train_loss_mse /= train_batches\n",
        "    train_loss_wmae /= train_batches\n",
        "    train_wmae_unscaled /= train_batches\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss_mse, val_loss_wmae, val_wmae_unscaled = 0.0, 0.0, 0.0\n",
        "    val_batches = 0\n",
        "    with torch.no_grad():\n",
        "        for xb_sales, xb_features, yb, wb in val_loader:\n",
        "            xb_sales, xb_features, yb, wb = xb_sales.to(device), xb_features.to(device), yb.to(device), wb.to(device)\n",
        "            preds = model(xb_sales, xb_features)\n",
        "            loss_mse = criterion_mse(preds, yb)\n",
        "            loss_wmae = criterion_wmae(preds, yb, wb)\n",
        "            if torch.isnan(loss_mse) or torch.isinf(loss_mse) or torch.isnan(loss_wmae) or torch.isinf(loss_wmae):\n",
        "                continue\n",
        "            val_loss_mse += loss_mse.item() * xb_sales.size(0)\n",
        "            val_loss_wmae += loss_wmae.item() * xb_sales.size(0)\n",
        "            preds_unscaled = scaler_sales.inverse_transform(preds.cpu().numpy())\n",
        "            yb_unscaled = scaler_sales.inverse_transform(yb.cpu().numpy())\n",
        "            val_wmae_unscaled += np.mean(wb.cpu().numpy() * np.abs(preds_unscaled - yb_unscaled)) * xb_sales.size(0)\n",
        "            val_batches += xb_sales.size(0)\n",
        "\n",
        "    val_loss_mse /= val_batches\n",
        "    val_loss_wmae /= val_batches\n",
        "    val_wmae_unscaled /= val_batches\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/50 — Train MSE: {train_loss_mse:.4f}, Train WMAE: {train_loss_wmae:.4f}, \"\n",
        "          f\"Train WMAE Unscaled: {train_wmae_unscaled:.2f}, Val MSE: {val_loss_mse:.4f}, \"\n",
        "          f\"Val WMAE: {val_loss_wmae:.4f}, Val WMAE Unscaled: {val_wmae_unscaled:.2f}\")\n",
        "    wandb.log({\n",
        "        \"train_mse\": train_loss_mse, \"train_wmae\": train_loss_wmae, \"train_wmae_unscaled\": train_wmae_unscaled,\n",
        "        \"val_mse\": val_loss_mse, \"val_wmae\": val_loss_wmae, \"val_wmae_unscaled\": val_wmae_unscaled,\n",
        "        \"lr\": optimizer.param_groups[0]['lr'], \"epoch\": epoch+1\n",
        "    })\n",
        "\n",
        "    scheduler.step(val_loss_wmae)\n",
        "    if val_loss_wmae < best_val_wmae:\n",
        "        best_val_wmae = val_loss_wmae\n",
        "        best_model_state = model.state_dict()\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "if best_model_state is not None:\n",
        "    torch.save(best_model_state, '/content/drive/MyDrive/ML-FinalProject/enhanced_dlinear_model_best_v3.pth')\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GA4Zj3wW9s7Q",
        "outputId": "660c8940-cb28-4006-988c-2c367f27bf8c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250726_134519-lbxtucq7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/lbxtucq7' target=\"_blank\">enhanced-dlinear-run-v3</a></strong> to <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/lbxtucq7' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/lbxtucq7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 — Train MSE: 0.1083, Train WMAE: 0.2435, Train WMAE Unscaled: 4986.68, Val MSE: 0.0754, Val WMAE: 0.1684, Val WMAE Unscaled: 3448.59\n",
            "Epoch 2/50 — Train MSE: 0.0849, Train WMAE: 0.2021, Train WMAE Unscaled: 4137.93, Val MSE: 0.0713, Val WMAE: 0.1629, Val WMAE Unscaled: 3335.77\n",
            "Epoch 3/50 — Train MSE: 0.0814, Train WMAE: 0.1940, Train WMAE Unscaled: 3972.61, Val MSE: 0.0700, Val WMAE: 0.1553, Val WMAE Unscaled: 3180.00\n",
            "Epoch 4/50 — Train MSE: 0.0806, Train WMAE: 0.1886, Train WMAE Unscaled: 3862.44, Val MSE: 0.0695, Val WMAE: 0.1522, Val WMAE Unscaled: 3117.29\n",
            "Epoch 5/50 — Train MSE: 0.0799, Train WMAE: 0.1854, Train WMAE Unscaled: 3796.65, Val MSE: 0.0700, Val WMAE: 0.1492, Val WMAE Unscaled: 3055.05\n",
            "Epoch 6/50 — Train MSE: 0.0787, Train WMAE: 0.1827, Train WMAE Unscaled: 3741.36, Val MSE: 0.0680, Val WMAE: 0.1468, Val WMAE Unscaled: 3007.07\n",
            "Epoch 7/50 — Train MSE: 0.0772, Train WMAE: 0.1798, Train WMAE Unscaled: 3681.43, Val MSE: 0.0659, Val WMAE: 0.1443, Val WMAE Unscaled: 2954.53\n",
            "Epoch 8/50 — Train MSE: 0.0752, Train WMAE: 0.1776, Train WMAE Unscaled: 3637.75, Val MSE: 0.0656, Val WMAE: 0.1425, Val WMAE Unscaled: 2917.67\n",
            "Epoch 9/50 — Train MSE: 0.0740, Train WMAE: 0.1753, Train WMAE Unscaled: 3589.99, Val MSE: 0.0640, Val WMAE: 0.1434, Val WMAE Unscaled: 2936.98\n",
            "Epoch 10/50 — Train MSE: 0.0722, Train WMAE: 0.1729, Train WMAE Unscaled: 3539.88, Val MSE: 0.0614, Val WMAE: 0.1387, Val WMAE Unscaled: 2840.19\n",
            "Epoch 11/50 — Train MSE: 0.0708, Train WMAE: 0.1695, Train WMAE Unscaled: 3471.12, Val MSE: 0.0595, Val WMAE: 0.1351, Val WMAE Unscaled: 2766.56\n",
            "Epoch 12/50 — Train MSE: 0.0693, Train WMAE: 0.1655, Train WMAE Unscaled: 3389.31, Val MSE: 0.0594, Val WMAE: 0.1322, Val WMAE Unscaled: 2707.93\n",
            "Epoch 13/50 — Train MSE: 0.0671, Train WMAE: 0.1601, Train WMAE Unscaled: 3279.31, Val MSE: 0.0565, Val WMAE: 0.1267, Val WMAE Unscaled: 2595.03\n",
            "Epoch 14/50 — Train MSE: 0.0657, Train WMAE: 0.1543, Train WMAE Unscaled: 3158.86, Val MSE: 0.0554, Val WMAE: 0.1207, Val WMAE Unscaled: 2472.00\n",
            "Epoch 15/50 — Train MSE: 0.0640, Train WMAE: 0.1495, Train WMAE Unscaled: 3061.77, Val MSE: 0.0542, Val WMAE: 0.1170, Val WMAE Unscaled: 2396.43\n",
            "Epoch 16/50 — Train MSE: 0.0631, Train WMAE: 0.1451, Train WMAE Unscaled: 2971.19, Val MSE: 0.0538, Val WMAE: 0.1186, Val WMAE Unscaled: 2428.04\n",
            "Epoch 17/50 — Train MSE: 0.0621, Train WMAE: 0.1421, Train WMAE Unscaled: 2909.89, Val MSE: 0.0556, Val WMAE: 0.1116, Val WMAE Unscaled: 2284.45\n",
            "Epoch 18/50 — Train MSE: 0.0614, Train WMAE: 0.1386, Train WMAE Unscaled: 2839.01, Val MSE: 0.0538, Val WMAE: 0.1107, Val WMAE Unscaled: 2267.11\n",
            "Epoch 19/50 — Train MSE: 0.0604, Train WMAE: 0.1362, Train WMAE Unscaled: 2789.75, Val MSE: 0.0524, Val WMAE: 0.1063, Val WMAE Unscaled: 2176.94\n",
            "Epoch 20/50 — Train MSE: 0.0593, Train WMAE: 0.1348, Train WMAE Unscaled: 2760.17, Val MSE: 0.0517, Val WMAE: 0.1062, Val WMAE Unscaled: 2173.81\n",
            "Epoch 21/50 — Train MSE: 0.0586, Train WMAE: 0.1325, Train WMAE Unscaled: 2713.69, Val MSE: 0.0482, Val WMAE: 0.1053, Val WMAE Unscaled: 2156.14\n",
            "Epoch 22/50 — Train MSE: 0.0574, Train WMAE: 0.1315, Train WMAE Unscaled: 2693.64, Val MSE: 0.0493, Val WMAE: 0.1019, Val WMAE Unscaled: 2087.33\n",
            "Epoch 23/50 — Train MSE: 0.0567, Train WMAE: 0.1293, Train WMAE Unscaled: 2648.63, Val MSE: 0.0474, Val WMAE: 0.1041, Val WMAE Unscaled: 2131.41\n",
            "Epoch 24/50 — Train MSE: 0.0558, Train WMAE: 0.1287, Train WMAE Unscaled: 2634.75, Val MSE: 0.0476, Val WMAE: 0.0998, Val WMAE Unscaled: 2044.21\n",
            "Epoch 25/50 — Train MSE: 0.0549, Train WMAE: 0.1270, Train WMAE Unscaled: 2600.61, Val MSE: 0.0478, Val WMAE: 0.1007, Val WMAE Unscaled: 2061.52\n",
            "Epoch 26/50 — Train MSE: 0.0547, Train WMAE: 0.1259, Train WMAE Unscaled: 2577.24, Val MSE: 0.0479, Val WMAE: 0.1008, Val WMAE Unscaled: 2063.81\n",
            "Epoch 27/50 — Train MSE: 0.0539, Train WMAE: 0.1247, Train WMAE Unscaled: 2552.87, Val MSE: 0.0469, Val WMAE: 0.0957, Val WMAE Unscaled: 1960.65\n",
            "Epoch 28/50 — Train MSE: 0.0536, Train WMAE: 0.1239, Train WMAE Unscaled: 2537.61, Val MSE: 0.0452, Val WMAE: 0.0958, Val WMAE Unscaled: 1962.18\n",
            "Epoch 29/50 — Train MSE: 0.0531, Train WMAE: 0.1227, Train WMAE Unscaled: 2513.30, Val MSE: 0.0471, Val WMAE: 0.0954, Val WMAE Unscaled: 1953.36\n",
            "Epoch 30/50 — Train MSE: 0.0527, Train WMAE: 0.1220, Train WMAE Unscaled: 2499.02, Val MSE: 0.0451, Val WMAE: 0.0974, Val WMAE Unscaled: 1994.87\n",
            "Epoch 31/50 — Train MSE: 0.0521, Train WMAE: 0.1213, Train WMAE Unscaled: 2483.87, Val MSE: 0.0438, Val WMAE: 0.0937, Val WMAE Unscaled: 1919.17\n",
            "Epoch 32/50 — Train MSE: 0.0520, Train WMAE: 0.1198, Train WMAE Unscaled: 2452.68, Val MSE: 0.0455, Val WMAE: 0.0930, Val WMAE Unscaled: 1905.11\n",
            "Epoch 33/50 — Train MSE: 0.0517, Train WMAE: 0.1197, Train WMAE Unscaled: 2451.05, Val MSE: 0.0439, Val WMAE: 0.0947, Val WMAE Unscaled: 1940.05\n",
            "Epoch 34/50 — Train MSE: 0.0517, Train WMAE: 0.1182, Train WMAE Unscaled: 2421.30, Val MSE: 0.0441, Val WMAE: 0.0921, Val WMAE Unscaled: 1886.51\n",
            "Epoch 35/50 — Train MSE: 0.0509, Train WMAE: 0.1176, Train WMAE Unscaled: 2407.36, Val MSE: 0.0453, Val WMAE: 0.0930, Val WMAE Unscaled: 1905.28\n",
            "Epoch 36/50 — Train MSE: 0.0507, Train WMAE: 0.1170, Train WMAE Unscaled: 2396.79, Val MSE: 0.0421, Val WMAE: 0.0894, Val WMAE Unscaled: 1830.30\n",
            "Epoch 37/50 — Train MSE: 0.0501, Train WMAE: 0.1169, Train WMAE Unscaled: 2394.80, Val MSE: 0.0424, Val WMAE: 0.0898, Val WMAE Unscaled: 1838.28\n",
            "Epoch 38/50 — Train MSE: 0.0497, Train WMAE: 0.1157, Train WMAE Unscaled: 2368.70, Val MSE: 0.0441, Val WMAE: 0.0905, Val WMAE Unscaled: 1852.87\n",
            "Epoch 39/50 — Train MSE: 0.0498, Train WMAE: 0.1154, Train WMAE Unscaled: 2363.04, Val MSE: 0.0425, Val WMAE: 0.0891, Val WMAE Unscaled: 1825.36\n",
            "Epoch 40/50 — Train MSE: 0.0497, Train WMAE: 0.1151, Train WMAE Unscaled: 2356.05, Val MSE: 0.0414, Val WMAE: 0.0895, Val WMAE Unscaled: 1832.57\n",
            "Epoch 41/50 — Train MSE: 0.0496, Train WMAE: 0.1142, Train WMAE Unscaled: 2338.95, Val MSE: 0.0404, Val WMAE: 0.0897, Val WMAE Unscaled: 1837.59\n",
            "Epoch 42/50 — Train MSE: 0.0491, Train WMAE: 0.1134, Train WMAE Unscaled: 2323.03, Val MSE: 0.0443, Val WMAE: 0.0956, Val WMAE Unscaled: 1958.45\n",
            "Epoch 43/50 — Train MSE: 0.0491, Train WMAE: 0.1132, Train WMAE Unscaled: 2317.15, Val MSE: 0.0396, Val WMAE: 0.0877, Val WMAE Unscaled: 1795.83\n",
            "Epoch 44/50 — Train MSE: 0.0486, Train WMAE: 0.1129, Train WMAE Unscaled: 2311.48, Val MSE: 0.0415, Val WMAE: 0.0884, Val WMAE Unscaled: 1810.13\n",
            "Epoch 45/50 — Train MSE: 0.0483, Train WMAE: 0.1120, Train WMAE Unscaled: 2294.34, Val MSE: 0.0408, Val WMAE: 0.0859, Val WMAE Unscaled: 1759.46\n",
            "Epoch 46/50 — Train MSE: 0.0482, Train WMAE: 0.1121, Train WMAE Unscaled: 2295.07, Val MSE: 0.0403, Val WMAE: 0.0833, Val WMAE Unscaled: 1706.62\n",
            "Epoch 47/50 — Train MSE: 0.0480, Train WMAE: 0.1111, Train WMAE Unscaled: 2274.80, Val MSE: 0.0400, Val WMAE: 0.0844, Val WMAE Unscaled: 1728.54\n",
            "Epoch 48/50 — Train MSE: 0.0475, Train WMAE: 0.1107, Train WMAE Unscaled: 2267.82, Val MSE: 0.0408, Val WMAE: 0.0846, Val WMAE Unscaled: 1731.51\n",
            "Epoch 49/50 — Train MSE: 0.0475, Train WMAE: 0.1104, Train WMAE Unscaled: 2260.74, Val MSE: 0.0392, Val WMAE: 0.0865, Val WMAE Unscaled: 1770.65\n",
            "Epoch 50/50 — Train MSE: 0.0476, Train WMAE: 0.1100, Train WMAE Unscaled: 2252.39, Val MSE: 0.0400, Val WMAE: 0.0835, Val WMAE Unscaled: 1710.73\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mse</td><td>█▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wmae</td><td>█▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wmae_unscaled</td><td>█▆▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mse</td><td>█▇▇▇▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>val_wmae</td><td>██▇▆▆▆▆▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_wmae_unscaled</td><td>█▇▇▆▆▆▆▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train_mse</td><td>0.04757</td></tr><tr><td>train_wmae</td><td>0.10999</td></tr><tr><td>train_wmae_unscaled</td><td>2252.3855</td></tr><tr><td>val_mse</td><td>0.04001</td></tr><tr><td>val_wmae</td><td>0.08354</td></tr><tr><td>val_wmae_unscaled</td><td>1710.73181</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">enhanced-dlinear-run-v3</strong> at: <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/lbxtucq7' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/lbxtucq7</a><br> View project at: <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250726_134519-lbxtucq7/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.init(project=\"walmart-dlinear\", name=\"submission-enhanced-run-v4\", config={\n",
        "    \"seq_len\": 36, \"pred_len\": 6, \"model\": \"EnhancedDLinear\", \"n_features\": 26\n",
        "})\n",
        "\n",
        "# Load and preprocess test data\n",
        "test = pd.read_csv('/content/walmart_data/test.csv')\n",
        "features = pd.read_csv('/content/walmart_data/features.csv')\n",
        "stores = pd.read_csv('/content/walmart_data/stores.csv')\n",
        "\n",
        "# Merge test with features and stores\n",
        "test_df = pd.merge(test, features, on=['Store', 'Date'], how='left')\n",
        "test_df = pd.merge(test_df, stores, on='Store', how='left')\n",
        "test_df = test_df.drop(columns=['IsHoliday_x']).rename(columns={'IsHoliday_y': 'IsHoliday'})\n",
        "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "test_df = test_df.sort_values(by=['Store', 'Dept', 'Date'])\n",
        "\n",
        "# Add holiday-specific features\n",
        "holiday_dates = {\n",
        "    'SuperBowl': ['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08'],\n",
        "    'LaborDay': ['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06'],\n",
        "    'Thanksgiving': ['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29'],\n",
        "    'Christmas': ['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27']\n",
        "}\n",
        "for holiday, dates in holiday_dates.items():\n",
        "    test_df[holiday] = test_df['Date'].isin(pd.to_datetime(dates)).astype(int)\n",
        "\n",
        "# Add time-based features\n",
        "test_df['WeekOfYear'] = test_df['Date'].dt.isocalendar().week\n",
        "test_df['Month'] = test_df['Date'].dt.month\n",
        "test_df['Year'] = test_df['Date'].dt.year - df['Date'].dt.year.min()\n",
        "\n",
        "# Add holiday proximity features (only for SuperBowl and LaborDay, as in training)\n",
        "for holiday, dates in holiday_dates.items():\n",
        "    if holiday in ['SuperBowl', 'LaborDay']:\n",
        "        for date in pd.to_datetime(dates):\n",
        "            test_df[f'{holiday}_Before'] = ((test_df['Date'] < date) & (test_df['Date'] >= date - pd.Timedelta(weeks=2))).astype(int)\n",
        "            test_df[f'{holiday}_After'] = ((test_df['Date'] > date) & (test_df['Date'] <= date + pd.Timedelta(weeks=2))).astype(int)\n",
        "\n",
        "# Add lagged sales (using scaled Weekly_Sales from training data)\n",
        "for _, row in test_df[['Store', 'Dept']].drop_duplicates().iterrows():\n",
        "    store, dept = row['Store'], row['Dept']\n",
        "    train_sub_df = df[(df['Store'] == store) & (df['Dept'] == dept)][['Date', 'Weekly_Sales']]\n",
        "    for lag in [1, 2, 4]:\n",
        "        test_df.loc[(test_df['Store'] == store) & (test_df['Dept'] == dept), f'Lag{lag}'] = \\\n",
        "            test_df[(test_df['Store'] == store) & (test_df['Dept'] == dept)]['Date'].map(\n",
        "                lambda x: train_sub_df[train_sub_df['Date'] == x - pd.Timedelta(weeks=lag)]['Weekly_Sales'].iloc[0] if \\\n",
        "                (x - pd.Timedelta(weeks=lag)) in train_sub_df['Date'].values else 0\n",
        "            )\n",
        "\n",
        "# Handle missing values\n",
        "test_df['MarkDown1'].fillna(0, inplace=True)\n",
        "test_df['MarkDown2'].fillna(0, inplace=True)\n",
        "test_df['MarkDown3'].fillna(0, inplace=True)\n",
        "test_df['MarkDown4'].fillna(0, inplace=True)\n",
        "test_df['MarkDown5'].fillna(0, inplace=True)\n",
        "test_df['CPI'].fillna(df['CPI'].mean(), inplace=True)\n",
        "test_df['Unemployment'].fillna(df['Unemployment'].mean(), inplace=True)\n",
        "test_df['Temperature'].fillna(df['Temperature'].mean(), inplace=True)\n",
        "test_df['Fuel_Price'].fillna(df['Fuel_Price'].mean(), inplace=True)\n",
        "test_df['IsHoliday'] = test_df['IsHoliday'].astype(int)\n",
        "test_df['Type'] = test_df['Type'].map({'A': 0, 'B': 1, 'C': 2})\n",
        "test_df['Size'].fillna(df['Size'].mean(), inplace=True)\n",
        "test_df[['Lag1', 'Lag2', 'Lag4']] = test_df[['Lag1', 'Lag2', 'Lag4']].fillna(0)\n",
        "for holiday in ['SuperBowl_Before', 'SuperBowl_After', 'LaborDay_Before', 'LaborDay_After']:\n",
        "    test_df[holiday].fillna(0, inplace=True)\n",
        "\n",
        "# Define feature columns (consistent with training)\n",
        "feature_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2',\n",
        "                'MarkDown3', 'MarkDown4', 'MarkDown5', 'Size', 'Type', 'IsHoliday',\n",
        "                'SuperBowl', 'LaborDay', 'Thanksgiving', 'Christmas',\n",
        "                'WeekOfYear', 'Month', 'Year', 'Lag1', 'Lag2', 'Lag4',\n",
        "                'SuperBowl_Before', 'SuperBowl_After', 'LaborDay_Before', 'LaborDay_After']\n",
        "assert len(feature_cols) == 26, f\"Expected 26 features, got {len(feature_cols)}\"\n",
        "\n",
        "# Scale features\n",
        "test_df[feature_cols] = scaler_features.transform(test_df[feature_cols].clip(lower=-1e5, upper=1e5))\n",
        "\n",
        "# Check for NaN or inf\n",
        "assert not test_df[feature_cols].isna().any().any(), \"NaN values in test data\"\n",
        "assert not test_df[feature_cols].isin([np.inf, -np.inf]).any().any(), \"Inf values in test data\"\n",
        "\n",
        "# Create test sequences\n",
        "def create_test_sequences(test_df, time_series_dict, seq_len=36, pred_len=6):\n",
        "    test_data = []\n",
        "    test_ids = []\n",
        "    store_dept_pairs = test_df[['Store', 'Dept']].drop_duplicates()\n",
        "\n",
        "    for _, row in store_dept_pairs.iterrows():\n",
        "        store, dept = row['Store'], row['Dept']\n",
        "        test_sub_df = test_df[(test_df['Store'] == store) & (test_df['Dept'] == dept)].sort_values('Date')\n",
        "\n",
        "        if len(test_sub_df) == 0:\n",
        "            continue\n",
        "\n",
        "        # Get training data for the store-dept pair\n",
        "        if (store, dept) in time_series_dict:\n",
        "            train_sales = time_series_dict[(store, dept)]['sales']\n",
        "            train_features = time_series_dict[(store, dept)]['features']\n",
        "            train_dates = time_series_dict[(store, dept)]['dates']\n",
        "        else:\n",
        "            # Use average from similar departments or store\n",
        "            similar_depts = df[df['Store'] == store]['Dept'].unique()\n",
        "            if len(similar_depts) > 0:\n",
        "                avg_sales = df[(df['Store'] == store) & (df['Dept'].isin(similar_depts))]['Weekly_Sales'].mean()\n",
        "                avg_features = df[(df['Store'] == store) & (df['Dept'].isin(similar_depts))][feature_cols].mean().values\n",
        "            else:\n",
        "                avg_sales = df['Weekly_Sales'].mean()\n",
        "                avg_features = df[feature_cols].mean().values\n",
        "            train_sales = np.full(seq_len, avg_sales, dtype=np.float32)\n",
        "            train_features = np.tile(avg_features, (seq_len, 1)).astype(np.float32)\n",
        "            train_dates = pd.date_range(end=test_sub_df['Date'].min() - pd.Timedelta(weeks=1), periods=seq_len, freq='W-FRI')\n",
        "\n",
        "        # Ensure training sequences are of length seq_len\n",
        "        if len(train_sales) < seq_len:\n",
        "            train_sales = np.pad(train_sales, (seq_len - len(train_sales), 0), mode='constant')\n",
        "            train_features = np.pad(train_features, ((seq_len - len(train_features), 0), (0, 0)), mode='constant')\n",
        "            train_dates = np.pad(train_dates, (seq_len - len(train_dates), 0), mode='constant', constant_values=train_dates[-1] if len(train_dates) > 0 else test_sub_df['Date'].iloc[0])\n",
        "\n",
        "        # Generate one sequence per test date\n",
        "        test_dates = test_sub_df['Date'].unique()\n",
        "        for date in test_dates:\n",
        "            end_date = date - pd.Timedelta(weeks=1)\n",
        "            if pd.to_datetime(end_date) <= pd.to_datetime(train_dates[-1]):\n",
        "                train_mask = pd.to_datetime(train_dates) <= end_date\n",
        "                x_sales = train_sales[train_mask][-seq_len:]\n",
        "                x_features = train_features[train_mask][-seq_len:]\n",
        "            else:\n",
        "                x_sales = train_sales[-seq_len:]\n",
        "                x_features = train_features[-seq_len:]\n",
        "\n",
        "            if len(x_sales) < seq_len:\n",
        "                x_sales = np.pad(x_sales, (seq_len - len(x_sales), 0), mode='constant')\n",
        "                x_features = np.pad(x_features, ((seq_len - len(x_features), 0), (0, 0)), mode='constant')\n",
        "\n",
        "            # Use features for the current test date\n",
        "            test_features = test_sub_df[test_sub_df['Date'] == date][feature_cols].values.astype(np.float32)\n",
        "            if len(test_features) > 0:\n",
        "                x_features = np.roll(x_features, -1, axis=0)\n",
        "                x_features[-1] = test_features[0]\n",
        "\n",
        "                date_str = date.strftime('%Y-%m-%d')\n",
        "                test_id = f\"{int(store)}_{int(dept)}_{date_str}\"\n",
        "                test_data.append((x_sales, x_features))\n",
        "                test_ids.append(test_id)\n",
        "\n",
        "    return test_data, test_ids\n",
        "\n",
        "# Create test sequences\n",
        "test_data, test_ids = create_test_sequences(test_df, time_series_dict, seq_len=36, pred_len=6)\n",
        "\n",
        "# Define test dataset\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.test_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_sales, x_features = self.test_data[idx]\n",
        "        return (torch.tensor(x_sales, dtype=torch.float32).unsqueeze(-1),\n",
        "                torch.tensor(x_features, dtype=torch.float32))\n",
        "\n",
        "test_dataset = TestDataset(test_data)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load best model and predict\n",
        "model = EnhancedDLinear(seq_len=36, pred_len=6, n_features=26, dropout=0.2, kernel_size=7).to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ML-FinalProject/enhanced_dlinear_model_best_v3.pth'))\n",
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for xb_sales, xb_features in test_loader:\n",
        "        xb_sales, xb_features = xb_sales.to(device), xb_features.to(device)\n",
        "        preds = model(xb_sales, xb_features)  # Shape: (batch_size, pred_len)\n",
        "        preds = preds[:, 0]  # Take first week for single-week prediction\n",
        "        predictions.append(preds.cpu().numpy())\n",
        "\n",
        "predictions = np.concatenate(predictions, axis=0).flatten()\n",
        "\n",
        "# Verify lengths\n",
        "print(f\"Length of test_ids: {len(test_ids)}\")\n",
        "print(f\"Length of predictions: {len(predictions)}\")\n",
        "assert len(test_ids) == len(predictions), f\"Mismatch: len(test_ids)={len(test_ids)}, len(predictions)={len(predictions)}\"\n",
        "\n",
        "# Inverse transform predictions\n",
        "predictions = scaler_sales.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_ids,\n",
        "    'Weekly_Sales': predictions\n",
        "})\n",
        "submission['Weekly_Sales'] = submission['Weekly_Sales'].clip(lower=0)\n",
        "submission_path = '/content/submission_enhanced_v4.csv'\n",
        "submission.to_csv(submission_path, index=False)\n",
        "print(f\"✅ Submission file created: {submission_path}\")\n",
        "\n",
        "# Log to WandB\n",
        "wandb.save(submission_path)\n",
        "wandb.log({\n",
        "    \"submission_mean_sales\": submission['Weekly_Sales'].mean(),\n",
        "    \"submission_std_sales\": submission['Weekly_Sales'].std(),\n",
        "    \"submission_min_sales\": submission['Weekly_Sales'].min(),\n",
        "    \"submission_max_sales\": submission['Weekly_Sales'].max()\n",
        "})\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yam77ev_-DRL",
        "outputId": "e40ac968-25bd-4bfb-8e64-c5fa10c465a2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">submission-enhanced-run-v3</strong> at: <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/7c7q0d2n' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/7c7q0d2n</a><br> View project at: <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250726_151200-7c7q0d2n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250726_151706-fvpzcayu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/fvpzcayu' target=\"_blank\">submission-enhanced-run-v4</a></strong> to <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/fvpzcayu' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/fvpzcayu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12-1658119318.py:58: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['MarkDown1'].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-12-1658119318.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['MarkDown2'].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-12-1658119318.py:60: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['MarkDown3'].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-12-1658119318.py:61: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['MarkDown4'].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-12-1658119318.py:62: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['MarkDown5'].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-12-1658119318.py:63: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['CPI'].fillna(df['CPI'].mean(), inplace=True)\n",
            "/tmp/ipython-input-12-1658119318.py:64: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['Unemployment'].fillna(df['Unemployment'].mean(), inplace=True)\n",
            "/tmp/ipython-input-12-1658119318.py:65: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['Temperature'].fillna(df['Temperature'].mean(), inplace=True)\n",
            "/tmp/ipython-input-12-1658119318.py:66: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['Fuel_Price'].fillna(df['Fuel_Price'].mean(), inplace=True)\n",
            "/tmp/ipython-input-12-1658119318.py:69: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['Size'].fillna(df['Size'].mean(), inplace=True)\n",
            "/tmp/ipython-input-12-1658119318.py:72: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df[holiday].fillna(0, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of test_ids: 115064\n",
            "Length of predictions: 115064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Submission file created: /content/submission_enhanced_v4.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>submission_max_sales</td><td>▁</td></tr><tr><td>submission_mean_sales</td><td>▁</td></tr><tr><td>submission_min_sales</td><td>▁</td></tr><tr><td>submission_std_sales</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>submission_max_sales</td><td>120749.07812</td></tr><tr><td>submission_mean_sales</td><td>14442.28125</td></tr><tr><td>submission_min_sales</td><td>0</td></tr><tr><td>submission_std_sales</td><td>18261.46289</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">submission-enhanced-run-v4</strong> at: <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/fvpzcayu' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/fvpzcayu</a><br> View project at: <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250726_151706-fvpzcayu/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LCybRypOR3aG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}