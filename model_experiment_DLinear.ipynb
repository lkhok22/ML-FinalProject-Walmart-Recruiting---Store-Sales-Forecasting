{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1MiZB4ep9ZcT3UgOh7u0K_jqXnsDzrp6c",
      "authorship_tag": "ABX9TyMVbFGfSMdpnfBmOGmX4PgP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkhok22/ML-FinalProject-Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_DLinear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iglDJqL0GHnc",
        "outputId": "913c0dc0-9c5d-49ef-e3f8-6e3267b6ead2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install pandas numpy matplotlib seaborn scikit-learn torch torchvision wandb pyyaml darts --quiet\n",
        "import wandb\n",
        "wandb.login(key=\"eccf2c915699fc032ad678daf0fd4b5ac60bf87c\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive and extract data\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "zip_path = '/content/drive/MyDrive/ML-FinalProject/data.zip'\n",
        "extract_to = '/content/walmart_data/'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "for file_name in os.listdir(extract_to):\n",
        "    if file_name.endswith('.zip'):\n",
        "        with zipfile.ZipFile(os.path.join(extract_to, file_name), 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "print(\"✅ Extracted files:\", os.listdir(extract_to))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU7ZjnIdW4P3",
        "outputId": "ea31308e-725a-4251-a3f1-f2fe8bc589a5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Extracted files: ['test.csv.zip', 'features.csv', 'train.csv.zip', 'train.csv', 'features.csv.zip', 'test.csv', 'stores.csv', 'sampleSubmission.csv.zip', 'sampleSubmission.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from datetime import timedelta"
      ],
      "metadata": {
        "id": "BiUyAJEaXcKg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "train = pd.read_csv('/content/walmart_data/train.csv')\n",
        "features = pd.read_csv('/content/walmart_data/features.csv')\n",
        "stores = pd.read_csv('/content/walmart_data/stores.csv')\n",
        "test = pd.read_csv('/content/walmart_data/test.csv')\n",
        "\n",
        "# Merge train with features and stores\n",
        "df = pd.merge(train, features, on=['Store', 'Date'], how='left')\n",
        "df = pd.merge(df, stores, on='Store', how='left')\n",
        "df = df.drop(columns=['IsHoliday_x']).rename(columns={'IsHoliday_y': 'IsHoliday'})\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values(by=['Store', 'Dept', 'Date'])\n",
        "\n",
        "# Handle missing values\n",
        "df['MarkDown1'].fillna(0, inplace=True)\n",
        "df['MarkDown2'].fillna(0, inplace=True)\n",
        "df['MarkDown3'].fillna(0, inplace=True)\n",
        "df['MarkDown4'].fillna(0, inplace=True)\n",
        "df['MarkDown5'].fillna(0, inplace=True)\n",
        "df['CPI'].fillna(df['CPI'].mean(), inplace=True)\n",
        "df['Unemployment'].fillna(df['Unemployment'].mean(), inplace=True)\n",
        "df['Temperature'].fillna(df['Temperature'].mean(), inplace=True)\n",
        "df['Fuel_Price'].fillna(df['Fuel_Price'].mean(), inplace=True)\n",
        "df['IsHoliday'] = df['IsHoliday'].astype(int)  # Convert boolean to 0/1\n",
        "df['Type'] = df['Type'].map({'A': 0, 'B': 1, 'C': 2})\n",
        "df['Size'].fillna(df['Size'].mean(), inplace=True)  # Fill missing Size values\n",
        "\n",
        "# Check for NaN or inf in data\n",
        "feature_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2',\n",
        "                'MarkDown3', 'MarkDown4', 'MarkDown5', 'Size', 'Type', 'IsHoliday']\n",
        "assert not df[['Weekly_Sales'] + feature_cols].isna().any().any(), \"NaN values found in data\"\n",
        "assert not df[['Weekly_Sales'] + feature_cols].isin([np.inf, -np.inf]).any().any(), \"Inf values found in data\"\n",
        "\n",
        "# Normalize features with robust scaling\n",
        "scaler_sales = StandardScaler()\n",
        "scaler_features = StandardScaler()\n",
        "df['Weekly_Sales'] = scaler_sales.fit_transform(df[['Weekly_Sales']].clip(lower=-1e5, upper=1e5))  # Clip outliers\n",
        "df[feature_cols] = scaler_features.fit_transform(df[feature_cols].clip(lower=-1e5, upper=1e5))  # Clip outliers\n",
        "\n",
        "# Create time series dictionary with exogenous features\n",
        "store_dept_pairs = df[['Store', 'Dept']].drop_duplicates()\n",
        "time_series_dict = {}\n",
        "for _, row in store_dept_pairs.iterrows():\n",
        "    store, dept = row['Store'], row['Dept']\n",
        "    sub_df = df[(df['Store'] == store) & (df['Dept'] == dept)].sort_values('Date')\n",
        "\n",
        "    # Ensure continuous weekly data\n",
        "    date_range = pd.date_range(start=sub_df['Date'].min(), end=sub_df['Date'].max(), freq='W-FRI')\n",
        "    sub_df = sub_df.set_index('Date').reindex(date_range, method='ffill').reset_index()  # Forward fill\n",
        "    sub_df['Store'] = store\n",
        "    sub_df['Dept'] = dept\n",
        "    sub_df['Weekly_Sales'].fillna(0, inplace=True)\n",
        "    sub_df['IsHoliday'].fillna(0, inplace=True)  # Fill missing holidays as non-holiday\n",
        "    sub_df['Type'].fillna(df[df['Store'] == store]['Type'].iloc[0], inplace=True)  # Fill with store's Type\n",
        "    sub_df['Size'].fillna(df[df['Store'] == store]['Size'].iloc[0], inplace=True)  # Fill with store's Size\n",
        "    sub_df[feature_cols] = sub_df[feature_cols].fillna(0)  # Fill remaining feature_cols with 0\n",
        "\n",
        "    # Check for NaN or inf after reindexing\n",
        "    assert not sub_df[['Weekly_Sales'] + feature_cols].isna().any().any(), f\"NaN in sub_df for Store {store}, Dept {dept}\"\n",
        "    assert not sub_df[['Weekly_Sales'] + feature_cols].isin([np.inf, -np.inf]).any().any(), f\"Inf in sub_df for Store {store}, Dept {dept}\"\n",
        "\n",
        "    time_series_dict[(store, dept)] = {\n",
        "        'sales': sub_df['Weekly_Sales'].values.astype(np.float32),\n",
        "        'features': sub_df[feature_cols].values.astype(np.float32),\n",
        "        'dates': sub_df['index'].values,\n",
        "        'is_holiday': sub_df['IsHoliday'].values.astype(np.float32)\n",
        "    }\n",
        "print(f\"Created time series for {len(time_series_dict)} store-department pairs.\")"
      ],
      "metadata": {
        "id": "qIjvZvFUYsME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WalmartSalesDataset(Dataset):\n",
        "    def __init__(self, time_series_dict, seq_len=36, pred_len=6, train=True):\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.data = []\n",
        "        self.holiday_weights = []\n",
        "\n",
        "        for (store, dept), ts_data in time_series_dict.items():\n",
        "            sales = ts_data['sales']\n",
        "            features = ts_data['features']\n",
        "            is_holiday = ts_data['is_holiday']\n",
        "            n = len(sales)\n",
        "            if n < seq_len + pred_len:\n",
        "                continue\n",
        "            split_idx = int(0.8 * (n - seq_len - pred_len + 1)) if train else 0\n",
        "            start_idx = 0 if train else split_idx\n",
        "            end_idx = split_idx if train else n - seq_len - pred_len + 1\n",
        "            for i in range(start_idx, end_idx):\n",
        "                x_sales = sales[i:i + seq_len]\n",
        "                x_features = features[i:i + seq_len]\n",
        "                y = sales[i + seq_len:i + seq_len + pred_len]\n",
        "                w = is_holiday[i + seq_len:i + seq_len + pred_len] * 4 + 1\n",
        "                # Validate data\n",
        "                if np.any(np.isnan(x_sales)) or np.any(np.isnan(x_features)) or np.any(np.isnan(y)) or np.any(np.isnan(w)):\n",
        "                    continue\n",
        "                if np.any(np.isinf(x_sales)) or np.any(np.isinf(x_features)) or np.any(np.isinf(y)) or np.any(np.isinf(w)):\n",
        "                    continue\n",
        "                self.data.append((x_sales, x_features, y))\n",
        "                self.holiday_weights.append(w)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_sales, x_features, y = self.data[idx]\n",
        "        w = self.holiday_weights[idx]\n",
        "        return (torch.tensor(x_sales, dtype=torch.float32).unsqueeze(-1),\n",
        "                torch.tensor(x_features, dtype=torch.float32),\n",
        "                torch.tensor(y, dtype=torch.float32),\n",
        "                torch.tensor(w, dtype=torch.float32))"
      ],
      "metadata": {
        "id": "AAi6dU74ah_Z"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and validation datasets\n",
        "train_dataset = WalmartSalesDataset(time_series_dict, seq_len=36, pred_len=6, train=True)\n",
        "val_dataset = WalmartSalesDataset(time_series_dict, seq_len=36, pred_len=6, train=False)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeiHZFfJagmI",
        "outputId": "4b02e6a3-7568-44ba-d51b-cad8b26cad2a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 251133, Val samples: 316241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced DLinear model with exogenous features\n",
        "class DLinear(nn.Module):\n",
        "    def __init__(self, seq_len, pred_len, n_features):\n",
        "        super(DLinear, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.n_features = n_features\n",
        "\n",
        "        # Linear layers for sales (trend and seasonal)\n",
        "        self.Linear_Trend = nn.Linear(seq_len, pred_len)\n",
        "        self.Linear_Seasonal = nn.Linear(seq_len, pred_len)\n",
        "\n",
        "        # Linear layer for exogenous features\n",
        "        self.Linear_Exogenous = nn.Linear(seq_len * n_features, pred_len)\n",
        "\n",
        "    def forward(self, x_sales, x_features):\n",
        "        # x_sales: (batch_size, seq_len, 1)\n",
        "        # x_features: (batch_size, seq_len, n_features)\n",
        "        x_sales = x_sales.squeeze(-1)  # (batch_size, seq_len)\n",
        "        trend = self.Linear_Trend(x_sales)\n",
        "        seasonal = self.Linear_Seasonal(x_sales)\n",
        "\n",
        "        # Flatten features and process\n",
        "        x_features = x_features.view(x_features.size(0), -1)  # (batch_size, seq_len * n_features)\n",
        "        exogenous = self.Linear_Exogenous(x_features)\n",
        "\n",
        "        return trend + seasonal + exogenous"
      ],
      "metadata": {
        "id": "qfFJ0fHBazNy"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom WMAE loss\n",
        "def wmae_loss(preds, targets, weights):\n",
        "    return torch.mean(weights * torch.abs(preds - targets))"
      ],
      "metadata": {
        "id": "yN4SYiVCbJ2M"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and validation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DLinear(seq_len=36, pred_len=6, n_features=12).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion_mse = nn.MSELoss()\n",
        "criterion_wmae = wmae_loss\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.init(project=\"walmart-dlinear\", name=\"dlinear-enhanced-run\", config={\n",
        "    \"seq_len\": 36, \"pred_len\": 6, \"batch_size\": 32, \"epochs\": 20,\n",
        "    \"learning_rate\": 0.001, \"model\": \"DLinear-Enhanced\", \"n_features\": 12\n",
        "})\n",
        "\n",
        "# Training and validation loop\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    train_loss_mse, train_loss_wmae = 0.0, 0.0\n",
        "    train_batches = 0\n",
        "    for xb_sales, xb_features, yb, wb in train_loader:\n",
        "        xb_sales, xb_features, yb, wb = xb_sales.to(device), xb_features.to(device), yb.to(device), wb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb_sales, xb_features)\n",
        "        loss_mse = criterion_mse(preds, yb)\n",
        "        loss_wmae = criterion_wmae(preds, yb, wb)\n",
        "        loss = loss_mse + loss_wmae\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            continue  # Skip batch if loss is invalid\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevent exploding gradients\n",
        "        optimizer.step()\n",
        "        train_loss_mse += loss_mse.item() * xb_sales.size(0)\n",
        "        train_loss_wmae += loss_wmae.item() * xb_sales.size(0)\n",
        "        train_batches += xb_sales.size(0)\n",
        "\n",
        "    train_loss_mse = train_loss_mse / train_batches if train_batches > 0 else float('nan')\n",
        "    train_loss_wmae = train_loss_wmae / train_batches if train_batches > 0 else float('nan')\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss_mse, val_loss_wmae = 0.0, 0.0\n",
        "    val_batches = 0\n",
        "    with torch.no_grad():\n",
        "        for xb_sales, xb_features, yb, wb in val_loader:\n",
        "            xb_sales, xb_features, yb, wb = xb_sales.to(device), xb_features.to(device), yb.to(device), wb.to(device)\n",
        "            preds = model(xb_sales, xb_features)\n",
        "            loss_mse = criterion_mse(preds, yb)\n",
        "            loss_wmae = criterion_wmae(preds, yb, wb)\n",
        "            if torch.isnan(loss_mse) or torch.isinf(loss_mse) or torch.isnan(loss_wmae) or torch.isinf(loss_wmae):\n",
        "                continue\n",
        "            val_loss_mse += loss_mse.item() * xb_sales.size(0)\n",
        "            val_loss_wmae += loss_wmae.item() * xb_sales.size(0)\n",
        "            val_batches += xb_sales.size(0)\n",
        "\n",
        "    val_loss_mse = val_loss_mse / val_batches if val_batches > 0 else float('nan')\n",
        "    val_loss_wmae = val_loss_wmae / val_batches if val_batches > 0 else float('nan')\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/20 — Train MSE: {train_loss_mse:.4f}, Train WMAE: {train_loss_wmae:.4f}, \"\n",
        "          f\"Val MSE: {val_loss_mse:.4f}, Val WMAE: {val_loss_wmae:.4f}\")\n",
        "    wandb.log({\"train_mse\": train_loss_mse, \"train_wmae\": train_loss_wmae,\n",
        "               \"val_mse\": val_loss_mse, \"val_wmae\": val_loss_wmae, \"epoch\": epoch+1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "Kp09s2XibZcx",
        "outputId": "91a77e2c-75c8-47bf-e979-83b41534d52b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250725_170810-dw52bu1z</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/dw52bu1z' target=\"_blank\">dlinear-enhanced-run</a></strong> to <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/dw52bu1z' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/dw52bu1z</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 — Train MSE: 0.1074, Train WMAE: 0.2894, Val MSE: 0.1209, Val WMAE: 0.2594\n",
            "Epoch 2/20 — Train MSE: 0.0961, Train WMAE: 0.2790, Val MSE: 0.1053, Val WMAE: 0.2376\n",
            "Epoch 3/20 — Train MSE: 0.0957, Train WMAE: 0.2771, Val MSE: 0.1103, Val WMAE: 0.2672\n",
            "Epoch 4/20 — Train MSE: 0.0955, Train WMAE: 0.2782, Val MSE: 0.1175, Val WMAE: 0.2670\n",
            "Epoch 5/20 — Train MSE: 0.0961, Train WMAE: 0.2785, Val MSE: 0.1009, Val WMAE: 0.2485\n",
            "Epoch 6/20 — Train MSE: 0.0956, Train WMAE: 0.2775, Val MSE: 0.1151, Val WMAE: 0.2935\n",
            "Epoch 7/20 — Train MSE: 0.0955, Train WMAE: 0.2783, Val MSE: 0.1192, Val WMAE: 0.2720\n",
            "Epoch 8/20 — Train MSE: 0.0957, Train WMAE: 0.2778, Val MSE: 0.1063, Val WMAE: 0.2626\n",
            "Epoch 9/20 — Train MSE: 0.0955, Train WMAE: 0.2776, Val MSE: 0.1006, Val WMAE: 0.2502\n",
            "Epoch 10/20 — Train MSE: 0.0955, Train WMAE: 0.2781, Val MSE: 0.1049, Val WMAE: 0.2695\n",
            "Epoch 11/20 — Train MSE: 0.0958, Train WMAE: 0.2780, Val MSE: 0.1092, Val WMAE: 0.2520\n",
            "Epoch 12/20 — Train MSE: 0.0954, Train WMAE: 0.2772, Val MSE: 0.1008, Val WMAE: 0.2601\n",
            "Epoch 13/20 — Train MSE: 0.0961, Train WMAE: 0.2778, Val MSE: 0.0965, Val WMAE: 0.2539\n",
            "Epoch 14/20 — Train MSE: 0.0955, Train WMAE: 0.2769, Val MSE: 0.1024, Val WMAE: 0.2534\n",
            "Epoch 15/20 — Train MSE: 0.0959, Train WMAE: 0.2780, Val MSE: 0.1086, Val WMAE: 0.2717\n",
            "Epoch 16/20 — Train MSE: 0.0958, Train WMAE: 0.2775, Val MSE: 0.1142, Val WMAE: 0.2844\n",
            "Epoch 17/20 — Train MSE: 0.0951, Train WMAE: 0.2761, Val MSE: 0.1020, Val WMAE: 0.2568\n",
            "Epoch 18/20 — Train MSE: 0.0952, Train WMAE: 0.2765, Val MSE: 0.1034, Val WMAE: 0.2764\n",
            "Epoch 19/20 — Train MSE: 0.0955, Train WMAE: 0.2776, Val MSE: 0.1016, Val WMAE: 0.2625\n",
            "Epoch 20/20 — Train MSE: 0.0956, Train WMAE: 0.2780, Val MSE: 0.1057, Val WMAE: 0.2752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Step 1: Preprocess the test data\n",
        "test = pd.read_csv('/content/walmart_data/test.csv')\n",
        "features = pd.read_csv('/content/walmart_data/features.csv')\n",
        "stores = pd.read_csv('/content/walmart_data/stores.csv')\n",
        "\n",
        "# Merge test with features and stores\n",
        "test_df = pd.merge(test, features, on=['Store', 'Date'], how='left')\n",
        "test_df = pd.merge(test_df, stores, on='Store', how='left')\n",
        "test_df = test_df.drop(columns=['IsHoliday_x']).rename(columns={'IsHoliday_y': 'IsHoliday'})\n",
        "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "test_df = test_df.sort_values(by=['Store', 'Dept', 'Date'])\n",
        "\n",
        "# Handle missing values consistently with training\n",
        "test_df['MarkDown1'].fillna(0, inplace=True)\n",
        "test_df['MarkDown2'].fillna(0, inplace=True)\n",
        "test_df['MarkDown3'].fillna(0, inplace=True)\n",
        "test_df['MarkDown4'].fillna(0, inplace=True)\n",
        "test_df['MarkDown5'].fillna(0, inplace=True)\n",
        "test_df['CPI'].fillna(df['CPI'].mean(), inplace=True)  # Use training mean\n",
        "test_df['Unemployment'].fillna(df['Unemployment'].mean(), inplace=True)\n",
        "test_df['Temperature'].fillna(df['Temperature'].mean(), inplace=True)\n",
        "test_df['Fuel_Price'].fillna(df['Fuel_Price'].mean(), inplace=True)\n",
        "test_df['IsHoliday'] = test_df['IsHoliday'].astype(int)\n",
        "test_df['Type'] = test_df['Type'].map({'A': 0, 'B': 1, 'C': 2})\n",
        "test_df['Size'].fillna(df['Size'].mean(), inplace=True)\n",
        "\n",
        "# Scale features using the same scaler as training\n",
        "feature_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2',\n",
        "                'MarkDown3', 'MarkDown4', 'MarkDown5', 'Size', 'Type', 'IsHoliday']\n",
        "test_df[feature_cols] = scaler_features.transform(test_df[feature_cols].clip(lower=-1e5, upper=1e5))\n",
        "\n",
        "# Check for NaN or inf\n",
        "assert not test_df[feature_cols].isna().any().any(), \"NaN values in test data\"\n",
        "assert not test_df[feature_cols].isin([np.inf, -np.inf]).any().any(), \"Inf values in test data\"\n",
        "\n",
        "# Step 2: Prepare test dataset for DLinear\n",
        "def create_test_sequences(test_df, time_series_dict, seq_len=36):\n",
        "    test_data = []\n",
        "    test_ids = []\n",
        "    store_dept_pairs = test_df[['Store', 'Dept']].drop_duplicates()\n",
        "\n",
        "    for _, row in store_dept_pairs.iterrows():\n",
        "        store, dept = row['Store'], row['Dept']\n",
        "        test_sub_df = test_df[(test_df['Store'] == store) & (test_df['Dept'] == dept)].sort_values('Date')\n",
        "        train_sub_df = df[(df['Store'] == store) & (df['Dept'] == dept)].sort_values('Date')\n",
        "\n",
        "        if len(test_sub_df) == 0:\n",
        "            continue\n",
        "\n",
        "        # Get the last seq_len weeks from training data\n",
        "        if (store, dept) in time_series_dict:\n",
        "            train_sales = time_series_dict[(store, dept)]['sales']\n",
        "            train_features = time_series_dict[(store, dept)]['features']\n",
        "            train_dates = time_series_dict[(store, dept)]['dates']\n",
        "        else:\n",
        "            # If no training data, use zeros or mean values\n",
        "            train_sales = np.zeros(seq_len, dtype=np.float32)\n",
        "            train_features = np.zeros((seq_len, len(feature_cols)), dtype=np.float32)\n",
        "            train_dates = test_sub_df['Date'].min() - pd.Timedelta(weeks=seq_len)\n",
        "            train_dates = pd.date_range(end=train_dates, periods=seq_len, freq='W-FRI')\n",
        "\n",
        "        # For each test date, create input sequence\n",
        "        for date in test_sub_df['Date'].unique():\n",
        "            test_row = test_sub_df[test_sub_df['Date'] == date]\n",
        "            if len(test_row) == 0:\n",
        "                continue\n",
        "\n",
        "            # Find the index of the closest prior date in training\n",
        "            last_train_date = train_dates[-1] if len(train_dates) > 0 else date - pd.Timedelta(weeks=seq_len)\n",
        "            weeks_diff = (date - pd.to_datetime(last_train_date)).days // 7\n",
        "\n",
        "            # Use the last seq_len weeks from training data\n",
        "            x_sales = train_sales[-seq_len:] if len(train_sales) >= seq_len else np.pad(train_sales, (seq_len - len(train_sales), 0), mode='constant')\n",
        "            x_features = train_features[-seq_len:] if len(train_features) >= seq_len else np.pad(train_features, ((seq_len - len(train_features), 0), (0, 0)), mode='constant')\n",
        "\n",
        "            # Append test features (only for the current week)\n",
        "            test_features = test_row[feature_cols].values.astype(np.float32)\n",
        "            if len(test_features) > 0:\n",
        "                x_features[-1] = test_features[0]  # Replace the last week's features with test week's features\n",
        "\n",
        "            # Create Id for submission\n",
        "            date_str = date.strftime('%Y-%m-%d')\n",
        "            test_id = f\"{int(store)}_{int(dept)}_{date_str}\"\n",
        "\n",
        "            test_data.append((x_sales, x_features))\n",
        "            test_ids.append(test_id)\n",
        "\n",
        "    return test_data, test_ids\n",
        "\n",
        "# Create test sequences\n",
        "test_data, test_ids = create_test_sequences(test_df, time_series_dict, seq_len=36)\n",
        "\n",
        "# Convert to DataLoader\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.test_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_sales, x_features = self.test_data[idx]\n",
        "        return (torch.tensor(x_sales, dtype=torch.float32).unsqueeze(-1),\n",
        "                torch.tensor(x_features, dtype=torch.float32))\n",
        "\n",
        "test_dataset = TestDataset(test_data)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Step 3: Generate predictions\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for xb_sales, xb_features in test_loader:\n",
        "        xb_sales, xb_features = xb_sales.to(device), xb_features.to(device)\n",
        "        preds = model(xb_sales, xb_features)  # Shape: (batch_size, pred_len)\n",
        "        # Take only the first prediction (for the next week)\n",
        "        preds = preds[:, 0]  # Shape: (batch_size,)\n",
        "        predictions.append(preds.cpu().numpy())\n",
        "\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# Step 4: Inverse transform predictions\n",
        "predictions = scaler_sales.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Step 5: Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_ids,\n",
        "    'Weekly_Sales': predictions\n",
        "})\n",
        "\n",
        "# Ensure no negative sales predictions\n",
        "submission['Weekly_Sales'] = submission['Weekly_Sales'].clip(lower=0)\n",
        "\n",
        "# Save submission file\n",
        "submission.to_csv('/content/submission.csv', index=False)\n",
        "print(\"✅ Submission file created: /content/submission.csv\")\n",
        "\n",
        "# Log submission to WandB\n",
        "wandb.save('/content/submission.csv')\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZTUD5lMefN1P",
        "outputId": "0acaff2b-5fb3-44ff-f413-7ca29984d712"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-57-1019850484.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['MarkDown1'].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-57-1019850484.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['MarkDown2'].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-57-1019850484.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['MarkDown3'].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-57-1019850484.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['MarkDown4'].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-57-1019850484.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['MarkDown5'].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-57-1019850484.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['CPI'].fillna(df['CPI'].mean(), inplace=True)  # Use training mean\n",
            "/tmp/ipython-input-57-1019850484.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['Unemployment'].fillna(df['Unemployment'].mean(), inplace=True)\n",
            "/tmp/ipython-input-57-1019850484.py:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['Temperature'].fillna(df['Temperature'].mean(), inplace=True)\n",
            "/tmp/ipython-input-57-1019850484.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['Fuel_Price'].fillna(df['Fuel_Price'].mean(), inplace=True)\n",
            "/tmp/ipython-input-57-1019850484.py:35: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['Size'].fillna(df['Size'].mean(), inplace=True)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Submission file created: /content/submission.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_mse</td><td>█▂▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>train_wmae</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▂</td></tr><tr><td>val_mse</td><td>█▄▅▇▂▆█▄▂▃▅▂▁▃▄▆▃▃▂▄</td></tr><tr><td>val_wmae</td><td>▄▁▅▅▂█▅▄▃▅▃▄▃▃▅▇▃▆▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_mse</td><td>0.09562</td></tr><tr><td>train_wmae</td><td>0.27796</td></tr><tr><td>val_mse</td><td>0.10572</td></tr><tr><td>val_wmae</td><td>0.27525</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dlinear-enhanced-run</strong> at: <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/dw52bu1z' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear/runs/dw52bu1z</a><br> View project at: <a href='https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear' target=\"_blank\">https://wandb.ai/abakh22-free-university-of-tbilisi-/walmart-dlinear</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>/content/wandb/run-20250725_170810-dw52bu1z/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# public score 6098"
      ],
      "metadata": {
        "id": "wq-O8y9ysWZD"
      }
    }
  ]
}